
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\newcommand{\slfrac}[2]{\left.#1\right/#2}
%\newcommand{\p}{\mathbb{P}}



\linespread{1.3}

\begin{document}

\title{Lecture 5: ``Focused'' Model Selection}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Local Mis-specification}

\subsection{Introduction}
In this lecture we'll be using a kind of asymptotic thought experiment that may be unfamiliar to you, so I'd like to spend a bit of time motivating it before proceeding. Roughly speaking, the idea is to consider a parameter whose value \emph{changes with sample size}. This basic idea is widely used in econometrics and statistics and is known by several different names. Among them are ``local alternatives,'' ``Pitman Drift,'' and ``local mis-specification.'' Although it may seem strange at first, ``drifting parameters'' are actually the natural asymptotic setting for certain problems, as I hope to convince you with the following two simple examples.

\subsection{What's Wrong with Asymptotic Power?}
Consider the following simple testing problem. Suppose we observe $N$ observations from the following DGP
$$X_1, X_2, \hdots, X_{N} \overset{iid}{\sim} \mathcal{N}(\mu, 1)$$
and want to test $H_0\colon \mu = 0$ against the one-sided alternative $H_1\colon \mu >0$. In this admittedly very simple example, the obvious test statistic is 
	$$T_{N} = \sqrt{N} \bar{X}_{N} \sim N\left(\mu \sqrt{N}, 1\right)$$
where $\bar{X}_{N}$ is the sample mean. We reject when $\sqrt{N} \bar{X}_{N}>z_{1-\alpha}$ where $z_{1-\alpha}$ is the $1-\alpha$ quantile of a standard normal distribution. We can calculate the power of this test as follows:
\begin{eqnarray*}
\mbox{Power}(T_{N}) &=& P\left(\sqrt{N} \bar{X}_{2N}>z_{1-\alpha}\right) = P\left(Z + \mu\sqrt{N} >z_{1-\alpha}\right)\\
	&=&P\left(Z >z_{1-\alpha} - \mu\sqrt{N}\right) = 1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{2N}\right)
\end{eqnarray*}
where $Z$ is a standard normal random variable and $\Phi$ is the corresponding CDF. Now suppose we decided to do something completely crazy: throw away half our sample. Let $\bar{X}_{N/2}$ denote the sample mean based on observations $1, 2, \hdots, \lfloor N/2 \rfloor $ \emph{only}. We can still construct a perfectly valid test with size $\alpha$ as follows. Define
	$$T_{N/2} = \sqrt{\lfloor N/2 \rfloor } \bar{X}_N \sim N\left(\mu \sqrt{\lfloor N/2 \rfloor }, 1\right)$$
and reject if $\sqrt{N} \bar{X}_N > z_{1-\alpha}$. But there's an obvious problem here: there \emph{must} be a cost for throwing away perfectly good data. Indeed, if we calculate the power for this crazy test, we'll find that it's \emph{strictly lower} than that of the sensible test based on the full sample. In particular,
	$$\mbox{Power}(T_{N/2}) = 1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{\lfloor N/2 \rfloor }\right)$$
using the same argument as above with $\lfloor N/2 \rfloor $ in place of $N$.  

Now, for an example this simple we'd never resort to asymptotics, but suppose we did. How do these two tests compare as the sample size goes to infinity? The asymptotic size in this example is the same as the finite-sample size since we know the exact sampling distribution of the test statistics under the null and neither depends on sample size. But what about the power? We have,
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{N}\right) \right] = 1\\
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N/2}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{\lfloor N/2 \rfloor }\right) \right] = 1
\end{eqnarray*}
In other words, both of these tests are \emph{consistent}: as the sample size goes to infinity, the power goes to one. Think about this for a moment: we know that for \emph{any} fixed sample size a test based on the full sample is \emph{strictly more powerful} but in the limit this difference disappears. This strongly suggests that something is wrong with our asymptotic thought experiment in this setting.

You might object that I've cooked up a particularly perverse example, but it turns out that this phenomenon is quite general. It's easy to find consistent tests, in fact it's difficult to find tests that \emph{aren't} consistent. But we know from simulation studies that not all consistent tests are created equal: some have \emph{much} better finite sample power than others. One way around this problem would be to only compare the finite-sample properties of different tests and never use asymptotics. But we almost \emph{never} know the exact sampling distribution of our test statistics. 

This is where \emph{local alternatives} come in. Rather than evaluating our tests against a \emph{fixed} alternative $\mu$, suppose we were to evaulate it against a \emph{sequence} of \emph{local} alternatives that \emph{drift towards the null} at rate $N^{-1/2}$. In other words, our alternative becomes $H_1 \colon \mu = \delta / \sqrt{N}$ where, for this one-sided test, $\delta > 0$. If we substitute $\delta/\sqrt{N}$ for $\mu$ and take the limit as $N\rightarrow \infty$, we find
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{N}}\sqrt{N}\right) \right]\\
	 &=& 1 - \Phi\left(z_{1-\alpha} - \delta \right)
\end{eqnarray*}
and similarly
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N/2}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{N}}\sqrt{\lfloor N/2 \rfloor }\right) \right]\\
	 &=& 1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{2}} \right)
\end{eqnarray*}
Wow! Our problem has disappeared! The asymptotic power of the two tests now differs in essentially the same way as the finite sample power. Also note that the power no longer converges to one. Intuitively, this is because the drifting sequence of alternatives $\delta/\sqrt{n}$ makes it ``harder and harder'' to reject the null as the sample size grows by shrinking \emph{just fast enough} but not so fast that the power goes to zero. This type of calculation is called a \emph{local power analysis}. A test that has asymptotic power greater than zero in such a setting is said to have ``power against local alternatives.''


\subsection{Weak Identification}
Drifting parameter sequences of the kind described above are also used in the weak instruments and weak identification literature. 
\todo[inline]{Possibly add a simple example later.}

\subsection{A Bias-Variance Tradeoff in the Limit}
When we derived Mallow's $C_p$, the idea was to compare models on the basis of predictive mean-squared error. Bigger models generally have a lower bias but a higher variance because there are more parameters to estimate. In the example we considered in class, everything was linear and we made enough assumptions about the finite sample distribution that we could deduce the \emph{exact} MSE conditional on $X$. In many settings, however, finite sample results unavailable and we are forced to rely on asymptotic approximations. We know there is a tradeoff between bias and variance in the finite sample and we'd like to capture this idea in our limit results. The question is how?

Suppose that $\widehat{\mu}$ is a \emph{potentially biased} estimator of $\mu$. Then we have 
	$$MSE(\widehat{\mu}) = E[(\widehat{\mu} - \mu)^2] = \left(E[\widehat{\mu} - \mu]\right)^2 + Var(\widehat{\mu})$$
Now, if we don't know the finite sample distribution of $\widehat{\mu}$, we can't calculate the proceeding expression. So what can we do instead? If $\widehat{\mu}$ is asymptotically normal, then we might try to use the features of its limit distribution to calculate the \emph{asymptotic} mean-squared error and use this to as a ``stand-in'' for the exact, finite-sample quantity. Let $\mu_0$ be the probability limit of $\widehat{\mu}$ and $\mu$ be the ``true'' parameter value. Suppose that 
	$$\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2)$$
In maximum likelihood estimation, $\mu_0$ would be the pseudo-true value that minimizes the KL divergence and $\sigma^2$ would be a diagonal element of $J^{-1}KJ^{-1}$. Now, an obvious idea is estimate $Var(\widehat{\mu})$ using the \emph{asymptotic variance}, namely $\mbox{AVAR}(\widehat{\mu}) = \sigma^2$. But what about the bias term $E[\widehat{\mu} - \mu]$? The limit distribution of $\widehat{\mu}$ is centered around $\mu_0$, the pseudo-true value, but we need to evaluate the bias relative to $\mu$. Let's try recentering by adding and subtracting $\sqrt{T}\mu$ as follows:
\begin{eqnarray*}
	\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) &=& \sqrt{T}\widehat{\mu} - \sqrt{T} \mu_0\\
	&=& \sqrt{T}\widehat{\mu} - \sqrt{T} \mu_0 - \sqrt{T} \mu + \sqrt{T} \mu\\
	&=& \sqrt{T}\left( \widehat{\mu} - \mu\right) + \sqrt{T}\left(\mu - \mu_0 \right)
\end{eqnarray*}
Rearranging, we can write
	$$\sqrt{T}\left( \widehat{\mu} - \mu\right) = \sqrt{T}\left(\widehat{\mu} - \mu \right) - \sqrt{T}\left(\mu - \mu_0 \right)$$
Now we have an expression for $\widehat{\mu}$ centered around $\mu$, so the obvious thing to do is look at the mean of the limiting distribution of $\sqrt{T}\left( \widehat{\mu} - \mu\right) $ and call this the ``asymptotic bias.'' Unfortunately, we have a problem. By assumption, the first term $\sqrt{T}\left(\widehat{\mu} - \mu_0 \right)$ is $O_p(1)$ but the second term \emph{diverges}! We recentered $\widehat{\mu}$ around $\mu$ \emph{precisely because} we thought that $\mu_0$ was potentially different from $\mu$. But if this is the case, then $ \sqrt{T}\left(\mu - \mu_0 \right) = O(T^{1/2})$. So what's going on here? The problem is that the asymptotic variance is of a \emph{different order} than the asymptotic bias. We need to scale $\widehat{\mu}$ up by $\sqrt{T}$ to get a result that has non-zero asymptotic variance, but this same scaling causes the bias to explode. In other words, there is no way to get a meaningful bias-variance tradeoff in the limit under conventional asymptotics.

So how can we fix this problem? Above we had $ \sqrt{T}\left(\mu - \mu_0\right) = O(T^{1/2})$ but what we want is $\sqrt{T}\left(\mu - \mu_0 \right) = O(1)$, so somehow or other we need to ensure that $\left(\mu - \mu_0 \right) = O(T^{-1/2})$. This is where local mis-specification makes its grand appearance. Suppose that we have a DGP under which the true parameter value is $\mu_T = \mu_0 + \delta/\sqrt{T}$ where $\delta$ is a constant. That is, suppose we assume that the true parameter value \emph{changes with sample size} and drifts towards $\mu_0$ at rate $T^{-1/2}$. This may sound like a crazy idea, but there's no arguing with the fact that it solves our problem. We have,
\begin{eqnarray*}
	\sqrt{T}\left( \widehat{\mu} - \mu_T\right) &=& \sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \sqrt{T}\left(\mu_T - \mu_0 \right)\\
		&=&\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \sqrt{T}\left(\mu_0 + \delta/\sqrt{T} - \mu_0 \right)\\
		&=& \sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \delta\\
		&\overset{d}{\rightarrow}& \mathcal{N}(0, \sigma^2) - \delta
\end{eqnarray*}
hence, the asymptotic mean-squared error of $\widehat{\mu}$ is $\mbox{AMSE}(\widehat{\mu}) = \delta^2 + \sigma^2$. But what does it mean to have a parameter that changes with sample size? It's important to be clear that this does \emph{not} mean that we think real-world datasets follow a DGP that changes with sample size. This is a \emph{thought experiment}: we also don't believe that it's possible to have an infinite sample size! When we use asymptotics, the point is to derive tractable expressions that approximate the effects that actually occur in finite samples. We know that there is a bias-variance tradeoff in finite samples but we showed above that the conventional asymptotics can't capture this. In other words, local mis-specification is a \emph{device} to get a limiting theory that provides a better approximation to what's really going on in finite samples. For more on the sense in which local mis-specification provides a much more realistic portrait of the effects of model selection, see Leeb and P\"{o}tscher (2005).



\subsection{Triangular Array Asymptotics}
\todo[inline]{Put together a simple example (iid) showing what's going on here. Perhaps a little $L_2$ WLLN. Also, give the Lindeberg-Feller CLT. Point them to the Andrews papers that give technical conditions.}

\section{Focused Evaluation}
Best model for a \emph{particular purpose} rather than ``one-size-fits all.'' Once we acknowledge that are models are probably wrong, we should ask ``how wrong'' and it might depend on what we want to use the model for. More generally, it's not clear that we would want to use the true model \emph{even if we knew its form}: bias-variance tradeoff that comes from estimation uncertainty of more complicated models.

This example comes from Hansen (2005). Consider AR$(k)$ model
	$$y_t = \mu + \beta_1 y_{t-1} + \cdots + \beta_k y_{t-k} + \epsilon_t$$
where $\{\epsilon_t\}$ is a martingale difference sequence, that is $E[\epsilon_t|I_{t-1}] = 0$. We're interested in learning about a scalar ``focus parameter'' $\theta = g(\beta)$. This could be for example, one of the individual coefficients $\beta_j$, the long-run variance, or an impulse response at some specified horizon. The point is that it's a scalar and a \emph{function} of the underlying model parameters $\beta_1, \hdots, \beta_k$. So what constitutes a ``good'' model for learning about $\theta$? The natural way to proceed is to specify a loss function and try to find the estimator $\widehat{\theta}$ that minimizes the expectation of the loss. For this example we'll use mean-squared error and search for a model that minimizes $E[(\widehat{\theta} - \theta)^2]$ 

Hansen (2005) uses a simple simulation experiment to show that different focus parameters can lead to \emph{very different} selected models. The setup is as follows. We consider the family of AR$(k)$ models for $k = 0, 1, \hdots, k_{\mbox{max}}$ but the true DGP is in fact an ARMA(1,1) model, namely
	\begin{eqnarray*}
		y_t &=& \alpha y_{t-1} + \epsilon_t - \gamma \epsilon_{t-1}\\
		\epsilon_t &\sim& \mbox{iid}\; N(0,1)
	\end{eqnarray*}
Thus \emph{none} of the models under consideration is correctly specified since the true DGP can be expressed as an AR$(\infty)$ model. Now suppose we're interested in the impulse responses. A little algebra reveals that the true impulse responses for the DGP are 
	$$\theta_m = (\alpha -\gamma)\alpha^{m-1}$$
where $m$ denotes the horizon. The estimated impulse responses for the class of models we are considering can be calculated recursively from the estimated AR parameters. By simulating the DGP with $T=200$ for a range of parameter values $(\alpha,\gamma)$ Hansen (2005) shows that the optimal AR order for approximating the impulse response of the true DGP in a minumum mean-squared error sense is \emph{highly} sensitive to $m$, the horizon of interest. To take a particularly stark example, when $\alpha = 0.5$ and $\beta = 0.9$ the optimal AR order for $m=2$ is $k=10$ but the optimal AR order for $m=6$ is $k=0$. 

\subsection{The FIC for Hansen's (2005) Example}



\section{The Focused Information Criterion (FIC)}
Portable, like AIC and BIC, based on risk minimization, like FPE and Mallow's $C_p$, but \emph{focused} in the sense of Hansen (2005). Want to be able to select different models for different purposes. Turns out to be even \emph{more} portable than AIC and BIC: although originally derived in a likelihood framework, the idea behind the FIC can be easily extended to any situation in which it is possible to derive a limiting distribution. Indeed extending the idea behing the FIC idea to novel settings has been a major area of my research in the past few years! 

Although it has been extended in a number of ways, here I'll follow the notation and framework of the original two papers: Claeskens \& Hjort (2003) and Hjort \& Claeskens (2003). Can look at various loss functions, but the original papers use MSE so that's what we'll look at here. I'll talk about extensions below. Roughly speaking, the idea is to estimate a user-specified target parameter $\mu$ with minimum mean-square error. Since finite-sample MSE can only be calculated in very simple examples, the FIC uses an asymptotic MSE to approximate finite-sample behavior. As discussed above, this requires an asymptotic framework based on drifting sequences of parameters. 

\paragraph{Local Mis-specification Framework:}
Suppose $Y_1, \hdots, Y_n$ are independent with density
	$$f_{true}(y)=f(y| \theta_0, \gamma_0 + \delta/\sqrt{n})$$
This could be a regression model, in which case the likelihood is conditional on $x$ but we'll suppress this in the notation. The $p$-vector $\theta$ contains the ``protected parameters.'' These are the parameters that we have decided in advance we definitely want to estimate. In contrast, the $q$-vector $\gamma$ contains the parameters over which we will carry out model selection: we consider the restriction $\gamma = \gamma_0$ where $\gamma_0$ is a \emph{known} parameter. When we restrict a component of $\gamma$ we \emph{do not estimate it}: we simply substitute the restriction into the likelihood. In a linear regression problem, for example, we might have something like
	$$y_i = x_i'\theta + z_i'\gamma + \epsilon_i$$
and consider setting some or all of the elements of $\gamma$ equal to zero rather than estimating them.  The true value of $\gamma$ is \emph{changing with sample size} according to $\gamma_n = \gamma_0 + \delta/\sqrt{n}$ where $\delta$ is a fixed but unknown constant $q$-vector. Thus, any specification that does not estimate $\gamma$ is \emph{locally mis-specified} but the mis-specification disappears in the limit as $n\rightarrow \infty$. 

\paragraph{N.B.} There's something slightly awkward in the notation here: $\theta_0$ is the true value of $\theta$ but $\gamma_0$ is \emph{not} the true value of $\gamma$. It is only \emph{in the limit} that $\gamma = \gamma_0$. Unlike $\theta_0$, which is unknown, $\gamma_0$ is \emph{known} since it's the restriction we're considering. This is something the econometrician chooses based on the specifics of the problem at hand.

\paragraph{The Focus Parameter:} The FIC is not a specific model selection criterion. Instead it is a \emph{procedure} that allows the \emph{user} to create her own model selection criterion for a particular problem. Let $\mu = \mu(\theta, \gamma)$ be the user-specified parameter of interest. Under local mis-specification, the true value of $\mu$ is changing with sample size according to
	$$\mu_{\mbox{true}} = \mu\left(\theta_0, \gamma_0 + \delta/\sqrt{n}\right)$$ 
The goal is to estimate $\mu$ with minimum mean-squared error. But since we are considering general ML models, it's not possible to work out the exact finite-sample distributions of the various estimators. Instead, we calculate the \emph{asymptotic mean-squared error} (AMSE) of our estimators of $\mu$ and attempt to select a model to minimize this quantity. The key innovation here is that we are \emph{not} interested in $\gamma$ for its own sake: all that matters is how our modeling decisions about $\gamma$ affect our estimates of $\mu$.

\paragraph{Candidate Models:} Considered in full generality, we could restrict any number of components of $\gamma$. Since this parameter is $q$-dimensional, we could consider a total of $2^q$ candidate models if desired. Alternatively, we could decide to consider only particular groups of restrictions. The simplest case considers only two models: the \emph{wide} model estimates \emph{all} elements of $\gamma$ and the \emph{narrow} model estimates \emph{none} of the elements of gamma. However we choose to restrict the set of candidates, each model is indexed by $S$ which is a subset of $\{1, \hdots, q\}$ that indicates which elements of $\gamma$ we estimate. Its complement, $S^c$, indicates which elements of $\gamma$ we set equal to the corresponding elements of $\gamma_0$. Each candidate model $S$ implies a maximum likelihood estimator
for the underlying model parameters $\theta$ and $\gamma_S$, where $\gamma_S$ denotes the elements of $\gamma$ that are esetimated under model $S$. The corresponding ML estimator $\widehat{\mu}_S = \mu\left(\widehat{\mu}_S, \widehat{\gamma}_S \right)$ for the target parameter $\mu$
	$$\widehat{\mu}_S = \mu\left(\widehat{\mu}_S, \widehat{\gamma}_S, \gamma_{0,S^c} \right)$$
where $\gamma_{0,S^c}$ denotes a vector containing the elements of $\gamma_0$ whose indices are in $S^c$. These are the elements of $\gamma$ that are \emph{not estimated}. 

\paragraph{Selection Matrices} In various matrix manipulations in the paper, it turns out to be helpful to define a matrix that \emph{selects} the elements of $\gamma$ that are estimated under model $S$. Let $\pi_S$ be the $|S|\times q$ matrix that ``selects'' only those elements of a $q$-vector that correspond to the indices in the set $S$. For example, suppose $q=3$ and $S = \{1,3\}$. Then,
	$$\pi_S = \left[\begin{array}
		{ccc} 1 & 0 & 0\\ 0 & 0 & 1
	\end{array} \right]$$
In this case $\gamma = (\gamma_1, \gamma_2, \gamma_3)'$ and $\pi_S \gamma = (\gamma_1, \gamma_3)'$. For the \emph{wide} or \emph{full} model, i.e.\ the model that estimates all components of $\gamma$, we have $S = \{1, \hdots, q\}$ and hence $\pi_S$ is simply the identity matrix of order $q$. An extremely useful fact about $\pi_S$ is that we can use it to construct 

\paragraph{Score Function:}
	$$\left[\begin{array}{c}
		U(y)\\
		V(y)
\end{array} \right] = \left[\begin{array}{c}
		\nabla_\theta \log{f(y, \theta_0, \gamma_0)}\\
		\nabla_\gamma \log{f(y, \theta_0, \gamma_0)}
\end{array}\right]\;\;\begin{array}{c}
		(p\times 1)\\
		(q\times 1)
\end{array}$$

\paragraph{Variance Matrix:}
	$$J_{Full} = Var_0\left[\begin{array}{c}
		U(y)\\
		V(y)
\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01}\\
		J_{10} & J_{11}
	\end{array}\right]
$$
\paragraph{Inverse of Variance Matrix:}
	$$
	J_{Full}^{-1} = \left[\begin{array}{cc}
		J^{00} & J^{01}\\
		J^{10} & J^{11}
	\end{array}\right]
$$
By the partitioned matrix inverse formula:
	$$
	K \equiv J^{11} = (J_{11} - J_{10}J_{00}^{-1}J_{01})^{-1}
$$
\paragraph{Projection Matrix:} $\pi_S$ maps vector $v$ to subvector $v_S$ containing only those components $v_j$ of $v$ for which $j$ is in the set $S$. We write $v_S = \pi_S v$.
	$$
	J_S = Var_0\left[\begin{array}{c}
		U(y)\\
		V_S(y)
\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01,S}\\
		J_{10,S} & J_{11,S}
	\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01}\pi_S'\\
		\pi_S J_{10} & \pi_S J_{11} \pi_S'
	\end{array}\right]
$$
By the partitioned matrix inverse formula:
\begin{eqnarray*}
	K_S \equiv J^{11,S} &=& \left(\pi_S K^{-1} \pi_S'\right)^{-1} = \left[\pi_S \left(J_{11} - J_{10}J_{00}^{-1}J_{01} \right) \pi_S'\right]^{-1} \\
	J^{01,S} &=& -J_{00}^{-1}J_{01}\pi_S'K_S\\
	J^{00,S} &=& J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}
\end{eqnarray*}

\paragraph{Lemma 3.1}
	$$
	\left[\begin{array}{c}
		\frac{1}{\sqrt{n}}\sum_{i=1}^n U(Y_i)\\
		\frac{1}{\sqrt{n}}\sum_{i=1}^n V(Y_i)
	\end{array}\right] \overset{d}{\rightarrow}
	\left(\begin{array}{c}
		J_{01}\delta\\
		J_{11}\delta
	\end{array}\right) + 	
	\left(\begin{array}{c}
		M\\
		N
	\end{array}\right)
$$
where
	$$
	\left(\begin{array}{c}
		M\\
		N
	\end{array}\right) \sim \mathcal{N}_{p+q}(0, J_{Full})
$$
\paragraph{Lemma 3.2}
	$$
	\left[\begin{array}{c}
		\sqrt{n} (\hat{\theta} - \theta_0)\\
		\sqrt{n} (\hat{\gamma} - \gamma_0)
\end{array}\right]\overset{d}{\rightarrow} 
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = J_S^{-1}
	\left(\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right) \sim \mathcal{N}_{p+|S|}
		\left(J_S^{-1}\left[\begin{array}{c}
			J_{01}\\
			\pi_S J_{11}
	\end{array}\right]\delta, J_S^{-1}\right)
$$
\paragraph{First Claim:} Define $W\equiv J^{10}M+J^{11}N$. Then, $W= K(N - J_{10}J_{00}^{-1}M)$ and $M$ and $W$ are indep.\ with $W\sim \mathcal{N}_q(0,K)$ and $M\sim\mathcal{N}_p(0,J_{00})$.
\begin{proof}
By the formula for the inverse of a partitioned matrix,
	\begin{eqnarray*}
		J^{11} &=&\left(J_{11} - J_{10}J_{00}^{-1}J_{01}\right)^{-1}\\
		J^{01} &=&-J_{00}^{-1}J_{01}J^{11}\\
		J^{10} &=&-J^{11}J_{10}J_{00}^{-1}\\
		J^{00} &=& J_{00}^{-1} + J_{00}^{-1}J_{01}J^{11}J_{10}J_{00}^{-1}
	\end{eqnarray*}
Thus,
	\begin{eqnarray*}
		W \equiv J^{10}M + J^{11}N &=& \left(-J^{11}J_{10}J_{00}^{-1}\right)M + J^{11}N\\
			&=&J^{11}\left( N - J_{10}J_{00}^{-1}M \right)\\
			&=&K\left( N - J_{10}J_{00}^{-1}M \right)
	\end{eqnarray*}
as required. Now we need to show the independence of $W$ and $M$. Write
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] = \left[\begin{array}{c}
		M\\
		J^{10}M + J^{11}N
	\end{array}\right] = \left[\begin{array}{cc}
		I_p&0_{p\times q}\\
		J^{10}&J^{11}
	\end{array}\right]\left[\begin{array}{c}
		M\\
		N
	\end{array}\right]\equiv A \left[\begin{array}{c}
		M\\
		N
	\end{array}\right]
$$
Since $\left[\begin{array}{c} M\\ N \end{array}\right]\sim \mathcal{N}_{p+q}(0, J_{Full})$, we have $A\left[\begin{array}{c} M\\ N \end{array}\right]\sim \mathcal{N}_{p+q}(0, A J_{Full}A')$. Multiplying through, we find that
	$$
	AJ_{Full}A' = \left[\begin{array}{cc}
		J_{00}& J_{00}J^{01}+J_{01}J^{11}\\
		J^{10}J_{00}+J^{11}J_{10}& J^{10}\left(J_{00}J^{01}+J_{01}J^{11}\right) + J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right)
	\end{array}\right]
$$
Now,
	\begin{eqnarray*}
		 J_{00}J^{01}+J_{01}J^{11} &=& J_{00}\left(-J_{00}^{-1}J_{01}J^{11}\right)+J_{01}J^{11} \\
			&=&  -J_{01}J^{11}+J_{01}J^{11} = 0 
	\end{eqnarray*}
and similarly
	\begin{eqnarray*}
		J^{10}J_{00}+J^{11}J_{10} &=& \left(-J^{11}J_{10}J_{00}^{-1}\right)J_{00}+J^{11}J_{10}\\
			&=& -J^{11}J_{10}+J^{11}J_{10} = 0
\end{eqnarray*}
Finally,
	\begin{eqnarray*}
		J^{10}\left(J_{00}J^{01}+J_{01}J^{11}\right) + J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right) &=& J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right)\\
		&=&J^{11}\left(J_{10}\left[-J_{00}^{-1}J_{01}J^{11}\right]+J_{11}J^{11}\right)\\
		&=&J^{11}\left(J_{11}-J_{10}J_{00}^{-1}J_{01}\right)J^{11}\\
		&=&J^{11}\left(J_{11}\right)^{-1}J^{11}=J^{11}
\end{eqnarray*}
where the first equality uses the fact that $J_{00}J^{01}+J_{01}J^{11} =0$. Therefore
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&J^{11}
	\end{array}\right]\right)
$$
Thus, $W\sim \mathcal{N}_q(0,J^{11})$ independent of $M\sim\mathcal{N}_p(0,J_{00})$.
\end{proof}


\paragraph{Second Claim:} Lemma 3.2 and some algebra imply that
	$$\hat{\delta}_S \equiv \sqrt{n}(\hat{\gamma}_S - \gamma_{0,S})\overset{d}{\rightarrow} D_S$$
where $D_S = K_S \pi_s K^{-1}(\delta + W) = K_S \pi_s K^{-1}D$, defining $D = \delta + W$. In particular:
	$$
	D_N \equiv \hat{\delta}_{Full} = \sqrt{n}(\hat{\gamma}_{Full} -\gamma_0) \overset{d}{\rightarrow} D = (\delta+W) \sim \mathcal{N}_q(\delta,K)
$$
where, as before, $K \equiv J^{11}$.
\begin{proof}
Lemma 3.2 establishes that
	$$
	\left[\begin{array}{c}
		\sqrt{n} (\hat{\theta} - \theta_0)\\
		\sqrt{n} (\hat{\gamma} - \gamma_0)
\end{array}\right]\overset{d}{\rightarrow} 
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = J_S^{-1}
	\left(\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right)$$
so we know immediately that $\hat{\delta}_S \equiv \sqrt{n} (\hat{\gamma}_S - \gamma_{0,S})\overset{d}{\rightarrow} D_S$. We need to show that $D_S = K_S\pi_S K^{-1}D$ where $D = \delta + W$. We have:
	\begin{eqnarray*}
		\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] &=& J_S^{-1}
	\left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] = \left[\begin{array}{cc}
				J^{00,S}&J^{01,S}\\
				J^{10,S}&J^{11,S}	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] \\ \\
	&=& \left[\begin{array}{cc}
				J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}&-J_{00}^{-1}J_{01}\pi_S'K_S\\
				-K_S \pi_S J_{10}J_{00}^{-1}&K_S	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] 
	\end{eqnarray*}
where $K_S = \left(\pi_S K^{-1} \pi_S'\right)^{-1}$ and $K\equiv J^{11}$. Thus, we have
	\begin{eqnarray*}
		D_S &=& -K_S \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) + K_S \left(\pi_S J_{11}\delta + N_S \right)\\
			&=& K_S \left[ \left(\pi_S J_{11}\delta + N_S \right) - \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) \right]\\
			&=& K_S \left[ \pi_S J_{11}\delta + \pi_S N  - \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) \right]\\
			&=& K_S \pi_S \left[ \left(J_{11} - J_{10}J_{00}^{-1}J_{01}\right)\delta +  N  -  J_{10}J_{00}^{-1} M \right]\\
		&=& K_S \pi_S \left[ K^{-1}\delta +K^{-1}K  \left(N  -  J_{10}J_{00}^{-1} M \right)\right]\\
		&=& K_S \pi_S K^{-1} \left[ \delta +K  \left(N  -  J_{10}J_{00}^{-1} M \right)\right]\\
		&=& K_S \pi_S K^{-1} \left( \delta + W\right)
	\end{eqnarray*}
\end{proof}


\paragraph{More Notation:} 
	\begin{eqnarray*}
		H_S &\equiv& \left(K^{-1}\right)^{1/2}(\pi_S' K_S \pi_S) \left( K^{-1} \right)^{1/2} \\
		\omega &\equiv& J_{10}J_{00}^{-1} \nabla_\theta \mu(\theta_0,\gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0) 
	\end{eqnarray*}
Notice that:
	\begin{enumerate}
		\item $\omega$ depends on the choice of focus parameter $\mu$
		\item $H_S$ is symmetric and idempotent, thus it is a projection matrix.
		\item From (2) it follows that $H_S$ is orthogonal to $I - H_S$
		\item Define $H_{\emptyset}$ as a $q\times q$ null matrix.
\end{enumerate}

\paragraph{Lemma 3.3} If $\mu$ has continuous partial derivatives in a neighborhood of $(\theta_0, \gamma_0)$, then:
	$$
	\sqrt{n}\left( \hat{\mu}_S - \mu_{true} \right) \overset{d}{\rightarrow} \Lambda_S
$$
where $\mu_{true} = \mu(\theta_0, \gamma_0+\delta/\sqrt{n})$ and 
	$$
	\Lambda_S = \nabla_\theta \mu(\theta_0, \gamma_0)' J_{00}^{-1} M + \omega'\left( \delta - K^{1/2}H_S K^{-1/2}D\right)
$$
Thus, the the scalar random variable $\Lambda_S$ follows a normal distribution with
	\begin{eqnarray*}
		\mbox{Mean}&=&\omega'(I - K^{1/2}H_SK^{-1/2})\delta\\
		\mbox{Variance}&=&\nabla_\theta(\theta_0, \gamma_0)'J_{00}^{-1}\nabla_\theta(\theta_0, \gamma_0) + \omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
\begin{proof}
Applying the delta-method to Lemma 3.2 along with a mean value expansion for 		$$
	\mu(\hat{\theta}_S, \hat{\gamma}_S) - \mu(\theta_0, \gamma_0+\delta/\sqrt{n})
$$
gives\footnote{for details see Section 2.5 of my First-Year Paper}
	$$
		\sqrt{n}\left( \hat{\mu}_S - \mu_{true} \right) \overset{d}{\rightarrow} \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta \equiv \Lambda_S
$$
From here, it is immediate that $\Lambda_S$ is MV normal, as it is a linear combination of a normal random vector. Although we \emph{could} find its mean and variance directly using this result, it will be helpful to express the limiting RV $\Lambda_S$ in the alternative formulation given in Lemma 3.3. This is because $M$ and $D = \delta + W$ are \emph{independent} normal random vectors! We established above that:
	\begin{eqnarray*}
		\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] &=& J_S^{-1}
	\left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] = \left[\begin{array}{cc}
				J^{00,S}&J^{01,S}\\
				J^{10,S}&J^{11,S}	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] \\ \\
	&=& \left[\begin{array}{cc}
				J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}&-J_{00}^{-1}J_{01}\pi_S'K_S\\
				-K_S \pi_S J_{10}J_{00}^{-1}&K_S	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] 
	\end{eqnarray*}
and, multiplying this out, found $D_S = K_S \pi_S K^{-1}(\delta + W)$. Now we will do the same for $C_S$. To begin:
	\begin{eqnarray*}
		C_S &=& J^{00,S}\left(J_{01}\delta + M\right) + J^{01,S}\left( \pi_S J_{11}\delta + N_S\right)\\
			&=& \left(J^{00,S}J_{01} + J^{01,S} \pi_S J_{11}\right)\delta + \left( J^{00,S}M + J^{01,S} N_S\right)\\
			&\equiv& A\delta + B
\end{eqnarray*}
Now, 
	\begin{eqnarray*}
		A &\equiv& J^{00,S}J_{01} + J^{01,S} \pi_S J_{11} \\
			&=& \left(J_{00}^{-1} + J_{00}^{-1}J_{01}\left[\pi_S'K_S\pi_S \right]J_{10}J_{00}^{-1}\right) J_{01} + \left( -J_{00}^{-1}J_{01}\pi_S'K_S\right) \pi_S J_{11}\\
			&=& J_{00}^{-1}J_{01}\left( I + \left[\pi_S'K_S\pi_S \right]J_{10}J_{00}^{-1}J_{01} -  \left[\pi_S'K_S\pi_S \right]J_{11} \right)\\
			&=&J_{00}^{-1}J_{01}\left[ I -  \left(\pi_S'K_S\pi_S \right)\left(J_{11} -  J_{10}J_{00}^{-1}J_{01}\right) \right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  \left(\pi_S'K_S\pi_S \right)K^{-1}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}K^{-1/2}\left(\pi_S'K_S\pi_S \right)K^{-1/2}K^{-1/2}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}\left(K^{-1/2}\pi_S'K_S\pi_S K^{-1/2}\right)K^{-1/2}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}H_SK^{-1/2}\right]
	\end{eqnarray*}
	\begin{eqnarray*}
		B &\equiv&  J^{00,S}M + J^{01,S} N_S\\
			&=& \left(J_{00}^{-1} + J_{00}^{-1}J_{01}\pi_S'K_S\pi_S J_{10}J_{00}^{-1}\right) M + \left( -J_{00}^{-1}J_{01}\pi_S'K_S\right) \pi_S N\\
			&=& J_{00}^{-1}M  + J_{00}^{-1}J_{01}\pi_S'K_S\pi_S\left( J_{10}J_{00}^{-1}M - N\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\pi_S'K_S\pi_S\left(N - J_{10}J_{00}^{-1}M\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1}K\right)\left(N - J_{10}J_{00}^{-1}M\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1}\right)\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1/2}K^{-1/2}\right)\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}\left(K^{-1/2}\pi_S'K_S\pi_SK^{-1/2}\right)K^{-1/2}\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}W 
	\end{eqnarray*}
where we have substituted the definition of $H_S$ and  used the fact that, as we showed above, $K(N - J_{10}J_{00}^{-1}M) = W$. Combining these, 
	\begin{eqnarray*}
	C_S &=& J_{00}^{-1}J_{01}\left( I -  K^{1/2}H_SK^{-1/2}\right) \delta + J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}W \\
		&=& J_{00}^{-1}J_{01}\delta -  \left(J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}\right)\delta + J_{00}^{-1}M  - \left(J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}\right)W \\
		&=& \left(J_{00}^{-1}J_{01}\right)\delta -  \left(J_{00}^{-1}J_{01}\right)K^{1/2}H_SK^{-1/2}(\delta+W) + J_{00}^{-1}M \\
		&=& J_{00}^{-1}M + J_{00}^{-1}J_{01}\left[\delta -  K^{1/2}H_SK^{-1/2}(\delta+W)\right]\\
		&=& J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)
\end{eqnarray*}
Thus, expressing everything in terms of the independent normal random vectors $M$ and $D = \delta + W$, we have
	$$
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = 	\left[\begin{array}{c}
		 J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\\
		K_S \pi_S K^{-1}D
	\end{array}\right] 
$$
Now, recall that
	$$
	\Lambda_S  = \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta
$$
Multiplying through,
	$$
			\nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S = 		 \nabla_{\theta}\mu(\theta_0, \gamma_0)'\left[J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\right]\\
$$
and
	\begin{eqnarray*}
			\left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S &=& \nabla_\gamma \mu(\theta_0, \gamma_0)'\pi_S' D_S\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)'\pi_S'  K_S \pi_S
 K^{-1}D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' \left(K^{1/2}K^{-1/2}\right)\pi_S'  K_S \pi_S
 \left(K^{-1/2}K^{-1/2}\right)D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}\left(K^{-1/2}\pi_S'  K_S \pi_S
K^{-1/2}\right)K^{-1/2}D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}H_S K^{-1/2}D
\end{eqnarray*}
Therefore,
	\begin{eqnarray*}
		\Lambda_S &=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'\left[J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\right]\\
			&& \;\;\;\;\;\;\; + \left[\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}H_S K^{-1/2}D\right] - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta\\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \nabla_{\theta}\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\\
			&&\;\;\;\;\;\;\; - \nabla_\gamma \mu(\theta_0, \gamma_0)' \left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \left[\nabla_{\theta}\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{01} - \nabla_\gamma \mu(\theta_0, \gamma_0)'\right]\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \left[J_{10}J_{00}^{-1}\nabla_{\theta}\mu(\theta_0, \gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0)\right]'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)
	\end{eqnarray*}
Now we can easily calculate the mean and variance of the scalar random variable $\Lambda_S$ as we have expressed it as a linear combination of two independent normal random vectors: $M$ and $D=\delta + W$. Recall that
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&K
	\end{array}\right]\right)
$$
where $K = J^{11}$. Exploiting the symmetry of variance matrices in several places as well as the symmetry and idempotency of $H_S$, we have
	\begin{eqnarray*}
		E[\Lambda_S] &=& E\left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M\right] + E\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}E\left[M\right] + \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}E[\delta + W]\\
			&=& \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}\left(\delta +E[W]\right)\\
			&=& \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}\delta\\
			&=& \omega' \left(I - K^{1/2}H_S K^{-1/2}\right)\delta
	\end{eqnarray*}
	\begin{eqnarray*}
		 Var\left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}M\right] &=& \left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\right]Var[M]\left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\right]'\\
		&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}J_{00}J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0)\\
		&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0)
	\end{eqnarray*}
	\begin{eqnarray*}
		Var\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]&=&\left( \omega'K^{1/2}H_S K^{-1/2}\right)Var[D]\left( \omega'K^{1/2}H_S K^{-1/2}\right)'\\
				&=&\omega'K^{1/2}H_S K^{-1/2}K K^{-1/2}H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S \left(K^{-1/2}K^{1/2}\right)\left(K^{1/2} K^{-1/2}\right)H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
	\begin{eqnarray*}
		Var[\Lambda_S] &=& Var\left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M\right] + Var\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]\\
					&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0) + \omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
\end{proof}

\paragraph{Compromise Estimators:} From Lemma 3.2, we know that 
	$$
	\hat{\delta}_S \equiv \sqrt{n} \left(\hat{\gamma}_S - \gamma_{0,S}\right)\overset{d}{\rightarrow} D_S = K_S\pi_S K^{-1}(\delta + W)
$$
In the case of the full model, that is $S = \left\{1,2, \overset{d}{\rightarrow}ots, q \right\}$ and $\pi_S = I_q$ so that $K_S = K$, this gives
	$$
	D_n \equiv \hat{\delta}_{full} \equiv \sqrt{n} \left(\hat{\gamma}_{full} - \gamma_0\right)\overset{d}{\rightarrow} D = (\delta + W)
$$
Thus, \emph{any} submodel estimator $\hat{\delta}_S$ of $\delta$ converges in distribution to a linear combination of $D$, while the full model estimator of $D_n$ of $\delta$ simply converges in distribution to $D$. In other words, the behavior of $\hat{\delta}_S$ is ``essentially determined'' by that of $D_n$. 
More precisely, the difference between $\hat{\delta}_S$ and $K_S \pi_S K^{-1}D_n$ is at most $o_p(1)$. Now consider a \textbf{Compromise Estimator} of the form:
	$$
	\hat{\mu} = \sum_S c\left(S|D_n\right)\hat{\mu}_S
$$
that is, we weight and sum submodel estimators where the weights are a function of $D_n = \hat{\delta}_{full} =\sqrt{n}(\hat{\gamma}_{full}-\gamma_0)$. \emph{To ensure consistency, the weights must sum to one}.

\paragraph{Notation:} Define $G$, a $q\times q$ matrix of functions, by
	$$
	G(\overset{d}{\rightarrow}ot) = K^{-1/2} \left\{\sum_S c(S|\overset{d}{\rightarrow}ot) H_S  \right\} K^{1/2}
$$
and $\hat{\delta}(D)$, an estimator of $\delta$ based on $D$, by
	$$
	\hat{\delta}(D) = G(D)'D 
$$
Since $H_S$ is symmetric and the weights $c(\overset{d}{\rightarrow}ot|\overset{d}{\rightarrow}ot)$ are scalars,
	$$
	\hat{\delta}(D) = \left[K^{-1/2} \left\{\sum_S c(S|D) H_S  \right\} K^{1/2}\right]'D = K^{1/2} \left\{\sum_S c(S|D) H_S  \right\} K^{-1/2} D
$$

\paragraph{Theorem 4.1} As long as the weight functions $c(\overset{d}{\rightarrow}ot|\overset{d}{\rightarrow}ot)$ sum to one and have at most a countable number of discontinuities, then
	$$
	\sqrt{n} \left(\hat{\mu} - \mu_{true}  \right) \overset{d}{\rightarrow} \sum_S c(S|D) \Lambda_S \equiv \Lambda
$$
and
	$$
\Lambda = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(D) \right] 
$$
This is, in general, a \textbf{non-normal distribution} with
	\begin{eqnarray*}
		\hbox{mean} &=& \omega' \left\{ \delta - E\left[  \hat{\delta}(D) \right] \right\}\\
		\hbox{variance} &=& \tau_0^2 + \omega'Var\left[\hat{\delta}(D)\right]\omega
	\end{eqnarray*}
where
	\begin{eqnarray*}
	\tau_0^2 &\equiv& \nabla+\theta \mu(\theta_0, \gamma_0)' J_{00}^{-1}\\
	\omega &\equiv& J_{10}J_{00}^{-1}\nabla_\theta \mu(\theta_0, \gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0)
\end{eqnarray*}
and the MSE of $\Lambda$ is
	$$
	E[\Lambda^2] = \tau_0^2 + R(\delta)
$$
where
	$$R(\delta) = \omega' \left[ \left\{ \hat{\delta}(D) - \delta \right\}  \left\{ \hat{\delta}(D) - \delta \right\}  ' \right]\omega$$

\begin{proof}
First, using the fact that $\sum_S c(S|D_n) = 1$, we have
	\begin{eqnarray*}
		\sqrt{n}\left( \hat{\mu} -\mu_{true} \right) &=& \sqrt{n}\left[\sum_{S} c(S|D_n)\hat{\mu}_S - \mu_{true} \right]\\
			&=& \sqrt{n}\left[\sum_{S} c(S|D_n)\hat{\mu}_S - \left\{\sum_{S} c(S|D_n)\right\}\mu_{true} \right]\\
			&=&\sum_S \left[ c(S|D_n) \sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)  \right]
\end{eqnarray*}
So we see that $\sqrt{n}\left( \hat{\mu} -\mu_{true} \right)$ is an almost-surely continuous function of the submodel estimators $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ and $D_n = \sqrt{n}\left( \hat{\gamma}_{full} - \gamma_0 \right)$. Thus, to find the limiting distribution of the compromise estimator, we can apply the continuous mapping theorem, provided we have \emph{joint} convergence in distribution of the submodel estimators and $D_n$.

Fortunately, we have already established precisely this joint convergence! In Lemma 3.3, we showed that the limit distribution of each submodel estimator $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ is a linear combination of
	$$
	\left(\begin{array}{c}M \\ N \end{array}\right) \sim \mathcal{N}_{p+q}(0, J_{full})
$$
Further the limit distribution of $D_n = \sqrt{n}\left( \hat{\gamma}_{full} - \gamma_0 \right)$ is another linear combination of $M$ and $N$, namely 
	$$D = (\delta + W) = \delta + K\left(N - J_{10}J_{00}^{-1}M\right)$$
Therefore, the limiting distribution of all the submodel estimators \emph{jointly} with $D_n$ can be written as the appropriate linear combination of $(M',N')'$, so the joint distribution is multivariate normal. Now we can apply the continuous mapping theorem as desired, to find:
	$$
\sqrt{n}\left( \hat{\mu} -\mu_{true} \right) = \sum_S \left[ c(S|D_n) \sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)  \right] \overset{d}{\rightarrow} \sum_S c(S|D_n) \Lambda_S
$$
where $\Lambda_S$ is the limit distribution of $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ defined above. Let
	$$\Lambda \equiv \sum_S c(S|D_n) \Lambda_S$$
We want to express $\Lambda$ in a more convenient form using the fact that
	$$\Lambda_S = \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)$$
as shown in Lemma 3.3. Substituting,
	\begin{eqnarray*}
	\Lambda &=& \sum_S c(S|D) \left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)  \right] \\
			&=&\left[ \sum_S c(S|D)\right]\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \sum_S \left[c(S|D)\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right) \right] \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \left[\sum_S c(S|D)\right]\omega'\delta - \sum_S c(S|D)\omega 'K^{1/2}H_S K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \sum_S c(S|D)\omega 'K^{1/2}H_S K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'K^{1/2}\left[ \sum_S c(S|D)H_S\right] K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega '\left(K^{-1/2}\left[ \sum_S c(S|D)H_S\right] K^{1/2}\right)'D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'G(D)'D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega' \hat{\delta}(D)\\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\left\{\delta - \hat{\delta}(D)\right\}
\end{eqnarray*}
where we have used the following facts:
	\begin{enumerate}
		\item Only $H_S$ depends on $S$.
		\item The weights sum to one.
		\item As scalars, the weights commute and are (trivially) symmetric.
		\item $H_S$, $K^{1/2}$, and $K^{-1/2}$ are symmetric.
	\end{enumerate}
along with the definitions of $G(\overset{d}{\rightarrow}ot)$ and $\hat{\delta}(D)$. Now we have shown that
	$$
\Lambda = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(D) \right] 
$$
Notice that, since $\hat{\delta}(D)$ depends only on $D = \delta + W$, and $M$ is independent of $W$, it follows that the two terms in this expression are likewise independent. The first follows a normal distribution but the second is, in general, non-normal.

Now, since $M$ and $D = \delta + W$ are independent, it follows that the distribution of $M|D$ is the same as that of $M$. Thus, 
	$$
	\Lambda|(D=d) \sim  \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(d) \right] 
$$
which is a normal distribution, since $\hat{\delta}(d)$ is a constant taking into account the conditioning. The mean and variance are as follows:
	\begin{eqnarray*}
	E\left[\Lambda  | D= d\right] &=& E\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + \omega' \left[\delta - \hat{\delta}(d) \right] \\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}E\left[M\right] + \omega' \left[\delta - \hat{\delta}(d) \right] \\
			&=& \omega' \left[\delta - \hat{\delta}(d) \right] 	
\end{eqnarray*}
	\begin{eqnarray*}
	Var\left[\Lambda  | D= d\right] &=& Var\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right]\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}Var[M]J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{00}J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
	&\equiv& \tau_0^2
\end{eqnarray*}
since $\omega'(\delta - \hat{\delta}(d) )$ is a constant. Note that $\tau^2_0$ is the \emph{minimal variance} of the estimators under consideration. Although the \emph{unconditional distribution} of $\Lambda$ is non-normal, we can still calculate its mean and variance using our decomposition into two independent terms and the linearity of expectation:
	\begin{eqnarray*}
	E\left[\Lambda\right] &=& E\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + E\left[\omega' \left\{\delta - \hat{\delta}(d) \right\}\right] \\
			&=& \omega' \delta  -\omega'  E\left[\hat{\delta}(d)\right] \\	
			&=& \omega' \left\{\delta  - E\left[\hat{\delta}(d)\right]\right\}
\end{eqnarray*}
	\begin{eqnarray*}
	Var\left[\Lambda\right] &=& Var\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + Var\left[\omega' \left\{\delta - \hat{\delta}(d) \right\}\right]\\
			&=& \tau^2_0 + \omega'Var\left[\hat{\delta}(D)  \right]\omega
\end{eqnarray*}
Now, $\Lambda$ is the limit distribution of $\sqrt{n}(\hat{\mu}-\mu_{true})$ where $\hat{\mu}$ is the compromise estimator, thus if asymptotically unbiased, it should be centered around zero. Accordingly we find the MSE of $\Lambda$ as follows:
	\begin{eqnarray*}
		MSE(\Lambda) &=& E\left[(\Lambda - 0)^2  \right] = E\left[\Lambda^2  \right]\\
					&=& E\left[\left( \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left\{\delta - \hat{\delta}(D)\right\}\right)^2 \right]\\
					&=& E\left[\left\{\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \right\}^2\right] + E\left[\left(\omega' \left\{\delta - \hat{\delta}(D)\right\}\right)^2 \right]\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2E \left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=& E\left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M M' J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\right]\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; + E\left[\omega' \{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \omega\right]\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2E \left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=&\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} E\left[M M' \right]J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; +\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1}E \left[ M\right]E\left[ \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=&\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} J_{00}J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; +\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
		&=&\tau_0^2+\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
	&=& \tau^2_0 + R(\delta)
\end{eqnarray*}
Where we have used the fact that $E[M]=0$ and hence $Var[M] = E[MM']$ and the independence of $M$ and $D$ and hence of any measurable functions thereof.
\end{proof}

\paragraph{IMPORTANT:} For convenience, define $\Lambda_0 = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M$. The \emph{key point} here is that the distribution of
	$$\Lambda = \sum_{S}c(S|D)\Lambda_S = \Lambda_0 + \omega'\{\delta - \hat{\delta}(D) \}$$
is often \textbf{dramatically non-normal}. To find the density of $\Lambda$, first condition on $D$ using the result from above:
	\begin{eqnarray*}
		\Lambda|(D=x) &=& \Lambda_0 + \omega'\{\delta - \hat{\delta}(x) \}\\
		 &\sim& \mathcal{N}(0,\tau_0^2) +  \omega'\{\delta - \hat{\delta}(x) \}
\end{eqnarray*}
Now, let $h(z)$ denote the density of $\Lambda$. We can calculate $h$ by integrating $D$ out of the joint density of $(\Lambda,D)$. Let $f(x)$ denote the density of $D$. We have
	$$
	\left\{\begin{array}{l}
		h(z|D=x) \sim \mathcal{N}\left( \omega'\{\delta - \hat{\delta}(x) \} , \tau_0^2\right)\\
		D = \delta + W \sim \mathcal{N}_q(\delta,K)
\end{array}\right.
$$
Now factor the joint density according to $h(z|D=x)f(x)$ and integrate out $D$ as follows:
	$$
	h(z) = \int h(z|D=x)f(x)\; dx
$$
We can then substitute the two normal distributions and then either numerically integrate or simulate. \emph{Notice}, however, that \textbf{the result depends on the unknown constant} $\delta$. 



\paragraph{Using the Full Model Variance} One approach to constructing a confidence interval that takes account of model selection uncertainty is to essentially use the variance of the full model. Define
	\begin{eqnarray*}
		\tau_{full}^2 &=& \mbox{AVAR}(\hat{\mu}_{full})= Var[\Lambda_{full}]\\
			&=&  \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0) + \omega'K^{1/2}H_{full} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2}H_{full} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2}\left\{ K^{-1/2}(\pi_{full}' K_{full} \pi_{full})  K^{-1/2}  \right\} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2} K^{-1/2} K K^{-1/2} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega' K \omega
\end{eqnarray*}
And accordingly $\tau_{full} = (\tau_0^2 + \omega' K \omega)^{1/2}$. Now let $\hat{\omega}$ be a consistent estimator of $\omega$ and $\widehat{\kappa}$ be a consistent estimator of $\tau_{full}$. Define
	$$T_n =\slfrac{\left[\sqrt{n}(\hat{\mu} - \mu_{true})  - \widehat{\omega}'\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \right]}{\widehat{\kappa}}$$
 From above, we know that the following converges jointly in distribution:
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		D_n
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}\\
		D
\end{array}\right] 	
$$
Thus
	\begin{eqnarray*}	
T_n &\overset{d}{\rightarrow}& \slfrac{\left[\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}  - \omega'\left\{D -  \sum_{S\in \mathcal{A}} c(S|D)G_S D \right\} \right]}{\tau_{full}}\\ \\
		&=& (\tau_0^2 + \omega' K \omega)^{-1/2}\left[\Lambda_0 + \omega'\left( \delta - D\right)\right] 
\end{eqnarray*}
We know from above that
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&K
	\end{array}\right]\right)
$$
where $K = J^{11}$, so
	\begin{eqnarray*}
		\Lambda_0 &\equiv& \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M\\
			&\sim&  \mathcal{N}\left(0, \tau_0^2\right)
\end{eqnarray*}
independently of
	\begin{eqnarray*}
		\omega'\left( \delta - D\right)&\equiv&- \omega'W\\
			&\sim& \mathcal{N}\left(0, \omega'K\omega \right)
\end{eqnarray*}
Therefore
	\begin{eqnarray*}
		T_n &\overset{d}{\rightarrow}& (\tau_0^2 + \omega' K \omega)^{-1/2}\left[\Lambda_0 + \omega'\left( \delta - D\right)\right] \\
		&=& (\tau_0^2 + \omega' K \omega)^{-1/2} \times \mathcal{N}\left(0, \tau_0^2 + \omega'K\omega\right)\\
		&\sim& \mathcal{N}\left(0,1 \right)
\end{eqnarray*}
which is a \textbf{standard normal}! We can use this result to create a approximate confidence interval for $\hat{\mu}$ as follows. For large $n$, 
	$$T_n =\widehat{\kappa}^{-1}\left[\sqrt{n}(\hat{\mu} - \mu_{true})  - \omega'\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \right] \approx \mathcal{N}(0,1)$$
To create a $(1-\alpha)\times 100\%$ interval, define $z_{\alpha/2}$ as the appropriate quantile of a standard normal random variable so that
	$$P \left[-z_{\alpha/2} \leq T_n \leq z_{\alpha/2} \right] \approx 1-\alpha $$
Then,
	\begin{eqnarray*}
		1-\alpha&\approx&P \left[-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq (\hat{\mu} - \mu_{true})  - \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \geq (\mu_{true} - \hat{\mu})  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \geq -\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\ \\
	&=&P \left[-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq (\mu_{true} - \hat{\mu})  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq \mu_{true}  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq \hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq \mu_{true}  + \Delta_n\leq \hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\left(\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}}\right) - \Delta_n \leq \mu_{true}  \leq \left(\hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}}\right) - \Delta_n \right]
\end{eqnarray*}
where
	$$\Delta_n \equiv \frac{\widehat{\omega}'}{\sqrt{n}}\left[D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right]$$
According to Claeskens and Hjort: ``this method is first-order equivalent to using the full model for confidence interval construction, with a modification for location.''




\paragraph{Correcting Confidence Intervals: Simulation} Another possibility is to simulate from the limiting distribution of $\Lambda$ \emph{for a range fixed value of} $\delta$, using consistent estimates of all other unknown quantities. This procedure can then be repeated for a variety of choices of $\delta$. To make this clearer, first rewrite $\Lambda$ using an expression from the proof Theorem 4.1
	\begin{eqnarray*}
		\Lambda &=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'K^{1/2}\left[ \sum_S c(S|D)H_S\right] K^{-1/2}D \\
			&=& \Lambda_0 + \omega' \left[ \delta - K^{1/2} \sum_S c(S|D)H_S K^{-1/2}D \right]\\
			&=& \Lambda_0 + \omega' \left[ \delta - \sum_S c(S|D)K^{1/2} H_S K^{-1/2}D \right]\\
			&=& \Lambda_0 + \omega' \left[ \delta - \sum_S c(S|D)G_S D \right]
	\end{eqnarray*}
where we have defined $G_S = K^{1/2} H_S K^{-1/2}$. We know from above  that $\Lambda_0 \sim \mathcal{N}(0,\tau_0^2)$ independent of $D\sim \mathcal{N}_q(\delta,K)$. The simulation procedure is as follows:
		\begin{enumerate}
			\item Calculate consistent estimates of $G_S$, $\tau_0^2$, $\omega$ and $K$ using the estimation results and fix a value of $\delta$.
			\item Using the result of step one, generate:
				\begin{enumerate}
					\item $\Lambda_{0,j}\sim\mathcal{N}(0, \hat{\tau}^{2}_0)$ independently of
					\item  $D_j \sim \mathcal{N}_q(\delta, \hat{K})$
				\end{enumerate}
			\item Calculate the weights $c$ using $D_j$ and set 
				$$\Lambda_j = \Lambda_{0,j} + \hat{\omega}' \left[ \delta - \sum_S c(S|D_j)\hat{G}_S D_j \right]$$
		\item Repeat steps 1 and 2 for $j= 1, 2, \hdots, B$
		\item Using the samples $\{\Lambda_1, \Lambda_2, \hdots, \Lambda_B\}$ generated in steps 3 and 4, calculate quantiles $a(\delta)$ and $b(\delta)$ that satisfy:
			$$P\left[ a(\delta) \leq \Lambda(\delta) \leq b(\delta) \right]= 0.95$$
		\item Repeat steps 2--5 for varying choices of $\delta$.
		\end{enumerate}
Now, suppose we know the values $[a(\delta), b(\delta)]$. Since $\Lambda$ is the limit distribution of $\sqrt{n}(\hat{\mu} - \mu_{true})$, it follows that
	$$P\left[a(\delta)\leq \sqrt{n}(\hat{\mu} - \mu_{true}) \leq b(\delta)  \right]\rightarrow P\left[a(\delta)\leq \Lambda(\delta) \leq b(\delta)  \right] = 0.95$$
Thus, $\left[\hat{\mu} - b(\delta)/\sqrt{n}, \hat{\mu} - a(\delta)/\sqrt{n}  \right]$ covers $\mu_{true}$ with probability $0.95$ asymptotically.

\paragraph{But We Don't Know $\delta$!} A naive approach would be to substitute our estimate $D_n = \hat{\delta}_{full} = \sqrt{n}(\hat{\gamma}_{full} - \gamma_0)$, carrying about the above simulations at this value and creating an interval based on $\hat{a} = a(D_n)$ and $\hat{b}=b(D_n)$. This is simple, but may not always work well. Let $p_n(\delta)$ be the coverage probability for this procedure. Its limit is
	$$p(\delta) = P\left[a(D)\leq \Lambda(\delta) \leq b(D)  \right]$$
which can be simulated by the method described above. It turns out that this method sometimes gives coverage that is \emph{far too low}.

\paragraph{A Better Procedure:} Instead of simply substituting $D_n$ for $\delta$ in the simulation described above, we could first construct a confidence region for $\delta$ and use this region to create an interval for $\hat{\mu}$. Since 
	$$D_n \overset{d}{\rightarrow} D = \delta + W \sim \mathcal{N}_q\left(\delta,  K\right)$$
where $K = J^{11}$, we have
	$$\left(D_n - \delta\right)' \widehat{K}^{-1}\left(D_n - \delta\right) \overset{d}{\rightarrow} \chi^2_q$$
Now, define 
	$$\rho_n(D_n,\delta) =\left[\left(D_n - \delta\right)' \widehat{K}^{-1}\left(D_n - \delta\right)\right]^{1/2}$$
and the event
	$$A_n(c) = \left\{\rho_n(D_n,\delta) \leq c  \right\}$$
Now, since $\rho_n(D_n, \delta)^2 \approx \chi^2_q$ we have
		$$P\left\{A_n(c)\right\}=P\left\{\rho_n(D_n,\delta) \leq c  \right\} = P\left\{\rho_n(D_n,\delta)^2 \leq c^2  \right\} \approx P\left\{\chi^2_q \leq c^2  \right\} 
$$
where we have used the fact that $x^2$ is strictly increasing for $x\geq0$ and that $\rho_n \geq 0$. Now define $z = (\chi^2_{q,0.95})^{1/2}$ and $A_n = A_n(z)$, so that $P\left\{A_n  \right\}\approx 0.95$. In the simulations described above in which we assumed that $\delta$ was known, we defined $a(\delta)$ and $b(\delta)$ so that
	$$P\left[ a(\delta) \leq \Lambda(\delta) \leq b(\delta) \right]= 0.95$$
Now, let
	\begin{eqnarray*}
		\widehat{a}_0(D_n)&=& \min \left\{a(\delta)\colon \rho_n(D_n, \delta) \leq z\right\}\\
		\widehat{b}_0(D_n)&=& \max \left\{b(\delta)\colon \rho_n(D_n, \delta) \leq z\right\}
\end{eqnarray*}
The claim is that the limit coverage level of
	$$\mbox{CI}_n^* = \left[\widehat{\mu} - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} , \;\;\; \widehat{\mu} - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]$$
is always \emph{above} 0.90, resulting in a conservative procedure. To see why this is the case, we return to the limit experiment, in which we have joint convergence of all the necessary random variables, as described above. This implies that the coverage probability $r_n(\delta)$ to which $\{\mu_{true}\in \mbox{CI}_n^*\}$ converges is given by
	$$r(\delta) = P\left\{ a_0(D) \leq \Lambda(\delta) \leq b_0(D)\right\}$$
where $\rho(D,\delta)^2 = \left(D - \delta\right)' K^{-1}\left(D- \delta \right)$ and
		\begin{eqnarray*}
		a_0(D)&=&\min \left\{a(\delta)\colon \rho(D, \delta) \leq z\right\}\\
		b_0(D)&=&\min \left\{b(\delta)\colon \rho(D, \delta) \leq z\right\}
\end{eqnarray*}
How does this work? The interval $\mbox{CI}_n^*$ is based on 
		\begin{eqnarray*}	
0.9&\leq&P\left[ \widehat{\mu} - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} \leq \mu_{true}\leq \widehat{\mu} - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]\\\\
		&=& P\left[  - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} \leq \mu_{true} - \widehat{\mu}\leq  - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]\\\\
		&=& P\left[  - \widehat{b}_0(D_n) \leq \sqrt{n}\left(\mu_{true} - \widehat{\mu}\right)\leq  - \widehat{a}_0(D_n) \right]\\
		&=& P\left[ \widehat{b}_0(D_n) \geq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\geq  \widehat{a}_0(D_n) \right]\\
		&=& P\left[ \widehat{a}_0(D_n) \leq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\leq  \widehat{b}_0(D_n) \right]
\end{eqnarray*}
Now, we know that
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		D_n
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}\\
		D
\end{array}\right] 	
$$
so, by the Continuous Mapping Theorem,
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		\rho_n(D_n,\delta)\\
		\widehat{a}_0(D_n)\\
		\widehat{b}_0(D_n)
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda(\delta)\\
		\rho(D,\delta)\\
		a_0(D)\\
		b_0(D)
\end{array}\right] 	
$$
and thus, 
	$$P\left[ \widehat{a}_0(D_n) \leq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\leq  \widehat{b}_0(D_n) \right] \rightarrow P\left\{ a_0(D) \leq \Lambda(\delta) \leq b_0(D)\right\} = r(\delta)$$
Now, let $A = \{  \rho(D,\delta)\leq z\}$ where, as before, $z = (\chi^2_{q,0.95})^{1/2}$ implying that $P\left\{A  \right\}=0.95$. Then,
	\begin{eqnarray*}
		0.95 &=& P\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\\
			&=& P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right] + P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right] 
\end{eqnarray*}
Now, since
		\begin{eqnarray*}
		a_0(D)&=&\min \left\{a(\delta)\colon \rho(D, \delta) \leq z\right\}\\
		b_0(D)&=&\min \left\{b(\delta)\colon \rho(D, \delta) \leq z\right\}
\end{eqnarray*}
we have
$$\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \Rightarrow \left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\}$$
and hence
	$$P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right]  \leq P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\}$$
Further, since
	$$\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \Rightarrow A^c$$
we have
	$$P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right]  \leq P(A^c)$$
Combining:
		\begin{eqnarray*}
		0.95 &=& P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right] + P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right] \\
			&\leq&  P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} + P(A^c)\\
			&=& P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} + 0.05
\end{eqnarray*}
since $A$ is defined with reference to a $95\%$ confidence interval. Subtracting, 
	$$P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} \geq 0.90$$
as claimed.

Here's the intuition for what just happened. $\Lambda$ is a random variable whose distribution depends on the unknown constant $\delta$. The constants $a(\delta)$ and $b(\delta)$ are quantiles of the distribution of $\Lambda$ such that 
	$$P\left[a(\delta) \leq \Lambda(\delta) \leq b(\delta)\right] = 0.95$$
Since $\Lambda$ depends on $\delta$, so do its quantiles: different values of $\delta$ would result in different intervals. 

This is the procedure. First we use $\rho(D,\delta)\leq z$ to get a confidence interval for $\delta$. Then we plug each point in this interval for $\delta$ into $\Lambda(\delta)$ and calculate the corresponding bounds $a(\delta)$ and $b(\delta)$. For each value of $\delta$ such that $\rho(D,\delta)\leq z$ we get a \emph{different} confidence interval for $\Lambda$. The lower bound of all these intervals is $a_0(D)$ while the upper bound is $b_0(D)$. The point here is to assess the coverage of the resulting interval.

The confusion here comes from bad notation: sometimes $\delta$ is being treated as fixed, other times as variable. Need to come up with some clearer notation...














\section{Schorfheide (2005)}




\end{document}