
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption} 
\newtheorem{pro}{Proposition} 
\newtheorem{lem}{Lemma} 
\newtheorem{thm}{Theorem} 
\newtheorem{cor}{Corollary} 
\newtheorem{ineq}{Inequality} 
\newtheorem{defn}{Definition} 
\newtheorem{rem}{Remark} 
\newtheorem{ex}{Example} 
\theoremstyle{definition}
\newtheorem{alg}{Algorithm} 


\newcommand{\slfrac}[2]{\left.#1\right/#2}
%\newcommand{\p}{\mathbb{P}}



\linespread{1.3}

\begin{document}

\title{Lecture 5: ``Focused'' Model Selection}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Local Mis-specification}

\subsection{Introduction}
In this lecture we'll be using a kind of asymptotic thought experiment that may be unfamiliar to you, so I'd like to spend a bit of time motivating it before proceeding. Roughly speaking, the idea is to consider a parameter whose value \emph{changes with sample size}. This basic idea is widely used in econometrics and statistics and is known by several different names. Among them are ``local alternatives,'' ``Pitman Drift,'' and ``local mis-specification.'' Although it may seem strange at first, ``drifting parameters'' are actually the natural asymptotic setting for certain problems, as I hope to convince you with the following two simple examples.

\subsection{What's Wrong with Asymptotic Power?}
Consider the following simple testing problem. Suppose we observe $N$ observations from the following DGP
$$X_1, X_2, \hdots, X_{N} \overset{iid}{\sim} \mathcal{N}(\mu, 1)$$
and want to test $H_0\colon \mu = 0$ against the one-sided alternative $H_1\colon \mu >0$. In this admittedly very simple example, the obvious test statistic is 
	$$T_{N} = \sqrt{N} \bar{X}_{N} \sim N\left(\mu \sqrt{N}, 1\right)$$
where $\bar{X}_{N}$ is the sample mean. We reject when $\sqrt{N} \bar{X}_{N}>z_{1-\alpha}$ where $z_{1-\alpha}$ is the $1-\alpha$ quantile of a standard normal distribution. We can calculate the power of this test as follows:
\begin{eqnarray*}
\mbox{Power}(T_{N}) &=& P\left(\sqrt{N} \bar{X}_{2N}>z_{1-\alpha}\right) = P\left(Z + \mu\sqrt{N} >z_{1-\alpha}\right)\\
	&=&P\left(Z >z_{1-\alpha} - \mu\sqrt{N}\right) = 1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{2N}\right)
\end{eqnarray*}
where $Z$ is a standard normal random variable and $\Phi$ is the corresponding CDF. Now suppose we decided to do something completely crazy: throw away half our sample. Let $\bar{X}_{N/2}$ denote the sample mean based on observations $1, 2, \hdots, \lfloor N/2 \rfloor $ \emph{only}. We can still construct a perfectly valid test with size $\alpha$ as follows. Define
	$$T_{N/2} = \sqrt{\lfloor N/2 \rfloor } \bar{X}_N \sim N\left(\mu \sqrt{\lfloor N/2 \rfloor }, 1\right)$$
and reject if $\sqrt{N} \bar{X}_N > z_{1-\alpha}$. But there's an obvious problem here: there \emph{must} be a cost for throwing away perfectly good data. Indeed, if we calculate the power for this crazy test, we'll find that it's \emph{strictly lower} than that of the sensible test based on the full sample. In particular,
	$$\mbox{Power}(T_{N/2}) = 1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{\lfloor N/2 \rfloor }\right)$$
using the same argument as above with $\lfloor N/2 \rfloor $ in place of $N$.  

Now, for an example this simple we'd never resort to asymptotics, but suppose we did. How do these two tests compare as the sample size goes to infinity? The asymptotic size in this example is the same as the finite-sample size since we know the exact sampling distribution of the test statistics under the null and neither depends on sample size. But what about the power? We have,
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{N}\right) \right] = 1\\
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N/2}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \mu\sqrt{\lfloor N/2 \rfloor }\right) \right] = 1
\end{eqnarray*}
In other words, both of these tests are \emph{consistent}: as the sample size goes to infinity, the power goes to one. Think about this for a moment: we know that for \emph{any} fixed sample size a test based on the full sample is \emph{strictly more powerful} but in the limit this difference disappears. This strongly suggests that something is wrong with our asymptotic thought experiment in this setting.

You might object that I've cooked up a particularly perverse example, but it turns out that this phenomenon is quite general. It's easy to find consistent tests, in fact it's difficult to find tests that \emph{aren't} consistent. But we know from simulation studies that not all consistent tests are created equal: some have \emph{much} better finite sample power than others. One way around this problem would be to only compare the finite-sample properties of different tests and never use asymptotics. But we almost \emph{never} know the exact sampling distribution of our test statistics. 

This is where \emph{local alternatives} come in. Rather than evaluating our tests against a \emph{fixed} alternative $\mu$, suppose we were to evaulate it against a \emph{sequence} of \emph{local} alternatives that \emph{drift towards the null} at rate $N^{-1/2}$. In other words, our alternative becomes $H_1 \colon \mu = \delta / \sqrt{N}$ where, for this one-sided test, $\delta > 0$. If we substitute $\delta/\sqrt{N}$ for $\mu$ and take the limit as $N\rightarrow \infty$, we find
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{N}}\sqrt{N}\right) \right]\\
	 &=& 1 - \Phi\left(z_{1-\alpha} - \delta \right)
\end{eqnarray*}
and similarly
\begin{eqnarray*}
	\lim_{N\rightarrow \infty} \mbox{Power}(T_{N/2}) &=& \lim_{N\rightarrow \infty}\left[1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{N}}\sqrt{\lfloor N/2 \rfloor }\right) \right]\\
	 &=& 1 - \Phi\left(z_{1-\alpha} - \frac{\delta}{\sqrt{2}} \right)
\end{eqnarray*}
Wow! Our problem has disappeared! The asymptotic power of the two tests now differs in essentially the same way as the finite sample power. Also note that the power no longer converges to one. Intuitively, this is because the drifting sequence of alternatives $\delta/\sqrt{n}$ makes it ``harder and harder'' to reject the null as the sample size grows by shrinking \emph{just fast enough} but not so fast that the power goes to zero. This type of calculation is called a \emph{local power analysis}. A test that has asymptotic power greater than zero in such a setting is said to have ``power against local alternatives.''


\subsection{Weak Identification}
Drifting parameter sequences of the kind described above are also used in the weak instruments and weak identification literature. 
\todo[inline]{Possibly add a simple example later.}

\subsection{A Bias-Variance Tradeoff in the Limit}
When we derived Mallow's $C_p$, the idea was to compare models on the basis of predictive mean-squared error. Bigger models generally have a lower bias but a higher variance because there are more parameters to estimate. In the example we considered in class, everything was linear and we made enough assumptions about the finite sample distribution that we could deduce the \emph{exact} MSE conditional on $X$. In many settings, however, finite sample results unavailable and we are forced to rely on asymptotic approximations. We know there is a tradeoff between bias and variance in the finite sample and we'd like to capture this idea in our limit results. The question is how?

Suppose that $\widehat{\mu}$ is a \emph{potentially biased} estimator of $\mu$. Then we have 
	$$MSE(\widehat{\mu}) = E[(\widehat{\mu} - \mu)^2] = \left(E[\widehat{\mu} - \mu]\right)^2 + Var(\widehat{\mu})$$
Now, if we don't know the finite sample distribution of $\widehat{\mu}$, we can't calculate the proceeding expression. So what can we do instead? If $\widehat{\mu}$ is asymptotically normal, then we might try to use the features of its limit distribution to calculate the \emph{asymptotic} mean-squared error and use this to as a ``stand-in'' for the exact, finite-sample quantity. Let $\mu_0$ be the probability limit of $\widehat{\mu}$ and $\mu$ be the ``true'' parameter value. Suppose that 
	$$\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2)$$
In maximum likelihood estimation, $\mu_0$ would be the pseudo-true value that minimizes the KL divergence and $\sigma^2$ would be a diagonal element of $J^{-1}KJ^{-1}$. Now, an obvious idea is estimate $Var(\widehat{\mu})$ using the \emph{asymptotic variance}, namely $\mbox{AVAR}(\widehat{\mu}) = \sigma^2$. But what about the bias term $E[\widehat{\mu} - \mu]$? The limit distribution of $\widehat{\mu}$ is centered around $\mu_0$, the pseudo-true value, but we need to evaluate the bias relative to $\mu$. Let's try recentering by adding and subtracting $\sqrt{T}\mu$ as follows:
\begin{eqnarray*}
	\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) &=& \sqrt{T}\widehat{\mu} - \sqrt{T} \mu_0\\
	&=& \sqrt{T}\widehat{\mu} - \sqrt{T} \mu_0 - \sqrt{T} \mu + \sqrt{T} \mu\\
	&=& \sqrt{T}\left( \widehat{\mu} - \mu\right) + \sqrt{T}\left(\mu - \mu_0 \right)
\end{eqnarray*}
Rearranging, we can write
	$$\sqrt{T}\left( \widehat{\mu} - \mu\right) = \sqrt{T}\left(\widehat{\mu} - \mu \right) - \sqrt{T}\left(\mu - \mu_0 \right)$$
Now we have an expression for $\widehat{\mu}$ centered around $\mu$, so the obvious thing to do is look at the mean of the limiting distribution of $\sqrt{T}\left( \widehat{\mu} - \mu\right) $ and call this the ``asymptotic bias.'' Unfortunately, we have a problem. By assumption, the first term $\sqrt{T}\left(\widehat{\mu} - \mu_0 \right)$ is $O_p(1)$ but the second term \emph{diverges}! We recentered $\widehat{\mu}$ around $\mu$ \emph{precisely because} we thought that $\mu_0$ was potentially different from $\mu$. But if this is the case, then $ \sqrt{T}\left(\mu - \mu_0 \right) = O(T^{1/2})$. So what's going on here? The problem is that the asymptotic variance is of a \emph{different order} than the asymptotic bias. We need to scale $\widehat{\mu}$ up by $\sqrt{T}$ to get a result that has non-zero asymptotic variance, but this same scaling causes the bias to explode. In other words, there is no way to get a meaningful bias-variance tradeoff in the limit under conventional asymptotics.

So how can we fix this problem? Above we had $ \sqrt{T}\left(\mu - \mu_0\right) = O(T^{1/2})$ but what we want is $\sqrt{T}\left(\mu - \mu_0 \right) = O(1)$, so somehow or other we need to ensure that $\left(\mu - \mu_0 \right) = O(T^{-1/2})$. This is where local mis-specification makes its grand appearance. Suppose that we have a DGP under which the true parameter value is $\mu_T = \mu_0 + \delta/\sqrt{T}$ where $\delta$ is a constant. That is, suppose we assume that the true parameter value \emph{changes with sample size} and drifts towards $\mu_0$ at rate $T^{-1/2}$. This may sound like a crazy idea, but there's no arguing with the fact that it solves our problem. We have,
\begin{eqnarray*}
	\sqrt{T}\left( \widehat{\mu} - \mu_T\right) &=& \sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \sqrt{T}\left(\mu_T - \mu_0 \right)\\
		&=&\sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \sqrt{T}\left(\mu_0 + \delta/\sqrt{T} - \mu_0 \right)\\
		&=& \sqrt{T}\left(\widehat{\mu} - \mu_0 \right) - \delta\\
		&\overset{d}{\rightarrow}& \mathcal{N}(0, \sigma^2) - \delta
\end{eqnarray*}
hence, the asymptotic mean-squared error of $\widehat{\mu}$ is $\mbox{AMSE}(\widehat{\mu}) = \delta^2 + \sigma^2$. But what does it mean to have a parameter that changes with sample size? It's important to be clear that this does \emph{not} mean that we think real-world datasets follow a DGP that changes with sample size. This is a \emph{thought experiment}: we also don't believe that it's possible to have an infinite sample size! When we use asymptotics, the point is to derive tractable expressions that approximate the effects that actually occur in finite samples. We know that there is a bias-variance tradeoff in finite samples but we showed above that the conventional asymptotics can't capture this. In other words, local mis-specification is a \emph{device} to get a limiting theory that provides a better approximation to what's really going on in finite samples. For more on the sense in which local mis-specification provides a much more realistic portrait of the effects of model selection, see Leeb and P\"{o}tscher (2005).



\subsection{Triangular Array Asymptotics}
\todo[inline]{Put together a simple example (iid) showing what's going on here. Perhaps a little $L_2$ WLLN. Also, give the Lindeberg-Feller CLT. Point them to the Andrews papers that give technical conditions.}

\section{Focused Evaluation}
Best model for a \emph{particular purpose} rather than ``one-size-fits all.'' Once we acknowledge that are models are probably wrong, we should ask ``how wrong'' and it might depend on what we want to use the model for. More generally, it's not clear that we would want to use the true model \emph{even if we knew its form}: bias-variance tradeoff that comes from estimation uncertainty of more complicated models.

This example comes from Hansen (2005). Consider AR$(k)$ model
	$$y_t = \mu + \beta_1 y_{t-1} + \cdots + \beta_k y_{t-k} + \epsilon_t$$
where $\{\epsilon_t\}$ is a martingale difference sequence, that is $E[\epsilon_t|I_{t-1}] = 0$. We're interested in learning about a scalar ``focus parameter'' $\theta = g(\beta)$. This could be for example, one of the individual coefficients $\beta_j$, the long-run variance, or an impulse response at some specified horizon. The point is that it's a scalar and a \emph{function} of the underlying model parameters $\beta_1, \hdots, \beta_k$. So what constitutes a ``good'' model for learning about $\theta$? The natural way to proceed is to specify a loss function and try to find the estimator $\widehat{\theta}$ that minimizes the expectation of the loss. For this example we'll use mean-squared error and search for a model that minimizes $E[(\widehat{\theta} - \theta)^2]$ 

Hansen (2005) uses a simple simulation experiment to show that different focus parameters can lead to \emph{very different} selected models. The setup is as follows. We consider the family of AR$(k)$ models for $k = 0, 1, \hdots, k_{\mbox{max}}$ but the true DGP is in fact an ARMA(1,1) model, namely
	\begin{eqnarray*}
		y_t &=& \alpha y_{t-1} + \epsilon_t - \gamma \epsilon_{t-1}\\
		\epsilon_t &\sim& \mbox{iid}\; N(0,1)
	\end{eqnarray*}
Thus \emph{none} of the models under consideration is correctly specified since the true DGP can be expressed as an AR$(\infty)$ model. Now suppose we're interested in the impulse responses. A little algebra reveals that the true impulse responses for the DGP are 
	$$\theta_m = (\alpha -\gamma)\alpha^{m-1}$$
where $m$ denotes the horizon. The estimated impulse responses for the class of models we are considering can be calculated recursively from the estimated AR parameters. By simulating the DGP with $T=200$ for a range of parameter values $(\alpha,\gamma)$ Hansen (2005) shows that the optimal AR order for approximating the impulse response of the true DGP in a minumum mean-squared error sense is \emph{highly} sensitive to $m$, the horizon of interest. To take a particularly stark example, when $\alpha = 0.5$ and $\beta = 0.9$ the optimal AR order for $m=2$ is $k=10$ but the optimal AR order for $m=6$ is $k=0$. 

\subsection{The FIC for Hansen's (2005) Example}



\section{The Focused Information Criterion (FIC)}
Portable, like AIC and BIC, based on risk minimization, like FPE and Mallow's $C_p$, but \emph{focused} in the sense of Hansen (2005). Want to be able to select different models for different purposes. Turns out to be even \emph{more} portable than AIC and BIC: although originally derived in a likelihood framework, the idea behind the FIC can be easily extended to any situation in which it is possible to derive a limiting distribution. Indeed extending the idea behing the FIC idea to novel settings has been a major area of my research in the past few years! 

Although it has been extended in a number of ways, here I'll follow the notation and framework of the original two papers: Claeskens \& Hjort (2003) and Hjort \& Claeskens (2003). These papers appear in the same issue of JASA and the derivations and explanations are split between them. Can look at various loss functions, but the original papers use MSE so that's what we'll look at here. I'll talk about extensions below. Roughly speaking, the idea is to estimate a user-specified target parameter $\mu$ with minimum mean-square error. Since finite-sample MSE can only be calculated in very simple examples, the FIC uses an asymptotic MSE to approximate finite-sample behavior. As discussed above, this requires an asymptotic framework based on drifting sequences of parameters. 

\paragraph{Local Mis-specification Framework:}
Suppose $Y_1, \hdots, Y_n$ are independent with density
	$$f_{true}(y)=f(y, \theta_0, \gamma_0 + \delta/\sqrt{n})$$
This could be a regression model, in which case the likelihood is conditional on $x$ but we'll suppress this in the notation. The $p$-vector $\theta$ contains the ``protected parameters.'' These are the parameters that we have decided in advance we definitely want to estimate. In contrast, the $q$-vector $\gamma$ contains the parameters over which we will carry out model selection: we consider the restriction $\gamma = \gamma_0$ where $\gamma_0$ is a \emph{known} parameter. When we restrict a component of $\gamma$ we \emph{do not estimate it}: we simply substitute the restriction into the likelihood. In a linear regression problem, for example, we might have something like
	$$y_i = x_i'\theta + z_i'\gamma + \epsilon_i$$
and consider setting some or all of the elements of $\gamma$ equal to zero rather than estimating them.  The true value of $\gamma$ is \emph{changing with sample size} according to $\gamma_n = \gamma_0 + \delta/\sqrt{n}$ where $\delta$ is a fixed but unknown constant $q$-vector. Thus, any specification that does not estimate $\gamma$ is \emph{locally mis-specified} but the mis-specification disappears in the limit as $n\rightarrow \infty$. 

\paragraph{N.B.} There's something slightly awkward in the notation here: $\theta_0$ is the true value of $\theta$ but $\gamma_0$ is \emph{not} the true value of $\gamma$. It is only \emph{in the limit} that $\gamma = \gamma_0$. Unlike $\theta_0$, which is unknown, $\gamma_0$ is \emph{known} since it's the restriction we're considering. This is something the econometrician chooses based on the specifics of the problem at hand.

\paragraph{The Focus Parameter:} The FIC is not a specific model selection criterion. Instead it is a \emph{procedure} that allows the \emph{user} to create her own model selection criterion for a particular problem. Let $\mu = \mu(\theta, \gamma)$ be the user-specified parameter of interest. Under local mis-specification, the true value of $\mu$ is changing with sample size according to
	$$\mu_{\mbox{true}} = \mu\left(\theta_0, \gamma_0 + \delta/\sqrt{n}\right)$$ 
The goal is to estimate $\mu$ with minimum mean-squared error. But since we are considering general ML models, it's not possible to work out the exact finite-sample distributions of the various estimators. Instead, we calculate the \emph{asymptotic mean-squared error} (AMSE) of our estimators of $\mu$ and attempt to select a model to minimize this quantity. The key innovation here is that we are \emph{not} interested in $\gamma$ for its own sake: all that matters is how our modeling decisions about $\gamma$ affect our estimates of $\mu$.

\paragraph{Candidate Models:} Considered in full generality, we could restrict any number of components of $\gamma$. Since this parameter is $q$-dimensional, we could consider a total of $2^q$ candidate models if desired. Alternatively, we could decide to consider only particular groups of restrictions. The simplest case considers only two models: the \emph{wide} model estimates \emph{all} elements of $\gamma$ and the \emph{narrow} model estimates \emph{none} of the elements of gamma. However we choose to restrict the set of candidates, each model is indexed by $S$ which is a subset of $\{1, \hdots, q\}$ that indicates which elements of $\gamma$ we estimate. Its complement, $S^c$, indicates which elements of $\gamma$ we set equal to the corresponding elements of $\gamma_0$. Each candidate model $S$ implies a maximum likelihood estimator
for the underlying model parameters $\theta$ and $\gamma_S$, where $\gamma_S$ denotes the elements of $\gamma$ that are esetimated under model $S$. The corresponding ML estimator $\widehat{\mu}_S = \mu\left(\widehat{\mu}_S, \widehat{\gamma}_S \right)$ for the target parameter $\mu$
	$$\widehat{\mu}_S = \mu\left(\widehat{\theta}_S, \widehat{\gamma}_S, \gamma_{0,S^c} \right)$$
where $\gamma_{0,S^c}$ denotes a vector containing the elements of $\gamma_0$ whose indices are in $S^c$. These are the elements of $\gamma$ that are \emph{not estimated}. 

\paragraph{The ``Full'' Model} The \emph{full}, aka \emph{wide}, model is the specification in which we estimate all elements of $\gamma$. Under the local mis-specificiation assumption, this model is \emph{correctly specified}. Any model selection criterion relies on some form of over-identification to evaluate the quality of a candidate model relative to alternatives. In the FIC framework this is achieved by comparing the results of each candidate $S$ to those of the full model. We denote the \textbf{score function} of the full model by
	$$\left[\begin{array}{c}
		U(y)\\
		V(y)
\end{array} \right] = \left[\begin{array}{c}
		\nabla_\theta \log{f(y, \theta_0, \gamma_0)}\\
		\nabla_\gamma \log{f(y, \theta_0, \gamma_0)}
\end{array}\right]\;\;\begin{array}{c}
		(p\times 1)\\
		(q\times 1)
\end{array}$$
Note that the score is evaluated at the \emph{null point} $(\theta_0, \gamma_0)$. This is \emph{not} the true parameter vector for \emph{any finite sample size}, but it is the true parameter vector in the limit. Similarly, the \textbf{information matrix} of the full model by
$$J_{Full} = Var_0\left[\begin{array}{c}
		U(y)\\
		V(y)
\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01}\\
		J_{10} & J_{11}

	\end{array}\right]\begin{array}
	{cc} (p\times p) & (p\times q) \\
	(q\times p) & (q\times q)
\end{array}$$
where the the zero subscript indicates that the expectation is being taken with respect to the distribution in which $\gamma = \gamma_0$. This is the \emph{limiting} DGP which is \emph{different} from the DGP for any finite sample size under local mis-specification. We partition the inverset of the information matrix for the full model as follows 
	$$J_{Full}^{-1} = \left[\begin{array}{cc}
		J^{00} & J^{01}\\
		J^{10} & J^{11}
	\end{array}\right]$$
where
	$$K \equiv J^{11} = (J_{11} - J_{10}J_{00}^{-1}J_{01})^{-1}$$
by the partitioned matrix inverse formula. The quantity $J^{11}$ appears so frequently in the derivation of the FIC that it is called $K$ to keep the superscripts from getting out of control. 

\paragraph{Selection Matrices} In various matrix manipulations in the paper, it turns out to be helpful to define a matrix that \emph{selects} the elements of $\gamma$ that are estimated under model $S$. Let $\pi_S$ be the $|S|\times q$ matrix that ``selects'' only those elements of a $q$-vector that correspond to the indices in the set $S$. For example, suppose $q=3$ and $S = \{1,3\}$. Then,
	$$\pi_S = \left[\begin{array}
		{ccc} 1 & 0 & 0\\ 0 & 0 & 1
	\end{array} \right]$$
In this case $\gamma = (\gamma_1, \gamma_2, \gamma_3)'$ and $\pi_S \gamma = (\gamma_1, \gamma_3)'$. For the \emph{wide} or \emph{full} model, i.e.\ the model that estimates all components of $\gamma$, we have $S = \{1, \hdots, q\}$ and hence $\pi_S$ is simply the identity matrix of order $q$. An extremely useful fact about $\pi_S$ is that we can use it to transform the information matrix for the \emph{full} aka \emph{wide} model -- the model that estimates all components of $\gamma$ -- into the information matrix for a candidate model $S$ as follows:
	$$J_S = Var_0\left[\begin{array}{c}
		U(y)\\
		V_S(y)
\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01,S}\\
		J_{10,S} & J_{11,S}
	\end{array}\right]=\left[\begin{array}{cc}
		J_{00} & J_{01}\pi_S'\\
		\pi_S J_{10} & \pi_S J_{11} \pi_S'
	\end{array}\right]$$
By the partitioned matrix inverse formula:
\begin{eqnarray*}
	K_S \equiv J^{11,S} &=& \left(\pi_S K^{-1} \pi_S'\right)^{-1} = \left[\pi_S \left(J_{11} - J_{10}J_{00}^{-1}J_{01} \right) \pi_S'\right]^{-1} \\
	J^{01,S} &=& -J_{00}^{-1}J_{01}\pi_S'K_S\\
	J^{00,S} &=& J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}
\end{eqnarray*}
Again, the quantity $J^{11,S}$ appears so many times in the derivation of the FIC that it is called $K_S$ for short.

\paragraph{CLT for the Score of the Full Model} The first step in deriving the FIC is to calculate the limiting distribution of the score for the full model evaluated at $(\theta_0, \gamma_0)$. This appears as Lemma 3.1 in Hjort \& Claeskens (2003):
\begin{lem}[CLT for Score of Full Model]
Under local mis-specification,
	$$\left[\begin{array}{c}
		\frac{1}{\sqrt{n}}\sum_{i=1}^n U(Y_i)\\
		\frac{1}{\sqrt{n}}\sum_{i=1}^n V(Y_i)
	\end{array}\right] \overset{d}{\rightarrow}
	\left(\begin{array}{c}
		J_{01}\delta\\
		J_{11}\delta
	\end{array}\right) + 	
	\left(\begin{array}{c}
		M\\
		N
	\end{array}\right)$$
where
	$$\left(\begin{array}{c}
		M\\
		N
	\end{array}\right) \sim \mbox{N}_{p+q}(0, J_{Full})$$
\end{lem}
\begin{proof}
To prove this result, we apply the Lindeberg-Feller CLT	to the triangular array of random variables
	$$\left[\begin{array}
		{cc} U(Y_i)/\sqrt{n} \\ V(Y_i)/\sqrt{n}
	\end{array} \right]$$
Since the $Y_i$ are iid \emph{for fixed} $n$, we have
	\begin{eqnarray*}
		Var\sum_{i = 1}^n \left[\begin{array}
		{cc} U(Y_i)/\sqrt{n} \\ V(Y_i)/\sqrt{n}
	\end{array} \right] = Var_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] \rightarrow Var_0 \left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] = J_{full}
	\end{eqnarray*}
under appropriate regularity conditions. Thus, assuming the Lindeberg condition is satisfied, we have
$$\frac{1}{\sqrt{n}}\sum_{i = 1}^n \left(\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] - E_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] \right) \overset{d}{\rightarrow} \left[\begin{array}
		{c} M \\N
	\end{array}\right]$$
where $(M', N')' \sim  N_{p+q}\left(0,J_{full}\right)$. Again, since the $Y_i$ are iid for fixed $n$,
$$\frac{1}{\sqrt{n}}\sum_{i = 1}^n \left(\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] - E_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] \right) =\left(\frac{1}{\sqrt{n}}\sum_{i = 1}^n \left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] \right) - \sqrt{n}E_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] $$
And by a mean-value expansion around $\gamma_n = \gamma_0 + \delta/\sqrt{n}$,
\begin{eqnarray*}
	E_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] &=&  E_n\left[\begin{array}
		{cc} \nabla_\theta \log f(Y_i, \theta_0, \gamma_n) \\ \nabla_\gamma \log f(Y_i, \theta_0, \gamma_n)
	\end{array} \right] + E_n\left[\begin{array}
		{cc} \nabla_{\theta\gamma'} \log f(Y_i, \theta_0, \gamma^*) \\ \nabla_{\gamma \gamma'} \log f(Y_i, \theta_0, \gamma^*)\end{array} \right] (\gamma_0 - \gamma_n)
\end{eqnarray*}
where $\gamma^*$ is between $\gamma_0$ and $\gamma_n$. The first term is simply the population moment condition for ML estimation and hence equals zero: thanks to the mean-value expansion, the expectation is now evaluated at $(\theta_0,\gamma_n)$ which is the true parameter value for the DGP based on a sample size of $n$. Thus, since $\gamma_0 = \gamma_n = -\delta/\sqrt{n}$, we have
	$$\sqrt{n}E_n\left[\begin{array}
		{cc} U(Y_i) \\ V(Y_i)
	\end{array} \right] = -E_n\left[\begin{array}
		{cc} \nabla_{\theta\gamma'} \log f(Y_i, \theta_0, \gamma^*) \\ \nabla_{\gamma \gamma'} \log f(Y_i, \theta_0, \gamma^*)\end{array} \right] \delta \rightarrow -E_0\left[\begin{array}
		{cc} \nabla_{\theta\gamma'} \log f(Y_i, \theta_0, \gamma_0) \\ \nabla_{\gamma \gamma'} \log f(Y_i, \theta_0, \gamma_0)\end{array} \right] \delta$$
under appropriate regularity conditions. Recall that, in the limit, $(\theta_0, \gamma_0)$ are the \emph{true} parameter values. Hence, 
$$-E_0\left[\begin{array}
		{cc} \nabla_{\theta\gamma'} \log f(Y_i, \theta_0, \gamma_0) \\ \nabla_{\gamma \gamma'} \log f(Y_i, \theta_0, \gamma_0)\end{array} \right] = \left[\begin{array}
			{c} J_{01} \\ J_{11}
		\end{array} \right]$$
by the information matrix equality, yielding the desired result.
\end{proof}


\paragraph{Asymptotic Normality of the Estimators}
The next step in the derivation of the FIC is to 
\paragraph{Lemma 3.2}
	$$
	\left[\begin{array}{c}
		\sqrt{n} (\hat{\theta} - \theta_0)\\
		\sqrt{n} (\hat{\gamma} - \gamma_0)
\end{array}\right]\overset{d}{\rightarrow} 
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = J_S^{-1}
	\left(\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right) \sim \mathcal{N}_{p+|S|}
		\left(J_S^{-1}\left[\begin{array}{c}
			J_{01}\\
			\pi_S J_{11}
	\end{array}\right]\delta, J_S^{-1}\right)
$$
\paragraph{First Claim:} Define $W\equiv J^{10}M+J^{11}N$. Then, $W= K(N - J_{10}J_{00}^{-1}M)$ and $M$ and $W$ are indep.\ with $W\sim \mathcal{N}_q(0,K)$ and $M\sim\mathcal{N}_p(0,J_{00})$.
\begin{proof}
By the formula for the inverse of a partitioned matrix,
	\begin{eqnarray*}
		J^{11} &=&\left(J_{11} - J_{10}J_{00}^{-1}J_{01}\right)^{-1}\\
		J^{01} &=&-J_{00}^{-1}J_{01}J^{11}\\
		J^{10} &=&-J^{11}J_{10}J_{00}^{-1}\\
		J^{00} &=& J_{00}^{-1} + J_{00}^{-1}J_{01}J^{11}J_{10}J_{00}^{-1}
	\end{eqnarray*}
Thus,
	\begin{eqnarray*}
		W \equiv J^{10}M + J^{11}N &=& \left(-J^{11}J_{10}J_{00}^{-1}\right)M + J^{11}N\\
			&=&J^{11}\left( N - J_{10}J_{00}^{-1}M \right)\\
			&=&K\left( N - J_{10}J_{00}^{-1}M \right)
	\end{eqnarray*}
as required. Now we need to show the independence of $W$ and $M$. Write
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] = \left[\begin{array}{c}
		M\\
		J^{10}M + J^{11}N
	\end{array}\right] = \left[\begin{array}{cc}
		I_p&0_{p\times q}\\
		J^{10}&J^{11}
	\end{array}\right]\left[\begin{array}{c}
		M\\
		N
	\end{array}\right]\equiv A \left[\begin{array}{c}
		M\\
		N
	\end{array}\right]
$$
Since $\left[\begin{array}{c} M\\ N \end{array}\right]\sim \mathcal{N}_{p+q}(0, J_{Full})$, we have $A\left[\begin{array}{c} M\\ N \end{array}\right]\sim \mathcal{N}_{p+q}(0, A J_{Full}A')$. Multiplying through, we find that
	$$
	AJ_{Full}A' = \left[\begin{array}{cc}
		J_{00}& J_{00}J^{01}+J_{01}J^{11}\\
		J^{10}J_{00}+J^{11}J_{10}& J^{10}\left(J_{00}J^{01}+J_{01}J^{11}\right) + J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right)
	\end{array}\right]
$$
Now,
	\begin{eqnarray*}
		 J_{00}J^{01}+J_{01}J^{11} &=& J_{00}\left(-J_{00}^{-1}J_{01}J^{11}\right)+J_{01}J^{11} \\
			&=&  -J_{01}J^{11}+J_{01}J^{11} = 0 
	\end{eqnarray*}
and similarly
	\begin{eqnarray*}
		J^{10}J_{00}+J^{11}J_{10} &=& \left(-J^{11}J_{10}J_{00}^{-1}\right)J_{00}+J^{11}J_{10}\\
			&=& -J^{11}J_{10}+J^{11}J_{10} = 0
\end{eqnarray*}
Finally,
	\begin{eqnarray*}
		J^{10}\left(J_{00}J^{01}+J_{01}J^{11}\right) + J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right) &=& J^{11}\left(J_{10}J^{01}+J_{11}J^{11}\right)\\
		&=&J^{11}\left(J_{10}\left[-J_{00}^{-1}J_{01}J^{11}\right]+J_{11}J^{11}\right)\\
		&=&J^{11}\left(J_{11}-J_{10}J_{00}^{-1}J_{01}\right)J^{11}\\
		&=&J^{11}\left(J_{11}\right)^{-1}J^{11}=J^{11}
\end{eqnarray*}
where the first equality uses the fact that $J_{00}J^{01}+J_{01}J^{11} =0$. Therefore
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&J^{11}
	\end{array}\right]\right)
$$
Thus, $W\sim \mathcal{N}_q(0,J^{11})$ independent of $M\sim\mathcal{N}_p(0,J_{00})$.
\end{proof}


\paragraph{Second Claim:} Lemma 3.2 and some algebra imply that
	$$\hat{\delta}_S \equiv \sqrt{n}(\hat{\gamma}_S - \gamma_{0,S})\overset{d}{\rightarrow} D_S$$
where $D_S = K_S \pi_s K^{-1}(\delta + W) = K_S \pi_s K^{-1}D$, defining $D = \delta + W$. In particular:
	$$
	D_N \equiv \hat{\delta}_{Full} = \sqrt{n}(\hat{\gamma}_{Full} -\gamma_0) \overset{d}{\rightarrow} D = (\delta+W) \sim \mathcal{N}_q(\delta,K)
$$
where, as before, $K \equiv J^{11}$.
\begin{proof}
Lemma 3.2 establishes that
	$$
	\left[\begin{array}{c}
		\sqrt{n} (\hat{\theta} - \theta_0)\\
		\sqrt{n} (\hat{\gamma} - \gamma_0)
\end{array}\right]\overset{d}{\rightarrow} 
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = J_S^{-1}
	\left(\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right)$$
so we know immediately that $\hat{\delta}_S \equiv \sqrt{n} (\hat{\gamma}_S - \gamma_{0,S})\overset{d}{\rightarrow} D_S$. We need to show that $D_S = K_S\pi_S K^{-1}D$ where $D = \delta + W$. We have:
	\begin{eqnarray*}
		\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] &=& J_S^{-1}
	\left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] = \left[\begin{array}{cc}
				J^{00,S}&J^{01,S}\\
				J^{10,S}&J^{11,S}	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] \\ \\
	&=& \left[\begin{array}{cc}
				J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}&-J_{00}^{-1}J_{01}\pi_S'K_S\\
				-K_S \pi_S J_{10}J_{00}^{-1}&K_S	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] 
	\end{eqnarray*}
where $K_S = \left(\pi_S K^{-1} \pi_S'\right)^{-1}$ and $K\equiv J^{11}$. Thus, we have
	\begin{eqnarray*}
		D_S &=& -K_S \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) + K_S \left(\pi_S J_{11}\delta + N_S \right)\\
			&=& K_S \left[ \left(\pi_S J_{11}\delta + N_S \right) - \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) \right]\\
			&=& K_S \left[ \pi_S J_{11}\delta + \pi_S N  - \pi_S J_{10}J_{00}^{-1}\left(J_{01}\delta + M\right) \right]\\
			&=& K_S \pi_S \left[ \left(J_{11} - J_{10}J_{00}^{-1}J_{01}\right)\delta +  N  -  J_{10}J_{00}^{-1} M \right]\\
		&=& K_S \pi_S \left[ K^{-1}\delta +K^{-1}K  \left(N  -  J_{10}J_{00}^{-1} M \right)\right]\\
		&=& K_S \pi_S K^{-1} \left[ \delta +K  \left(N  -  J_{10}J_{00}^{-1} M \right)\right]\\
		&=& K_S \pi_S K^{-1} \left( \delta + W\right)
	\end{eqnarray*}
\end{proof}


\paragraph{More Notation:} 
	\begin{eqnarray*}
		H_S &\equiv& \left(K^{-1}\right)^{1/2}(\pi_S' K_S \pi_S) \left( K^{-1} \right)^{1/2} \\
		\omega &\equiv& J_{10}J_{00}^{-1} \nabla_\theta \mu(\theta_0,\gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0) 
	\end{eqnarray*}
Notice that:
	\begin{enumerate}
		\item $\omega$ depends on the choice of focus parameter $\mu$
		\item $H_S$ is symmetric and idempotent, thus it is a projection matrix.
		\item From (2) it follows that $H_S$ is orthogonal to $I - H_S$
		\item Define $H_{\emptyset}$ as a $q\times q$ null matrix.
\end{enumerate}

\paragraph{Lemma 3.3} If $\mu$ has continuous partial derivatives in a neighborhood of $(\theta_0, \gamma_0)$, then:
	$$
	\sqrt{n}\left( \hat{\mu}_S - \mu_{true} \right) \overset{d}{\rightarrow} \Lambda_S
$$
where $\mu_{true} = \mu(\theta_0, \gamma_0+\delta/\sqrt{n})$ and 
	$$
	\Lambda_S = \nabla_\theta \mu(\theta_0, \gamma_0)' J_{00}^{-1} M + \omega'\left( \delta - K^{1/2}H_S K^{-1/2}D\right)
$$
Thus, the the scalar random variable $\Lambda_S$ follows a normal distribution with
	\begin{eqnarray*}
		\mbox{Mean}&=&\omega'(I - K^{1/2}H_SK^{-1/2})\delta\\
		\mbox{Variance}&=&\nabla_\theta(\theta_0, \gamma_0)'J_{00}^{-1}\nabla_\theta(\theta_0, \gamma_0) + \omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
\begin{proof}
Applying the delta-method to Lemma 3.2 along with a mean value expansion for 		$$
	\mu(\hat{\theta}_S, \hat{\gamma}_S) - \mu(\theta_0, \gamma_0+\delta/\sqrt{n})
$$
gives\footnote{for details see Section 2.5 of my First-Year Paper}
	$$
		\sqrt{n}\left( \hat{\mu}_S - \mu_{true} \right) \overset{d}{\rightarrow} \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta \equiv \Lambda_S
$$
From here, it is immediate that $\Lambda_S$ is MV normal, as it is a linear combination of a normal random vector. Although we \emph{could} find its mean and variance directly using this result, it will be helpful to express the limiting RV $\Lambda_S$ in the alternative formulation given in Lemma 3.3. This is because $M$ and $D = \delta + W$ are \emph{independent} normal random vectors! We established above that:
	\begin{eqnarray*}
		\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] &=& J_S^{-1}
	\left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] = \left[\begin{array}{cc}
				J^{00,S}&J^{01,S}\\
				J^{10,S}&J^{11,S}	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] \\ \\
	&=& \left[\begin{array}{cc}
				J_{00}^{-1} + J_{00}^{-1}J_{01}\left(\pi_S'K_S\pi_S \right)J_{10}J_{00}^{-1}&-J_{00}^{-1}J_{01}\pi_S'K_S\\
				-K_S \pi_S J_{10}J_{00}^{-1}&K_S	
		\end{array}\right] \left[\begin{array}{c}
		J_{01}\delta + M\\
		\pi_S J_{11}\delta + N_S 
	\end{array}\right] 
	\end{eqnarray*}
and, multiplying this out, found $D_S = K_S \pi_S K^{-1}(\delta + W)$. Now we will do the same for $C_S$. To begin:
	\begin{eqnarray*}
		C_S &=& J^{00,S}\left(J_{01}\delta + M\right) + J^{01,S}\left( \pi_S J_{11}\delta + N_S\right)\\
			&=& \left(J^{00,S}J_{01} + J^{01,S} \pi_S J_{11}\right)\delta + \left( J^{00,S}M + J^{01,S} N_S\right)\\
			&\equiv& A\delta + B
\end{eqnarray*}
Now, 
	\begin{eqnarray*}
		A &\equiv& J^{00,S}J_{01} + J^{01,S} \pi_S J_{11} \\
			&=& \left(J_{00}^{-1} + J_{00}^{-1}J_{01}\left[\pi_S'K_S\pi_S \right]J_{10}J_{00}^{-1}\right) J_{01} + \left( -J_{00}^{-1}J_{01}\pi_S'K_S\right) \pi_S J_{11}\\
			&=& J_{00}^{-1}J_{01}\left( I + \left[\pi_S'K_S\pi_S \right]J_{10}J_{00}^{-1}J_{01} -  \left[\pi_S'K_S\pi_S \right]J_{11} \right)\\
			&=&J_{00}^{-1}J_{01}\left[ I -  \left(\pi_S'K_S\pi_S \right)\left(J_{11} -  J_{10}J_{00}^{-1}J_{01}\right) \right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  \left(\pi_S'K_S\pi_S \right)K^{-1}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}K^{-1/2}\left(\pi_S'K_S\pi_S \right)K^{-1/2}K^{-1/2}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}\left(K^{-1/2}\pi_S'K_S\pi_S K^{-1/2}\right)K^{-1/2}\right]\\
			&=&J_{00}^{-1}J_{01}\left[ I -  K^{1/2}H_SK^{-1/2}\right]
	\end{eqnarray*}
	\begin{eqnarray*}
		B &\equiv&  J^{00,S}M + J^{01,S} N_S\\
			&=& \left(J_{00}^{-1} + J_{00}^{-1}J_{01}\pi_S'K_S\pi_S J_{10}J_{00}^{-1}\right) M + \left( -J_{00}^{-1}J_{01}\pi_S'K_S\right) \pi_S N\\
			&=& J_{00}^{-1}M  + J_{00}^{-1}J_{01}\pi_S'K_S\pi_S\left( J_{10}J_{00}^{-1}M - N\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\pi_S'K_S\pi_S\left(N - J_{10}J_{00}^{-1}M\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1}K\right)\left(N - J_{10}J_{00}^{-1}M\right)  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1}\right)\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}\left(K^{1/2}K^{-1/2}\right)\pi_S'K_S\pi_S\left(K^{-1/2}K^{-1/2}\right)\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}\left(K^{-1/2}\pi_S'K_S\pi_SK^{-1/2}\right)K^{-1/2}\left[K\left(N - J_{10}J_{00}^{-1}M\right)\right]  \\
			&=& J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}W 
	\end{eqnarray*}
where we have substituted the definition of $H_S$ and  used the fact that, as we showed above, $K(N - J_{10}J_{00}^{-1}M) = W$. Combining these, 
	\begin{eqnarray*}
	C_S &=& J_{00}^{-1}J_{01}\left( I -  K^{1/2}H_SK^{-1/2}\right) \delta + J_{00}^{-1}M  - J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}W \\
		&=& J_{00}^{-1}J_{01}\delta -  \left(J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}\right)\delta + J_{00}^{-1}M  - \left(J_{00}^{-1}J_{01}K^{1/2}H_SK^{-1/2}\right)W \\
		&=& \left(J_{00}^{-1}J_{01}\right)\delta -  \left(J_{00}^{-1}J_{01}\right)K^{1/2}H_SK^{-1/2}(\delta+W) + J_{00}^{-1}M \\
		&=& J_{00}^{-1}M + J_{00}^{-1}J_{01}\left[\delta -  K^{1/2}H_SK^{-1/2}(\delta+W)\right]\\
		&=& J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)
\end{eqnarray*}
Thus, expressing everything in terms of the independent normal random vectors $M$ and $D = \delta + W$, we have
	$$
	\left[\begin{array}{c}
		C_S\\
		D_S
	\end{array}\right] = 	\left[\begin{array}{c}
		 J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\\
		K_S \pi_S K^{-1}D
	\end{array}\right] 
$$
Now, recall that
	$$
	\Lambda_S  = \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta
$$
Multiplying through,
	$$
			\nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S = 		 \nabla_{\theta}\mu(\theta_0, \gamma_0)'\left[J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\right]\\
$$
and
	\begin{eqnarray*}
			\left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S &=& \nabla_\gamma \mu(\theta_0, \gamma_0)'\pi_S' D_S\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)'\pi_S'  K_S \pi_S
 K^{-1}D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' \left(K^{1/2}K^{-1/2}\right)\pi_S'  K_S \pi_S
 \left(K^{-1/2}K^{-1/2}\right)D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}\left(K^{-1/2}\pi_S'  K_S \pi_S
K^{-1/2}\right)K^{-1/2}D\\
		&=&\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}H_S K^{-1/2}D
\end{eqnarray*}
Therefore,
	\begin{eqnarray*}
		\Lambda_S &=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'C_S + \left[\pi_S\nabla_\gamma \mu(\theta_0, \gamma_0)\right]'D_S - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'\left[J_{00}^{-1}M + J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\right]\\
			&& \;\;\;\;\;\;\; + \left[\nabla_\gamma \mu(\theta_0, \gamma_0)' K^{1/2}H_S K^{-1/2}D\right] - \nabla_\gamma \mu(\theta_0,\gamma_0)'\delta\\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \nabla_{\theta}\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{01}\left(\delta -  K^{1/2}H_SK^{-1/2}D\right)\\
			&&\;\;\;\;\;\;\; - \nabla_\gamma \mu(\theta_0, \gamma_0)' \left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \left[\nabla_{\theta}\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{01} - \nabla_\gamma \mu(\theta_0, \gamma_0)'\right]\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \left[J_{10}J_{00}^{-1}\nabla_{\theta}\mu(\theta_0, \gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0)\right]'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)
	\end{eqnarray*}
Now we can easily calculate the mean and variance of the scalar random variable $\Lambda_S$ as we have expressed it as a linear combination of two independent normal random vectors: $M$ and $D=\delta + W$. Recall that
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&K
	\end{array}\right]\right)
$$
where $K = J^{11}$. Exploiting the symmetry of variance matrices in several places as well as the symmetry and idempotency of $H_S$, we have
	\begin{eqnarray*}
		E[\Lambda_S] &=& E\left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M\right] + E\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]\\
			&=& \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}E\left[M\right] + \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}E[\delta + W]\\
			&=& \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}\left(\delta +E[W]\right)\\
			&=& \omega' \delta -\omega' K^{1/2}H_S K^{-1/2}\delta\\
			&=& \omega' \left(I - K^{1/2}H_S K^{-1/2}\right)\delta
	\end{eqnarray*}
	\begin{eqnarray*}
		 Var\left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}M\right] &=& \left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\right]Var[M]\left[\nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\right]'\\
		&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}J_{00}J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0)\\
		&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0)
	\end{eqnarray*}
	\begin{eqnarray*}
		Var\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]&=&\left( \omega'K^{1/2}H_S K^{-1/2}\right)Var[D]\left( \omega'K^{1/2}H_S K^{-1/2}\right)'\\
				&=&\omega'K^{1/2}H_S K^{-1/2}K K^{-1/2}H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S \left(K^{-1/2}K^{1/2}\right)\left(K^{1/2} K^{-1/2}\right)H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S H_SK^{1/2}\omega\\
				&=&\omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
	\begin{eqnarray*}
		Var[\Lambda_S] &=& Var\left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M\right] + Var\left[\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)\right]\\
					&=& \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0) + \omega'K^{1/2}H_S K^{1/2}\omega
	\end{eqnarray*}
\end{proof}





\section{Schorfheide (2005)}




\end{document}