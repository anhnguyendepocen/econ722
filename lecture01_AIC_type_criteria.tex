\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\begin{document}

\title{``AIC-type'' Information Criteria}

\author{Francis J.\ DiTraglia}

\maketitle 


This material is covered in numerous textbooks and articles. The derivations given here most closely follow Konishi \& Kitagawa (2008) for AIC and TIC, and Burnham and Anderson (2002) for $\mbox{AIC}_c$.  

\todo[inline]{Direct them to some review material on MLE.}

\section{Introduction}
Akaike's Information Criterion (AIC) is often described as summarizing the fit of a model by its maximized Log-likelihood minus a complexity penalty. This is an accurate description of the \emph{formula} for the AIC, but overlooks the deep mathematical ideas that underlie it. The AIC uses an estimate of the Kullback-Leibler divergence to decide which model best approximates the true data generating process.

\todo[inline]{Change $i$ subscripts to $t$ subscripts!}

\subsection{Notation etc.}
The ideas presented here apply to arbitrary ML models, but to keep the notation simple, we consider only a scalar random variable $Y$ without conditioning on covariates. We also restrict attention to the iid setting.  The notation is as follows:
		\begin{itemize}
			\item Parametric Model: $f\left(y,\theta\right)$, sometimes abbreviated $f(y)$
			\item $\theta$ is a $p$-dimensional vector of parameters
			\item $Y_1, Y_2, \hdots Y_n$ are iid 
			\item Log-likelihood of sample: $\ell(\theta) = \sum_{i=1}^n \log{f\left(y_i,\theta\right)}$
		\end{itemize}


\subsection{Kullback-Leibler Divergence}
Consider the following setup:
	\begin{itemize}
		\item Truth: $Y_1, Y_2, \hdots Y_n \sim \mbox{ iid } g(y)$
		\item Model: $f(y)$
		\item Question: How well does $f$ approximate $g$?
	\end{itemize}
To find the best approximation to $g$, we might consider a number of models and choose the one that minimizes some measure of discrepancy. One such measure is the Kullback-Leibler Divergence. Let $E_G$ denote expectation with respect to the true, unknown density $g(y)$.

\begin{defn}[KL Divergence]
The Kullback-Leibler divergence from $g(y)$ to $f(y)$ is given by
	$$KL(g;f) = E_G \left[ \log{\left\{\frac{g(Y)}{f(Y)}\right\}}\right]= E_G\left[ \log{g(Y)}\right] - E_G\left[ \log{f(Y)} \right]$$
The quantity $E_G\left[ \log{f(Y)} \right]$ is called the Expected Log-likelihood.
\end{defn}

\paragraph{Properties of KL Divergence}
	\begin{enumerate}
		\item It is \emph{not} a metric: $KL(g;f) \neq KL(f;g)$
		\item $KL(g;f) \geq 0$ with equality iff $f=g$
		
		Since $\log$ is a concave function, $-\log$ is convex. Thus
			\begin{eqnarray*}
			KL(g;f) &=&  E_G \left[ \log{\left\{\frac{g(Y)}{f(Y)}\right\}}\right] = E_G \left[- \log{\left\{\frac{f(Y)}{g(Y)}\right\}}\right]\\\\
				&\geq& -\log{\left\{E_G\left[ \frac{f(Y)}{g(Y)} \right]\right\}} = -\log{\left(\int g(y) \frac{f(y)}{g(y)} dy  \right)}\\\\
				&=&  -\log{\left(\int f(y) dy  \right)} = -\log{(1)} = 0
			\end{eqnarray*} 
		by Jensen's Inequality. The inequality is strict only for a non-degenerate random variable and a strictly convex function. Since $-\log$ is strictly convex, the only way to make the inequality strict is for $f(Y)/g(Y)$ to be degenerate. This occurs precisely when $f=g$.
		
		\item \emph{Maximize Expected Log-likelihood $\iff$ Minimize KL Divergence}
		
		The first term in the KL divergence is a constant: it in no way depends on the model $f(y)$. The expected Log-likelihood enters negatively:
		$$KL(g;f) = E_G\left[ \log{g(Y)}\right] - E_G\left[ \log{f(Y)} \right]$$ 
		Thus, if we can find a way to estimate the Expected Log-likelihood, we can use the KL divergence for model selection: the larger the Expected Log-likelihood, the smaller the KL divergence, and hence the better the model.
		
		\item The KL divergence equals the negative of \textbf{Boltzmann's Entropy} from Statistical Mechanics. Accordingly, it represents the \emph{information lost} when $g(y)$ is encoded by $f(y)$.
	\end{enumerate}

\subsection{Relationship of MLE to KL}
Parameterize $f$ by $\theta$. Then the expected Log-likelihood can be written as:
	$$E_G\left[\log{f(Y,\theta)}  \right] = \int \log{f(y,\theta)}dG(y)$$
Unfortunately, $G$ is unknown. (Remember: $G$ is the true data-generating process.) Replacing $G$ with the empirical distribution $\hat{G}$ yields the estimator:
	$$E_{\hat{G}}\left[\log{f(Y,\theta)}  \right] = \frac{1}{n}\sum_{i=1}^n \log{f(y_i, \theta)} = \frac{1}{n}\ell(\theta)$$
By the Weak Law of Large Numbers for iid observations
	$$\frac{1}{n} \ell(\theta) \overset{p}{\rightarrow} E_G\left[ \log{f(Y,\theta)} \right]$$
Under the standard regularity conditions (see Newey and McFadden) we can strengthen this result to show that
	$$\hat{\theta} = \arg \max_{\theta \in \Theta} \frac{1}{n}\ell(\theta) \overset{p}{\rightarrow} \arg \max_{\theta \in \Theta} E_G\left[ \log{f(Y,\theta)}\right]$$
Since minimizing the KL divergence is the same as maximizing the expected log-likelihood:
\begin{pro}
The ML estimator $\hat{\theta}$ converges in probability to the value of $\theta$ that minimizes the KL divergence from unknown true density $g(y)$ to the parametric family $f(y,\theta)$. When $g(y)=f(y,\theta)$ for some value of $\theta$, the divergence is minimized at zero.
\end{pro}


\subsection{A Na\"{i}ve Information Criterion} 
If $g(y)$ were known, we could choose between two parametric models $f(y,\theta)$ and $h(y,\gamma)$ by comparing maximized Log-likelihoods. Define
	\begin{eqnarray*}
	\theta_0 &=&  \arg \max_{\theta \in \Theta} E_G\left[ \log{f(Y,\theta)} \right]\\
	\gamma_0 &=&  \arg \max_{\gamma\in \Gamma} E_G\left[ \log{h(Y,\gamma)} \right]
	\end{eqnarray*}
If $E_G\left[ \log{f(Y,\theta_0)} \right] > E_G\left[ \log{h(Y,\gamma_0)} \right]$, then the KL divergence from $g(y)$ to the parametric family $f_\theta$ is smaller than that from $g(y)$ to $h_\gamma$.

From the previous subsection, we know that $\hat{\theta}_{ML} \overset{p}{\rightarrow} \theta_{0}$. Further, $\frac{1}{n}\ell(\theta) \overset{p}{\rightarrow} E_G\left[\log{f(Y,\theta)} \right]$. Of course, $n$ will be constant across models, so why not use the maximized sample likelihood $\ell(\hat{\theta})$ for model comparison? \emph{BECAUSE IT'S BIASED!}

\subsection{Intuition for the Bias}
The basic problem is that $\ell(\hat{\theta})$ uses the data twice: first to estimate $\hat{\theta}$ and then directly in the sum $\sum_{i=1}^n \log{ f(y_i,\hat{\theta})}$. Because $\hat{\theta}$ was chosen to conform to the idiosyncrasies of the data at hand, $\ell(\hat{\theta})$ is overly optimistic.\footnote{This is similar in spirit is the \emph{post hoc contrast}: measuring two groups on a number of dimensions, choosing the dimension along which they differ the most, and carrying out a $t$-test on this dimension. What a surprise; the result is significant!}

Since $\theta_0$ is the population minimizer of the KL divergence from $g$ to $f_\theta$, 
	\begin{eqnarray*}
		KL\left[ g(y);f(y,\theta) \right] &\geq& KL\left[ g(y);f(y,\theta_0) \right]\\
		E_G\left[  \log{g(Y)}\right] - E_G\left[ \log{f(Y,\theta)} \right] &\geq& E_G\left[  \log{g(Y)}\right] - E_G\left[ \log{f(Y,\theta_0)} \right]\\
	  E_G\left[ \log{f(Y,\theta)} \right] &\leq&  E_G\left[ \log{f(Y,\theta_0)} \right]\
	\end{eqnarray*}
for all $\theta \in \Theta$. Recall that $\frac{1}{n}\ell(\theta) = E_{\hat{G}}\left[ \log{f(Y,\theta)}\right]$. By the definition of the maximum likelihood estimate, $\ell(\hat{\theta})\geq\ell(\theta_0)$. Thus,
		 $$E_{\hat{G}}\left[ \log{f(Y,\hat{\theta})}\right]\geq  E_{\hat{G}}\left[ \log{f\left(Y,\theta_0\right)}\right] $$
In sample, the estimate $\hat{\theta}$ will show a higher maximized log-likelihood than the value of $\theta$ that maximizes the population log-likelihood.

\emph{What we should be evaluating is the likelihood at a \textbf{new} set of observations not used to estimate $\hat{\theta}$.}



\section{The AIC and TIC}
In the previous section we outlined the problem. Now we'll consider a solution that involves constructing an asymptotic approximation to the bias. This leads to the TIC and AIC.

\subsection{An Asymptotic Approximation to the Bias}
First some notation:
	\begin{itemize}
		\item Make the sample size and the dependence of $\hat{\theta}$ on the data explicit by writing
	$$\log{f\left(\left.\mathbf{y}_n\right|\hat{\theta}(\mathbf{y}_n)\right)} = \sum_{i=1}^n \log{f(y_i|\hat{\theta})}$$
		\item Let $E_{G(\mathbf{y}_n)}$ denote expectation with respect to the joint distribution of $\mathbf{Y}^{(n)} = (Y_1, \hdots, Y_n)'$. The realizations of this random vector are $\mathbf{y}_n = y_1, \hdots, y_n$.
		\item Let $\mathbf{Z}^{(n)} = (Z_1, \hdots, Z_n)'$ be a collection of iid random variables with the same distribution as $\mathbf{Y}^{(n)}$ that were \emph{not} used to estimate $\hat{\theta}$. (Assume that $\mathbf{Z}^{(n)}$ and $\mathbf{Y}^{(n)}$ are mutually independent.) Let $G(\mathbf{z}_n)$ denote expectation with respect to the joint distribution of $\mathbf{Z}^{(n)}$.
		\item The bias of the maximized Log-likelihood as an estimator of the expected Log-likelihood is given by
	$$Bias = E_{G(\mathbf{y}_n)}\left[ \log{f\left(\left.\mathbf{Y}^{(n)}\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)} -  E_{G(\mathbf{z}_n)}\left\{  \log{f\left(\left.\mathbf{Z}^{(n)}\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)}\right\}\right]$$
\end{itemize}


\paragraph{Calculating the Bias:} Since $Z_1, \hdots, Z_n$ are iid, the expectation with respect to $G(\mathbf{z}_n)$ simplifies, and we can write
			$$Bias = E_{G(\mathbf{y}_n)}\left[ \log{f\left(\left.\mathbf{Y}^{(n)}\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)} -  nE_{G(z)}\left\{  \log{f\left(\left.Z\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)}\right\}\right]$$
where $Z$ is any of the $Z_1, \hdots, Z_n$ and $G(z)$ is its \emph{marginal} distribution. We expand the bias into $D_1 + D_2 + D_3$ where
	\begin{eqnarray*}
		D_1 &=& E_{G(\mathbf{y}_n)}\left[ \log{f\left(\left.\mathbf{Y}^{(n)}\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)} - \log{f\left( \mathbf{Y}^{(n)}|\theta_0 \right)}\right]\\\\
		D_2 &=& E_{G(\mathbf{y}_n)}\left[  \log{f\left( \mathbf{Y}^{(n)}|\theta_0 \right)} - nE_{G(z)}\left\{  \log{f\left(\left.Z\right|\theta_0\right)}\right\} \right]\\\\
		D_3 &=& E_{G(\mathbf{y}_n)}\left[ nE_{G(z)}\left\{  \log{f\left(\left.Z\right|\theta_0\right)}\right\}  -  nE_{G(z)}\left\{  \log{f\left(\left.Z\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)}\right\} \right]
	\end{eqnarray*}
Now consider each part separately.

\paragraph{Step 1: Calculation of $D_2$} This is the easiest part as it involves no estimators. Note first that $E_{G(z)}\left\{  \log{f\left(\left.Z\right|\theta_0\right)}\right\} $ is a constant, so that
	\begin{eqnarray*}
		D_2 &=& E_{G(\mathbf{y}_n)}\left[  \log{f\left( \mathbf{Y}^{(n)}|\theta_0 \right)}\right] - nE_{G(z)}\left[  \log{f\left(\left.Z\right|\theta_0\right)}\right] \\
		&=& n E_{G(y)}\left[ \log{f(Y|\theta_0)} \right] -  nE_{G(z)}\left[  \log{f\left(\left.Z\right|\theta_0\right)}\right] \\
		&=& 0
	\end{eqnarray*}
where we have used the fact that $Y_1, \hdots, Y_n$ are iid and that $Y$ and $Z$ have the same distribution. 


\paragraph{Step 2: Calculation of $D_3$} Define $\eta(\hat{\theta}) = E_{G(z)}\left[ \log{f\left(z|\hat{\theta}(\mathbf{Y}^{(n)})\right)} \right] $ and Taylor expand around $\theta_0$, the solution to the population moment condition
	$$E_{G(z)}\left[ \frac{\partial}{\partial \theta}  \log{f(z|\theta)} \right]=0$$
yielding
	$$\eta(\hat{\theta})= \eta(\theta_0) + \left( \hat{\theta} - \theta_0 \right)' \frac{\partial \eta(\theta_0)}{\partial \theta} + \frac{1}{2}\left( \hat{\theta} - \theta_0 \right)  \frac{\partial^2 \eta(\theta_0)}{\partial \theta \partial \theta'}\left( \hat{\theta} - \theta_0 \right) + o_p(1)$$
Now, under the appropriate regularity conditions (Lebesgue Dominated Convergence) we can exchange the order of expectation and differentiation, so that
	$$ \frac{\partial \eta(\theta_0)}{\partial \theta} =  \frac{\partial}{\partial \theta} E_{G(z)}\left[ \log{f(z|\theta_0)} \right]=  E_{G(z)}\left[\frac{\partial}{\partial \theta} \log{f(z|\theta_0)} \right]=0$$
by the definition of $\theta_0$ as the solution to the population moment condition. Substituting this into the Taylor Expansion, and assuming further that we may exchange the \emph{second} derivative and expectation with respect to $G(z)$, we have
	\begin{eqnarray*}
		\eta(\hat{\theta}) &=& \eta(\theta_0) + \frac{1}{2}\left( \hat{\theta} - \theta_0 \right) ' \frac{\partial^2 }{\partial \theta \partial \theta'} E_{G(z)}\left[ \log{f(z|\theta_0)} \right]\left( \hat{\theta} - \theta_0 \right) + o_p(1)\\
			&=&  \eta(\theta_0) + \frac{1}{2}\left( \hat{\theta} - \theta_0 \right) ' E_{G(z)}\left[ \frac{\partial^2 }{\partial \theta \partial \theta'} \log{f(z|\theta_0)} \right]\left( \hat{\theta} - \theta_0 \right) + o_p(1)
	\end{eqnarray*}
Thus, defining 
	$$J(\theta_0) = -E_{G(z)}\left[ \frac{\partial^2 }{\partial \theta \partial \theta'} \log{f(z|\theta_0)} \right]$$
we see that
	$$\eta(\hat{\theta}) = E_{G(z)}\left[ \log{f\left(z|\theta_0\right)} \right] - \frac{1}{2} \left( \hat{\theta}- \theta_0\right)'J(\theta_0)\left(\hat{\theta}- \theta_0\right) + o_p(1)$$
Substituting this expansion for $ E_{G(z)}\left[ \log{f\left(z|\hat{\theta}(\mathbf{Y}^{(n)})\right)} \right]$ into the expression for $D_3$ cancels the leading first term $nE_{G(z)}\left\{  \log{f\left(\left.Z\right|\theta_0\right)}\right\} $ in the expectation so that (remember that $\hat{\theta}$ is a function of $\mathbf{y}_n$ and thus, so is $o_p(1)$)
	\begin{eqnarray*}
		D_3 &=& E_{G(\mathbf{y}_n)}\left[  \frac{n}{2} \left( \hat{\theta}- \theta_0\right)'J(\theta_0)\left(\hat{\theta}- \theta_0\right) + o_p(1)  \right]\\
		&=& \frac{1}{2} E_{G(\mathbf{y}_n)}\left[ \mbox{trace}\left\{n\left( \hat{\theta}- \theta_0\right)'J(\theta_0)\left(\hat{\theta}- \theta_0\right) \right\} \right] +o_p(1)\\
			&=& \frac{1}{2}\mbox{trace}\left\{J(\theta_0) E_{G(\mathbf{y}_n)}\left[\sqrt{n} \left( \hat{\theta}- \theta_0\right)\sqrt{n}\left(\hat{\theta}- \theta_0\right)' \right]\right\} +o_p(1)
	\end{eqnarray*}
where we have used the facts that: (1) a scalar equals its trace, (2) multiplication within the trace operator is commutative provided that the products remain conformable, (3) trace and expectation can be exchanged because both are linear operators, and (4) $J(\theta_0)$ is a constant with respect to $E_{G(\mathbf{y}_n)}$. We know that
	$$E_{G(\mathbf{y}_n)}\left[\sqrt{n} \left( \hat{\theta}- \theta_0\right)\sqrt{n}\left(\hat{\theta}- \theta_0\right)' \right]\rightarrow J^{-1}(\theta_0) I(\theta_0)J^{-1}(\theta_0)$$
as $n\rightarrow \infty$ where
	\begin{eqnarray*}
	I(\theta_0) &=&E_{G(z)}\left[ \frac{\partial \log{f(Z|\theta)}}{\partial \theta} \;\frac{\partial \log{f(Z|\theta)}}{\partial \theta'} \right]\\\\
	J(\theta_0) &=& - E_{G(z)}\left[ \frac{\partial^2  \log{f(Z|\theta)}}{\partial \theta \partial \theta'}\right]
	\end{eqnarray*}
Thus, assuming the necessary regularity conditions to ensure that the remainder is indeed of small order, as $n\rightarrow \infty$ we have
	$$D_3 \rightarrow \frac{1}{2} \mbox{trace}\left\{ J(\theta_0) J(\theta_0)^{-1}I(\theta_0)J(\theta_0)^{-1}  \right\} = \frac{1}{2}\mbox{trace}\left\{ I(\theta_0)J(\theta_0)^{-1} \right\}$$



\paragraph{Step 3: Calculation of $D_1$} Taylor expand $\ell(\theta_0) = \log{f\left(\mathbf{y}_n| \theta_0\right)}$ around $\hat{\theta}$, noting that by the definition of the MLE, 
	$$\frac{\partial \ell(\hat{\theta})}{\partial \theta} = 0$$
Thus,
	$$\ell(\theta_0) = \ell(\hat{\theta}) + \frac{1}{2} \left( \theta - \hat{\theta} \right)' \frac{\partial^2  \ell(\tilde{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta} \right)$$
where $\tilde{\theta}$ is between $\hat{\theta}$ and $\theta_0$, so we have
	$$ \ell(\hat{\theta}) - \ell(\theta_0)  = \frac{1}{2} \sqrt{n}\left( \theta - \hat{\theta} \right)'\left[ - \frac{1}{n} \frac{\partial^2  \ell(\tilde{\theta})}{\partial \theta \partial \theta'} \right]\sqrt{n}\left( \theta - \hat{\theta} \right)$$
By the WLLN for iid observations, we know that for fixed $\theta$
	$$- \frac{1}{n} \frac{\partial^2  \ell(\theta)}{\partial \theta \partial \theta'} = -\frac{1}{n}\sum_{i=1}^n \frac{\partial^2}{\partial \theta \partial \theta'}\log{f(y_i|\theta)}\overset{p}{\rightarrow} J(\theta)$$
Furthermore, by the standard consistency result for MLE $\hat{\theta} \rightarrow \theta_0$. Thus, by a Uniform WLLN,
	$$- \frac{1}{n} \frac{\partial^2  \ell(\hat{\theta})}{\partial \theta \partial \theta'}\overset{p}{\rightarrow} J(\theta_0)$$
It follows that 	
	\begin{eqnarray*}
		D_1 &=& E_{G(\mathbf{y}_n)}\left[ \log{f\left(\left.\mathbf{Y}^{(n)}\right|\hat{\theta}\left(\mathbf{Y}^{(n)}\right)\right)} - \log{f\left( \mathbf{Y}^{(n)}|\theta_0 \right)}\right]\\
			&=& E_{G(\mathbf{y}_n)}\left[ \ell(\hat{\theta}) - \ell(\theta_0) \right]\\
			&\rightarrow& \frac{1}{2} \mbox{trace}\left\{ I(\theta_0) J(\theta_0)^{-1} \right\}
	\end{eqnarray*}
using the same arguments as in Step 2.  
\paragraph{Finally The Bias:} Combining the three steps, we have
	$$Bias = D_1 + D_2 + D_3 = \mbox{trace}\left\{ I(\theta_0)J(\theta_0)^{-1} \right\}$$
	
\subsection{Correcting the Bias}
Now that we have an asymptotic approximation to the bias of the maximized likelihood as an estimator of the expected log-likelihood, we can correct it. Suppose that we have consistent estimators $\hat{I}$ and $\hat{J}$ of $I(\theta_0)$ and $J(\theta_0)$. Then, by the Continuous Mapping Theorem $\hat{b}=\mbox{trace}(\hat{I}\hat{J}^{-1})$ is a consistent estimator of the bias term. Recall that the maximized log-likelihood is biased \emph{upwards}, hence to correct it we need to subtract this quantity. This estimate yields \textbf{Takeuchi's Information Criterion} (TIC):
	$$TIC = 2\left[ \sum_{i=1}^n \log{f\left( y_i | \hat{\theta} \right)} - \mbox{trace}(\hat{I}\hat{J}^{-1}) \right] = 2 \left[\ell(\theta) - \mbox{trace}\left(\widehat{I}\widehat{J}^{-1}\right) \right]$$
The scaling factor of two is traditional, but has no effect on model comparisons.

Akaike's Information Criterion (AIC) uses a slightly different bias correction. Recall that if there exists a $\theta \in \Theta$ such that the true density $g(y)$ equals the model $f(y|\theta)$, then $I(\theta_0)=J(\theta_0)^{-1}$. This is the famous ``Information Matrix Equality.'' In this case, the bias approximation becomes:
	$$Bias = \mbox{trace}\left\{ I(\theta_0)J(\theta_0)^{-1} \right\} = \mbox{trace}\left\{ \mathbf{I}_p \right\}=p$$
that is, the dimension of the parameter vector. Using this as a bias correction yields \textbf{Akaike's Information Criterion}:
	$$AIC = 2\left[ \sum_{i=1}^n \log{f\left( y_i | \hat{\theta} \right)} - p \right] = 2\left[ \ell(\theta) - p \right]$$
Again, it is traditional to multiply by two. Although the TIC and AIC are similar, there are several subtleties:
	\begin{itemize}
		\item The AIC is derived assuming that the model is \emph{correctly specified}, while the TIC is not. In this sense the AIC is a special case of the TIC.
		\item Typically, the matrices $I(\theta_0)$ and $J(\theta_0)$ are large, meaning that the estimates will have high variance (we need to estimate $p^2 + p$ elements).
		\item In contrast, the AIC a has much smaller variance because the bias correction \emph{does not depend on the data}. 
		\item Thus, even if the model is mis-specified, it may be preferable to use AIC rather than TIC unless the sample size is large.
		\item It has been argued that for models where the Information Matrix is not satisfied, the AIC will still be close to the TIC. (The log-likelihood term should dominate the bias correction in such situations.)
	\end{itemize}

\subsection{Friends of AIC}
Recall the idea behind the AIC:
	\begin{enumerate}
		\item Use the KL divergence to choose a model.
		\item Suffices to compare Expected Log-likelihoods. 
		\item Sample analogue, maximized log-likelihood, requires a bias correction.
	\end{enumerate}
The AIC and TIC use an analytical bias correction. Other possibilities:
	\begin{itemize}
		\item Cross-Validation
		\item Bootstrap Information Criterion (EIC)
	\end{itemize}
Maximum likelihood estimators can be unstable. If we want to use a different kind of estimator (e.g.\ maximum penalized likelihood) but are still willing to write down a likelihood, we can use the KL divergence for model selection via the Generalized Information Criterion (GIC) of Konishi and Kitagawa, or its bootstrap equivalent (EGIC).

\section{Refinements to the AIC}
In the derivations above we used asymptotic theory to derive an analytical bias correction. These approximations tend to work well as long $n$ is fairly large relative to $p$ but when this is not the case, they can break down. We'll now consider some alternatives that work better in such situations. The first uses \emph{exact} small-sample theory to derive the appropriate bias correction, while the second uses the Bootstrap.


\subsection{Corrected AIC}
This is based on the derivation in Hurvich \& Tsai (1989). Suppose that the true DGP is
	$$\textbf{y} = X\beta_0 + \boldsymbol{\epsilon}$$
where $\mathbf{\epsilon} \sim N(\mathbf{0}, \sigma_0^2 \mathbf{I}_T)$. Then $\mathbf{y}|X \sim N(X\beta_0, \sigma_0^2 \mathbf{I}_T)$ so the likelihood is
	$$g(\textbf{y}|X;\beta_0, \sigma^2_0) = \left(2\pi\sigma_0^2\right)^{-T/2} \exp\left\{ -\frac{1}{2\sigma^2}(y - X\beta_0)'(y - X\beta_0)\right\}$$
and the log-likelihood is
	$$\log\left[g(\textbf{y}|X;\beta_0, \sigma_0^2)\right] = -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_0) - \frac{1}{2\sigma_0^2}\left(\textbf{y} - X\beta_0\right)'\left(\textbf{y} -X\beta_0\right)$$
To keep the notation from getting out of control, we'll use the shorthand $\log[g(\mathbf{y})]$ for this quantity. Now suppose we evaluated the log-likelihood at some \emph{other} parameter values $\beta_1$ and $\sigma^2_1$. The vector $\beta_1$ might, for example, correspond to dropping some regressors from the model by setting their coefficients to zero, or perhaps adding in some additional regressors. We have
	$$\log[f(\textbf{y}|X;\beta_1, \sigma_1^2)] =  -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_1) - \frac{1}{2\sigma_1^2}\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right)$$

\begin{eqnarray*}
	KL(g;f)&=& \int \log[g(\mathbf{y})] g(\mathbf{y}) \; d \mathbf{y} - \int \log[f(\mathbf{y})]g(\mathbf{y}) \; d \mathbf{y} =  A - B 
\end{eqnarray*}

	$$KL(g;f) =  \frac{T}{2}\left[\frac{\sigma_0^2}{\sigma_1^2} - \log\left(\frac{\sigma_0^2}{\sigma_1^2}\right) - 1 \right] + \left(\frac{1}{2 \sigma_1^2}\right)\left(\beta_0 - \beta_1\right)'X'X\left(\beta_0 - \beta_1\right)$$

\begin{eqnarray*}
	A &=&\int \log[g(\mathbf{y})] g(\mathbf{y}) \; d \mathbf{y} \\
	&=& \int \left[-\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_0) - \frac{1}{2\sigma_0^2}\left(\textbf{y} - X\beta_0\right)'\left(\textbf{y} -X\beta_0\right) \right] g(\mathbf{y})\; d\mathbf{y}\\
	&=& -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0)\right] -  \frac{1}{2\sigma_0^2} E_{\mathbf{y}|X}\left[\left(\textbf{y} - X\beta_0\right)'\left(\textbf{y} -X\beta_0\right)\right]\\
	&=& -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0)\right] -  \frac{1}{2\sigma_0^2}\mbox{trace}\left\{ E_{\mathbf{y}|X}\left[\left(\textbf{y} - X\beta_0\right)\left(\textbf{y} -X\beta_0\right)'\right]\right\}\\
	&=& -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0)\right] -  \frac{1}{2\sigma_0^2}\mbox{trace}\left\{ Var(\mathbf{y}|X)\right\}\\
	&=& -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0)\right] -  \frac{1}{2\sigma_0^2}\left(T \sigma_0^2\right)\\
	&=& -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0) +  1 \right]
\end{eqnarray*}


\begin{eqnarray*}
	B &=& \int \log[f(\mathbf{y})] g(\mathbf{y}) \; d \mathbf{y}\\
	&=& \int \left[ -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_1) - \frac{1}{2\sigma_1^2}\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right) \right] g(\mathbf{y})\;  d \mathbf{y}\\
		&=& -\frac{T}{2}\left[\log(2\pi) + \log(\sigma^2_1) \right]- \frac{1}{2\sigma_1^2}E_{\mathbf{y}|X}\left[\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right)\right]\\
		&=&  -\frac{T}{2}\left[\log(2\pi) + \log(\sigma^2_1) \right] - \left(\frac{1}{2\sigma_1^2}\right)C
\end{eqnarray*}

\begin{eqnarray*}
	C &=& E_{\mathbf{y}|X}\left[\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right)\right]\\
		&=&E_{\mathbf{y}|X}\left[\left\{ \left(\mathbf{y} - X\beta_0\right) + X\left(\beta_0 - \beta_1\right)\right\}'\left\{ \left(\mathbf{y} - X\beta_0\right)+ X\left(\beta_0 - \beta_1\right)\right\} \right]\\ 
	&=& E_{\mathbf{y}|X}\left[\left(\mathbf{y} - X\beta_0\right)'\left(\mathbf{y} - X\beta_0\right)\right] \\
	&&\quad +\; E_{\mathbf{y}|X}\left[\left(\mathbf{y} -X\beta_0\right)'X\left(\beta_0 - \beta_1\right)\right] \\
	&& \quad +\;  E_{\mathbf{y}|X}\left\{\left[X\left(\beta_0 -\beta_1 \right) \right]'\left(\mathbf{y} - X\beta_0 \right)\right\} \\
	&& \quad + \;E_{\mathbf{y}|X}\left[\left\{X\left(\beta_0 - \beta_1\right)\right\}'\left\{X\left(\beta_0 - \beta_1\right)\right\} \right]\\ 
		&=& Var(\mathbf{y}|X)\\
		&&\quad +\; E_{\mathbf{y}|X}\left[\mathbf{y} - X\beta_0 \right]'X(\beta_0 -\beta_1) \\
		&& \quad + \;(\beta_0 -\beta_1)'X'E_{\mathbf{y}|X}\left[\mathbf{y} - X\beta_0 \right] \\
		&& \quad + \; (\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)\\
	&=& T\sigma_0^2 + 0 + 0 + (\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)
\end{eqnarray*}
Hence,
\begin{eqnarray*}
	B &=& -\frac{T}{2}\left[\log(2\pi) + \log(\sigma^2_1) \right] - \left(\frac{1}{2\sigma_1^2}\right) \left[ T\sigma_0^2 + (\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)\right]\\
		&=& -\frac{T}{2}\left[\log(2\pi) + \log(\sigma^2_1) + \frac{\sigma_0^2}{\sigma_1^2}\right] - \left(\frac{1}{2\sigma_1^2} \right)(\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
	KL(g;f) &=& A - B\\
		&=& \left\{ -\frac{T}{2}\left[ \log(2\pi) + \log(\sigma^2_0) +  1 \right]\right\}\\
		&& \quad - \; \left\{ -\frac{T}{2}\left[\log(2\pi) + \log(\sigma^2_1) + \frac{\sigma_0^2}{\sigma_1^2}\right] - \left(\frac{1}{2\sigma_1^2} \right)(\beta_0 - \beta_1)X'X(\beta_0 - \beta_1) \right\}\\
	&=& -\frac{T}{2} \left[ \log(\sigma_0^2) + 1 - \log(\sigma_1^2)  - \frac{\sigma_0^2}{\sigma_1^2}\right] + \left(\frac{1}{2\sigma_1^2} \right)(\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)\\
	&=& \frac{T}{2}\left[\frac{\sigma_0^2}{\sigma_1^2}  - \log\left(\frac{\sigma_0^2}{\sigma_1^2} \right) - 1\right] + \left(\frac{1}{2\sigma_1^2} \right)(\beta_0 - \beta_1)X'X(\beta_0 - \beta_1)
\end{eqnarray*}
We need to estimate this quantity for it to be of any use in model selection. If let $\widehat{\beta}$ and $\widehat{\sigma}^2$ be the maximum likelihood estimators of $\beta_1$ and $\sigma_1^2$ and substitute them into the expression for the KL divergence, we have
\begin{eqnarray*}
	\widehat{KL}(g;f) &=& \frac{T}{2}\left[\frac{\sigma_0^2}{\widehat{\sigma}^2}  - \log\left(\frac{\sigma_0^2}{\widehat{\sigma}^2} \right) - 1\right] + \left(\frac{1}{2\widehat{\sigma}^2} \right)\left(\beta_0 - \widehat{\beta}\right)X'X\left(\beta_0 - \widehat{\beta}\right)
\end{eqnarray*}
We still have two problems. First, we haven't been entirely clear about what $\beta_1$ and $\sigma_1$ are. At the moment, they seem to be something like ``pseudo-true'' values. Second, and more importantly, we don't know $\beta_0$ and $\sigma_0^2$ so we can't use the preceding expression to compare models.

Hurvich and Tsai (1989) address both of these problems with the assumption that all models under consideration are \emph{at least correctly specified}. That is, while they may include a regressor whose coefficient is in fact zero, they do not exclude any regressors with a non-zero coefficient. This is the same assumption that we used above to reduce TIC to AIC. Under this assumption, $\beta_1$ and $\sigma_1^2$ \emph{are precisely the same} as $\beta_0$ and $\sigma_0^2$. More importantly, we can use all of the standard results for the exact finite sample distribution of regression estimators to help us. The idea is to construct an \emph{unbiased} estimator of the KL divergence. Taking expecations and rearranging slightly, we have
\begin{eqnarray*}
	E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{  E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right]  - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
	&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]
\end{eqnarray*}
Now, under our assumptions $T\widehat{\sigma}^2/\sigma_0^2 \sim \chi^2_{T-k}$ where $k$ is the number of estimated coefficients in $\widehat{\beta}$. Further, if $Z \sim \chi^2_\nu$ then $E[1/Z] = 1/(\nu-2)$. It follows that
	$$E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] = E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
We can rewrite the final term similarly:
	$$E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right] = E\left[\left(\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right)\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right]$$
Under our assumptions the two terms in the product are independent, so we can break apart the expectation. First, we have
$$E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
as above. For the second part, 
	$$\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \sim \chi^2_k$$
and hence
	$$E\left[\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right] = k$$
Putting all the pieces together,
\begin{eqnarray*}
	E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{  E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right]  + \log(\sigma_0^2) - E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
	&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]\\
	&=& \frac{T}{2} \left( \frac{T}{T-k-2} - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right) + \frac{T}{2}\left(\frac{k}{T - k -2}\right)\\
	&=& \frac{T}{2} \left(\frac{T + k}{T - k -2}  - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right)
\end{eqnarray*}
Since $\log(\widehat{\sigma}^2)$ is an unbiased estimator of $E[\log(\widehat{\sigma}^2)]$, substituting this give us an unbiased estimator of $E\left[\widehat{KL}(g;f) \right]$ as desired. 
The only terms that vary across candidate models are the first and the third. Moreover, the multiplicative factor of $T/2$ does not affect model selection. Hence, the criterion is
	$$AIC_c = \log(\widehat{\sigma}^2) + \frac{T + k}{T - k -2}$$
So how does this compare to the plain-vanilla AIC for normal linear regression? The maximum likelihood estimators for this problem are
\begin{eqnarray*}
	\widehat{\beta} &=& (X'X)^{-1}X'\mathbf{y}\\
	\widehat{\sigma}^2 &=& \frac{(\mathbf{y} - X\widehat{\beta})'(\mathbf{y} - X\widehat{\beta})}{T}
\end{eqnarray*}
It follows that the maximized log-likehood is
\begin{eqnarray*}
	\log\left[f(\mathbf{y}|X;\widehat{\theta})\right] &=&  -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{1}{2\widehat{\sigma}^2}(y - X\widehat{\beta})'(y -X\widehat{\beta})\\
		&=& -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{T}{2}
\end{eqnarray*}
by substituting $T\widehat{\sigma}^2$ for the numerator of the second term. Hence, the AIC for this problem is 
	$$AIC = $$







\end{document}