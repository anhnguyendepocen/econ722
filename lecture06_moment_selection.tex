
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 6: Moment Selection for GMM}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Review of Generalized Method of Moments}
The best reference for GMM is Hall (2005). 

\subsection{Key Assumptions}
Let $f$ be a $q$-vector of functions of an observable random $r$-vector $v_t$ and a $p$-vector of parameters $\theta \in \Theta \subseteq \mathbb{R}^p$ where $\Theta$ is compact. The GMM estimator is defined as follows:
	\begin{eqnarray*}
		\bar{f}_T(\theta) &=& \frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\\
		\widehat{\theta}_T &=& \underset{\theta \in \Theta}{\arg \min} \; \bar{f}_T(\theta)'W_T \bar{f}_T(\theta) 
	\end{eqnarray*}
The basic assumptions that make this problem well-defined are as follows.

\paragraph{Strict Stationarity} The sequence $\{v_t\colon -\infty <t <\infty\}$ of random $r$-vectors is a strictly stationary process with sample space $\mathcal{V}\subseteq \mathbb{R}^r$. Importantly, this implies that the expectations of \emph{any} functions of $v_t$ are constant over time.

\paragraph{Population Moment Condition} $E[f(v_t, \theta_0)]=0$ for some $\theta_0 \in \mbox{interior}(\Theta)$.

\paragraph{Global Identification} For any $\widetilde{\theta}\in \Theta$ such that $\widetilde{\theta}\neq \theta_0$, $E[f(v_t,\widetilde{\theta})]\neq 0 $.


\paragraph{Weighting Matrix} The weighting matrix $W_T$ is positive semi-definite and converges in probability to a postitive definite constant matrix $W$.

\subsection{Regularity Conditions}


\paragraph{Regularity Conditions for Moment Functions}
The $q$ moment functions $f\colon \mathcal{V}\times \Theta \rightarrow \mathbb{R}^q$ satisfy the following conditions:
	\begin{enumerate}[(i)]
		\item $f$ is $v_t$-almost surely continuous on $\Theta$
		\item $E[f(v_t, \theta)]<\infty$ exists and is continuous on $\Theta$
	\end{enumerate}

\paragraph{Regularity Conditions for Derivative Matrix}
	\begin{enumerate}[(i)]
		\item The $q\times p$ matrix $\nabla_\theta f(v_t, \theta)'$ exists and is $v_t$-almost continuous on $\Theta$ 
		\item $E[\nabla_{\theta}f(v_t, \theta_0)]<\infty$ exists and is continuous in a  neighborhood $N_\epsilon$ of $\theta_0$
		\item $\underset{\theta \in N_\epsilon}{\sup}\left| \left| T^{-1} \sum_{t=1}^T \nabla_\theta f(v_t, \theta) - E[\nabla_\theta f(v_t, \theta)]\right|\right| \overset{p}{\rightarrow} 0$
	\end{enumerate}

\paragraph{Regularity Conditions for Variance of Sample Moment Conditions}
\begin{enumerate}[(i)]
	\item 
\end{enumerate}

\subsection{Basic Asymptotic Results}
Under the set of assumptions given above, we obtain the following basic asymptotic results.

\paragraph{Consistency of GMM Estimator}

\paragraph{Asymptotic Normality of GMM Estimator}

\paragraph{Asymptotic Normality of Estimated Sample Moment} Under standard regularity conditions, most importantly that $E[f(v_t, \theta_0)]=0$, 
	$$W_T^{1/2} \sqrt{T} \bar{f}_T(\widehat{\theta}_T) \overset{d}{\rightarrow} $$





\subsection{Identifying and Overidentifying Restrictions}
The ``Generalized'' in Generalized Method of Moments, comes from the fact that we may use more moment conditions in estimation than we have parameters to estimate. In this case we say that our estimator is ``overidentified.'' In the overidentified case, the GMM estimator does \emph{not} in fact use all of the information contained in the moment conditions for estimation. This turns out to be \emph{crucial} because it is what allows us to carry out specification tests and moment selection. The key point is that GMM estimation can be viewed as a \emph{plain-vanilla method of moments} problem for a \emph{transformed} set of moment conditions.

The population moment condition for GMM estimation is $E[f(v_t, \theta_0)]=0$ where $f$ is $q\times 1$. If $f$ is differentiable and we can interchange expectation and derivative, then


\section{Andrews' GMM Moment Selection Criteria}

\subsection{Overview}
The consistency and asymptotic normality results for GMM estimation reviewed above rely on the assumption that the moment conditions used in estimation are correct. That is, they assume that $E[f(v_t,\theta_0)]=0$. But what if we are unsure of this assumption? In many real-world applications we have a fairly large collection of moment functions, the $q$ elements of $f$, some of which may have been derived under different economic or statistical assumptions that others. It could easily be the case that only \emph{some} of the moment functions in $f$ satisfy the moment conditions, while others do not. To take a simple example, we may have a collection of instrumental variables that arise from different sources or different assumptions on the DGP. Perhaps only some of these instruments are truly exogenous but we are unsure which. Andrews (1999) proposes a family of \emph{moment selection criteria} (MSC) for this situation, in which the aim is to consistently select \emph{any and all} elements of $f$ that satisfy the moment condition, and eliminate those that do not. 

Roughly speaking, the intuition is as follows. When we studied AIC, BIC and friends, we discussed how the maximized log-likelihood measures model fit but unfairly advantages models with more parameters. The various model selection criteria we examined amounted to adding some kind of ``penalty'' term to correct for this by \emph{penalizing} more complicated models. In a similar vein, so long as we have more moment conditions than parameters, the $J$-test statistic provides a measure of how well the data ``fit'' the moment conditions: the bigger the statistic, the greater the evidence that the moment conditions are violated. The problem is that $J$-test statistic tends to increase as we add additional moment conditions \emph{even if they are correct}. Thus, if we simply compared $J$-statistics, we would be led to select \emph{too few} moment conditions. To correct for this, Andrews (1999) considers a variety of ``bonus terms'' that \emph{reward} estimators based on a lager number of moment conditions. Using this idea, he derives GMM analogues of AIC, BIC and the Hannan-Quinn information criterion, and studies the conditions under which a bonus term will yield consistent moment selection.

\subsection{Notation}
Let $f_{max}$ be a $(q\times 1)$ vector containing all of the moment functions under consideration. Let $c$ be a \emph{selection vector}, a $(q\times 1)$ vector of ones and zeros indicating which elements of $f_{max}$ we use in estimation for a \emph{particular candidate specification}. Let $\mathcal{C}$ denote the set of all candidates and $|c|$ denote the number of moment conditions used to estimate candidate $c$. Naturally, we require that there are at least as many moment conditions as parameters to estimate.

Let $\widehat{\theta}_T(c)$ be the (efficient two-step) GMM estimator based on $E[f(v_t,\theta, c)]=0$ and let $V_\theta(c) = \left[G_0(c) S(c)^{-1} G_0(c) \right]^{-1}$ where $G_0(c) = E[\nabla_\theta' f(v_t, \theta_0; c)]$ and $S(c) = \lim_{T\rightarrow\infty} Var\left[\frac{1}{\sqrt{T}}\sum_{t=1}^T f(v_t, \theta_0;c)\right]$


So how should we choose $c$? Talk about why identification is tricky in this setting. Explain about asymptotic efficiency. Adding moment valid moment conditions can never increase asymptotic variance (give reference or maybe even the proof) so it makes sense to use any and all correctly specified moment conditions.

Define $J_T(c) = \left[\frac{1}{\sqrt{T}}\sum_{t=1}^T f(v_t, \widehat{\theta}_T(c);c) \right]' \widehat{S}_T(c)^{-1}\left[\frac{1}{\sqrt{T}}\sum_{t=1}^T f(v_t, \widehat{\theta}_T(c);c) \right]$

Consider moment selection criteria of the form $\mbox{MSC}(c) = J_T(c) - B(T, |c|)$ where $B$ is a ``bonus term.''

Select according to $\widehat{c}_T = \underset{c\in\mathcal{C}}{\arg\min} \;\mbox{MSC}(c)$

Distribution of J-test under correct specification.

Rate of divergence of J-test statistic under mis-specification. Centered covariance matrix estimator.

\subsection{Consistent Selection}

\paragraph{Regularity Conditions for the $J$-test Statistic}
	\begin{enumerate}[(i)]
		\item If $E[f(v_t, \theta;c)] =0$ for a unique $\theta \in \Theta$, then $J_T(c) \overset{d}{\rightarrow} \chi^2_{|c| -p}$
		\item If $E[f(v_t, \theta;c)] \neq 0$ for a \emph{all} $\theta \in \Theta$ then $T^{-1} J_T(c) \overset{p}{\rightarrow} a(c)$, a finite, positive constant that may depend on $c$.
	\end{enumerate}

\todo[inline]{Need to talk about this assumption in terms of low-level conditions, and what happens when GMM is mis-specified. Also need to talk about centered covariance matrix estimator.}

\paragraph{Regularity Conditions for Bonus Term}
	\begin{enumerate}[(i)]
		\item $h(\cdot)$ is strictly increasing
		\item $\kappa_T \rightarrow \infty$ as $T\rightarrow \infty$ and $\kappa_T =o(T)$
	\end{enumerate}

\paragraph{Identification Conditions}
	\begin{enumerate}[(i)]
		\item $\mathcal{MZ}^0 = \{c_0\}$
		\item $E[f(v_t, \theta_0; c_0)] = 0$ and $E[f(v_t, \theta; c_0)]\neq 0$ for any $\theta \neq \theta_0$
	\end{enumerate}


\begin{thm}
	Under the preceding assumptions, $\widehat{c}_T \overset{p}{\rightarrow} c_0$.
\end{thm}
\begin{proof}
We're trying to show that the moment conditions $\widehat{c}_T$ selected by our criterion are consistent for the maximal set $c_0$ of correct moment conditions. By definition $\widehat{c}_T = \underset{c\in \mathcal{C}}{\arg \min} \; MSC_T(c)$, so we need to show that
	$$\lim_{T\rightarrow \infty} P\left[\left\{MSC_T(c) - MSC_T(c_0)>0, \; \forall c \neq c_0\right\} \right] = 1$$
To simplify the notation, define
	\begin{eqnarray*}
		\Delta_T(c, c_0) &=& MSC_T(c) - MSC_T(c_0)\\
						&=& \left[J_T(c) -h(|c|)\kappa_T \right] - \left[J_T(c_0) -h(|c_0|)\kappa_T \right]\\
						&=& \left[J_T(c) - J_T(c_0)\right] + \kappa_T \left[h(|c_0|) - h(|c|) \right]
	\end{eqnarray*}
Now, we are interested in $\Delta_T(c, c_0)$ \emph{only} for situations in which $c\neq c_0$. Subject to this restriction, there are two cases, which we consider in turn. 
\paragraph{Case 1} Consider $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In this case the first Regularity Condition for the $J$-test Statistic applies to \emph{both} $c_1$ \emph{and} $c_0$ and we have 
	$$J_T(c_1) - J_T(c_0) \overset{d}{\rightarrow} \chi^2_{|c_1| - p} - \chi^2_{|c_0|-p} = O_p(1)$$
By the first Identification Condition, $c_0$ is the \emph{unique} maximal set of correct moment conditions. Hence $|c_0| > |c_1|$. Now, by the first Regularity Condition for the Bonus Term, $h$ is strictly increasing. It follows that $h(|c_0|) - h(|c_1|)>0$. By the second Regularity Condition for the Bonus Term, $\kappa_T \rightarrow \infty$. Thus, 
	$$\kappa_T \left[h(|c_0|) - h(|c|) \right] \rightarrow \infty$$ 
It follows that $\Delta_T(c_1, c_0) \rightarrow \infty$ and we obtain our desired result.


\paragraph{Case 2} Consider $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In this case, the \emph{first} Regularity Condition for the $J$-test Statistic applies to $c_0$, while the \emph{second} applies to $c_2$ so we have
	$$T^{-1}\left[J_T(c_2) - J_T(c_0) \right] = a(c_2) + o_p(1) - T^{-1} O_p(1)$$
Now, whatever the value $\left[h(|c_0|) - h(|c|) \right]$ happens to be, it is definitely finite since $h$ is strictly increasing by the first Regularity Condition for the Bonus Term, and both $|c|$ and $|c_0|$ are finite. By the second Regularity Condition for the Bonus Term, $\kappa_T = o(T)$. Hence,
	$$T^{-1} \kappa_T\left[h(|c_0|) - h(|c|) \right] = o(1)$$
Putting the pieces together, we have
\begin{eqnarray*}
	T^{-1} \Delta_T(c_2, c_0) &=& a(c_2) + o_p(1) - T^{-1}O_p(1) + o(1)\\
	&=& a(c_2) + o_p(1)
\end{eqnarray*}
By the second Regularity Condition for the $J$-test Statistic, $a(c_2) >0$. Thus, $T^{-1}\Delta_T(c_2,c_0) >0$ with probability approaching one as $T\rightarrow \infty$. It follows that $\Delta_T(c_2,c_0)  \rightarrow \infty$ with probability approaching one as $T\rightarrow \infty$, as required.
\end{proof}

\subsection{Which Criteria Are Consistent?}
Recall from above that:
\begin{eqnarray*}
	\mbox{GMM-BIC}(c) &=& J_T(c) - \left(|c| - p\right)\log(T)\\
	\mbox{GMM-HQ}(c) &=& J_T(c) - 2.01\left(|c| - p\right)\log(\log(T))\\
	\mbox{GMM-AIC}(c) &=& J_T(c) - 2\left(|c| - p\right)\log(T)
\end{eqnarray*}
We see immediately that GMM-AIC does \emph{not} satisfy the necessary conditions for consistency, since $\kappa_T = 2$ does not diverge as $T\rightarrow \infty$. In constrast, both the GMM-BIC and GMM-HQ diverge as $T\rightarrow \infty$, so we simply need to check the requirement that $\kappa_T = o(T)$. For GMM-BIC we have 
	$$\lim_{T \rightarrow \infty} \frac{\log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{T} = 0$$
by l'H\^{o}pital's rule, and similarly for GMM-HQ
	$$\lim_{T \rightarrow \infty} \frac{\log \log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{\log T} = 0$$
Thus both GMM-BIC and GMM-HQ provide consistent moment selection.
 
\subsection{Asymptotics for GMM-AIC}
We saw in the previous subsection that GMM-AIC does not satisfy the sufficient conditions for consistent moment selection. The question remains: how does this criterion behave in the limit? To answer this question, we revisit the proof of consistent selection from above. It turns out that GMM-AIC behaves \emph{differently} in the two cases considered in the proof. Combining them, we will see that GMM-AIC is \emph{not} a consistent moment selection criterion.

\paragraph{Case 2} In this case, we examined $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In other words, the moment conditions indexed by $c_2$ are \emph{not} satisfied for \emph{any} parameter value $\theta$. Asymptotically, GMM-AIC will \emph{never} select such a set of moment conditions. To see why, recall that $\kappa_T = 2$ for GMM-AIC. Although it does not diverge, this choice of $\kappa_T$ is \emph{still} $o(T)$. Thus, the argument from Case 2 \emph{still applies} to the GMM-AIC. We did not in fact use the assumption that $\kappa_T$ diverges in the proof of this case! 

\paragraph{Case 1} In this case, we examined $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In other words, we considered a situation in which there \emph{is} a parameter vector $\theta_1$ at which the moment conditions indexed by $c_1$ are satisfied. Now, the difference of $J$-test statistics continues to be $O_p(1)$ regardless of the choice of $\kappa_T$, provided the regularity conditions are satisfied. Thus, substituting $\kappa_T = 2$, we have
	$$\Delta_T(c_1,c_0) = O_p(1) + 2\left[h(|c_0|) - h(|c|) \right]$$
But since the second term is a \emph{constant}, this is simply $\Delta_T(c_1,c_0) = O_p(1)$. In other words, the GMM-AIC is a \emph{random variable}, even in the limit as $T\rightarrow \infty$.

So where does this leave us? In Case 2 GMM-AIC cosistently selects $c_0$, but in Case 1 GMM-AIC is \emph{random even in the limit}. Putting these two results together, we see that, although it will never select a set of false moment conditions, GMM-AIC chooses \emph{randomly} among the set of correct moment conditions. In other words, it will not necessarily select $c_0$ as $T\rightarrow \infty$. 

\subsection{Problems with Andrews' Approach}
Irrelevant moment conditions: Hall \& Peixe. Identification condition is stronger than it sounds, can easily fail in simple examples. Comparatively rare that we are actually interested in determining whether our moment conditions are correct. More typically, the goal is to estimate $\theta$. This motivates FMSC, descibed in the next section.

\section{The Focused Moment Selection Criterion}
DiTraglia (2014). The motivation is simply that it's comparatively rare that we are actually interested in determining whether our moment conditions are correct. More typically, the goal is to estimate $\theta$.


\todo[inline]{Add asymptotics for Andrews criteria under my asymptotics.}


\end{document}