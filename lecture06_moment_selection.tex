
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 6: Moment Selection for GMM}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Review of GMM}
The best reference for GMM is Hall (2005). This review follows Chapter 3.
\subsection{The Basic Idea}
	$$\widehat{\theta} = \underset{\theta \in \Theta}{\arg \min} \left[\frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\right]' W_T \left[ \frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\right]$$

\subsection{Basic Assumptions for GMM}
Let $f$ be a $q$-vector of functions of an observable random $r$-vector $v_t$ and a $p$-vector of parameters $\theta \in \Theta \subseteq \mathbb{R}^p$. The basic assumptions for GMM estimation are as follows.

\paragraph{Strict Stationarity} The sequence $\{v_t\colon -\infty <t <\infty\}$ of random $r$-vectors is a strictly stationary process with sample space $\mathcal{V}\subseteq \mathbb{R}^r$. Importantly, this implies that the expectations of \emph{any} functions of $v_t$ are constant over time.

\paragraph{Regularity Conditions for Moment Functions}
The $q$ moment functions $f\colon \mathcal{V}\times \Theta \rightarrow \mathbb{R}^q$ satisfy the following conditions:
	\begin{enumerate}[(i)]
		\item $f$ is $v_t$-almost surely continuous on $\Theta$
		\item $E[f(v_t, \theta)]<\infty$ exists $\forall \theta \in \Theta$
		\item $E[f(v_t,\theta)]$ is continuous on $\Theta$
	\end{enumerate}

\paragraph{Population Moment Condition} $E[f(v_t, \theta_0)]=0$

\paragraph{Regularity Conditions for Derivative Matrix}
	\begin{enumerate}[(i)]
		\item The $q\times p$ derivative matrix $\nabla_\theta f(v_t, \theta)'$ exists and is $v_t$-almost continuous on $\Theta$
		\item $\theta_0 \in \mbox{interior}(\Theta)$ [Needed for Mean-value Expansion]
		\item $E[\nabla_{\theta}f(v_t, \theta_0)]<\infty$ exists
	\end{enumerate}

\paragraph{Global Identification} For any $\widetilde{\theta}\in \Theta$ such that $\widetilde{\theta}\neq \theta_0$, $E[f(v_t,\widetilde{\theta})]\neq 0 $.

\paragraph{Local Identification}

\paragraph{Weighting Matrix} The weighting matrix $W_T$ is positive semi-definite and converges in probability to a postitive definite constant matrix $W$.


\subsection{Identifying and Overidentifying Restrictions}
The ``Generalized'' in Generalized Method of Moments, comes from the fact that we may use more moment conditions in estimation than we have parameters to estimate. In this case we say that our estimator is ``overidentified.'' In the overidentified case, the GMM estimator does \emph{not} in fact use all of the information contained in the moment conditions for estimation. This turns out to be \emph{crucial} because it is what allows us to carry out specification tests and moment selection. The key point is that GMM estimation can be viewed as a \emph{plain-vanilla method of moments} problem for a \emph{transformed} set of moment conditions.

The population moment condition for GMM estimation is $E[f(v_t, \theta_0)]=0$ where $f$ is $q\times 1$. If $f$ is differentiable and we can interchange expectation and derivative, then


\section{Andrews' GMM Moment Selection Criteria}

\subsection{Overview}
The consistency and asymptotic normality results for GMM estimation reviewed above rely on the assumption that the moment conditions used in estimation are correct. That is, they assume that $E[f(v_t,\theta_0)]=0$. But what if we are unsure of this assumption? In many real-world applications we have a fairly large collection of moment functions, the $q$ elements of $f$, some of which may have been derived under different economic or statistical assumptions that others. It could easily be the case that only \emph{some} of the moment functions in $f$ satisfy the moment conditions, while others do not. To take a simple example, we may have a collection of instrumental variables that arise from different sources or different assumptions on the DGP. Perhaps only some of these instruments are truly exogenous but we are unsure which. Andrews (1999) proposes a family of \emph{moment selection criteria} (MSC) for this situation, in which the aim is to consistently select \emph{any and all} elements of $f$ that satisfy the moment condition, and eliminate those that do not. 

Roughly speaking, the intuition is as follows. When we studied AIC, BIC and friends, we discussed how the maximized log-likelihood measures model fit but unfairly advantages models with more parameters. The various model selection criteria we examined amounted to adding some kind of ``penalty'' term to correct for this by \emph{penalizing} more complicated models. In a similar vein, so long as we have more moment conditions than parameters, the $J$-test statistic provides a measure of how well the data ``fit'' the moment conditions: the bigger the statistic, the greater the evidence that the moment conditions are violated. The problem is that $J$-test statistic tends to increase as we add additional moment conditions \emph{even if they are correct}. Thus, if we simply compared $J$-statistics, we would be led to select \emph{too few} moment conditions. To correct for this, Andrews (1999) considers a variety of ``bonus terms'' that \emph{reward} estimators based on a lager number of moment conditions. Using this idea, he derives GMM analogues of AIC, BIC and the Hannan-Quinn information criterion, and studies the conditions under which a bonus term will yield consistent moment selection.

\subsection{Notation}
Talk about why identification is tricky in this setting.

\subsection{Consistent Selection}

\paragraph{Regularity Conditions for the $J$-test Statistic}
	\begin{enumerate}[(i)]
		\item If $E[f(v_t, \theta;c)] =0$ for a unique $\theta \in \Theta$, then $J_T(c) \overset{d}{\rightarrow} \chi^2_{|c| -p}$
		\item If $E[f(v_t, \theta;c)] \neq 0$ for a \emph{all} $\theta \in \Theta$ then $T^{-1} J_T(c) \overset{p}{\rightarrow} a(c)$, a finite, positive constant that may depend on $c$.
	\end{enumerate}

\todo[inline]{Need to talk about this assumption in terms of low-level conditions, and what happens when GMM is mis-specified. Also need to talk about centered covariance matrix estimator.}

\paragraph{Regularity Conditions for Bonus Term}
	\begin{enumerate}[(i)]
		\item $h(\cdot)$ is strictly increasing
		\item $\kappa_T \rightarrow \infty$ as $T\rightarrow \infty$ and $\kappa_T =o(T)$
	\end{enumerate}

\paragraph{Identification Conditions}
	\begin{enumerate}[(i)]
		\item $\mathcal{MZ}^0 = \{c_0\}$
		\item $E[f(v_t, \theta_0; c_0)] = 0$ and $E[f(v_t, \theta; c_0)]\neq 0$ for any $\theta \neq \theta_0$
	\end{enumerate}


\begin{thm}
	Under the preceding assumptions, $\widehat{c}_T \overset{p}{\rightarrow} c_0$.
\end{thm}
\begin{proof}
We're trying to show that the moment conditions $\widehat{c}_T$ selected by our criterion are consistent for the maximal set $c_0$ of correct moment conditions. By definition $\widehat{c}_T = \underset{c\in \mathcal{C}}{\arg \min} \; MSC_T(c)$, so we need to show that
	$$\lim_{T\rightarrow \infty} P\left[\left\{MSC_T(c) - MSC_T(c_0)>0, \; \forall c \neq c_0\right\} \right] = 1$$
To simplify the notation, define
	\begin{eqnarray*}
		\Delta_T(c, c_0) &=& MSC_T(c) - MSC_T(c_0)\\
						&=& \left[J_T(c) -h(|c|)\kappa_T \right] - \left[J_T(c_0) -h(|c_0|)\kappa_T \right]\\
						&=& \left[J_T(c) - J_T(c_0)\right] + \kappa_T \left[h(|c_0|) - h(|c|) \right]
	\end{eqnarray*}
Now, we are interested in $\Delta_T(c, c_0)$ \emph{only} for situations in which $c\neq c_0$. Subject to this restriction, there are two cases, which we consider in turn. 
\paragraph{Case 1} Consider $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In this case the first Regularity Condition for the $J$-test Statistic applies to \emph{both} $c_1$ \emph{and} $c_0$ and we have 
	$$J_T(c_1) - J_T(c_0) \overset{d}{\rightarrow} \chi^2_{|c_1| - p} - \chi^2_{|c_0|-p} = O_p(1)$$
By the first Identification Condition, $c_0$ is the \emph{unique} maximal set of correct moment conditions. Hence $|c_0| > |c_1|$. Now, by the first Regularity Condition for the Bonus Term, $h$ is strictly increasing. It follows that $h(|c_0|) - h(|c_1|)>0$. By the second Regularity Condition for the Bonus Term, $\kappa_T \rightarrow \infty$. Thus, 
	$$\kappa_T \left[h(|c_0|) - h(|c|) \right] \rightarrow \infty$$ 
It follows that $\Delta_T(c_1, c_0) \rightarrow \infty$ and we obtain our desired result.


\paragraph{Case 2} Consider $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In this case, the \emph{first} Regularity Condition for the $J$-test Statistic applies to $c_0$, while the \emph{second} applies to $c_2$ so we have
	$$T^{-1}\left[J_T(c_2) - J_T(c_0) \right] = a(c_2) + o_p(1) - T^{-1} O_p(1)$$
Now, whatever the value $\left[h(|c_0|) - h(|c|) \right]$ happens to be, it is definitely finite since $h$ is strictly increasing by the first Regularity Condition for the Bonus Term, and both $|c|$ and $|c_0|$ are finite. By the second Regularity Condition for the Bonus Term, $\kappa_T = o(T)$. Hence,
	$$T^{-1} \kappa_T\left[h(|c_0|) - h(|c|) \right] = o(1)$$
Putting the pieces together, we have
\begin{eqnarray*}
	T^{-1} \Delta_T(c_2, c_0) &=& a(c_2) + o_p(1) - T^{-1}O_p(1) + o(1)\\
	&=& a(c_2) + o_p(1)
\end{eqnarray*}
By the second Regularity Condition for the $J$-test Statistic, $a(c_2) >0$. Thus, $T^{-1}\Delta_T(c_2,c_0) >0$ with probability approaching one as $T\rightarrow \infty$. It follows that $\Delta_T(c_2,c_0)  \rightarrow \infty$ with probability approaching one as $T\rightarrow \infty$, as required.
\end{proof}

\subsection{Which Criteria Are Consistent?}
Recall from above that:
\begin{eqnarray*}
	\mbox{GMM-BIC}(c) &=& J_T(c) - \left(|c| - p\right)\log(T)\\
	\mbox{GMM-HQ}(c) &=& J_T(c) - 2.01\left(|c| - p\right)\log(\log(T))\\
	\mbox{GMM-AIC}(c) &=& J_T(c) - 2\left(|c| - p\right)\log(T)
\end{eqnarray*}
We see immediately that GMM-AIC does \emph{not} satisfy the necessary conditions for consistency, since $\kappa_T = 2$ does not diverge as $T\rightarrow \infty$. In constrast, both the GMM-BIC and GMM-HQ diverge as $T\rightarrow \infty$, so we simply need to check the requirement that $\kappa_T = o(T)$. For GMM-BIC we have 
	$$\lim_{T \rightarrow \infty} \frac{\log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{T} = 0$$
by l'H\^{o}pital's rule, and similarly for GMM-HQ
	$$\lim_{T \rightarrow \infty} \frac{\log \log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{\log T} = 0$$
Thus both GMM-BIC and GMM-HQ provide consistent moment selection.
 
\subsection{Asymptotics for GMM-AIC}
We saw in the previous subsection that GMM-AIC does not satisfy the sufficient conditions for consistent moment selection. The question remains: how does this criterion behave in the limit? To answer this question, we revisit the proof of consistent selection from above. It turns out that GMM-AIC behaves \emph{differently} in the two cases considered in the proof. Combining them, we will see that GMM-AIC is \emph{not} a consistent moment selection criterion.

\paragraph{Case 2} In this case, we examined $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In other words, the moment conditions indexed by $c_2$ are \emph{not} satisfied for \emph{any} parameter value $\theta$. Asymptotically, GMM-AIC will \emph{never} select such a set of moment conditions. To see why, recall that $\kappa_T = 2$ for GMM-AIC. Although it does not diverge, this choice of $\kappa_T$ is \emph{still} $o(T)$. Thus, the argument from Case 2 \emph{still applies} to the GMM-AIC. We did not in fact use the assumption that $\kappa_T$ diverges in the proof of this case! 

\paragraph{Case 1} In this case, we examined $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In other words, we considered a situation in which there \emph{is} a parameter vector $\theta_1$ at which the moment conditions indexed by $c_1$ are satisfied. Now, the difference of $J$-test statistics continues to be $O_p(1)$ regardless of the choice of $\kappa_T$, provided the regularity conditions are satisfied. Thus, substituting $\kappa_T = 2$, we have
	$$\Delta_T(c_1,c_0) = O_p(1) + 2\left[h(|c_0|) - h(|c|) \right]$$
But since the second term is a \emph{constant}, this is simply $\Delta_T(c_1,c_0) = O_p(1)$. In other words, the GMM-AIC is a \emph{random variable}, even in the limit as $T\rightarrow \infty$.

So where does this leave us? In Case 2 GMM-AIC cosistently selects $c_0$, but in Case 1 GMM-AIC is \emph{random even in the limit}. Putting these two results together, we see that, although it will never select a set of false moment conditions, GMM-AIC chooses \emph{randomly} among the set of correct moment conditions. In other words, it will not necessarily select $c_0$ as $T\rightarrow \infty$. 

\section{The Focused Moment Selection Criterion}
DiTraglia (2014). The motivation is simply that it's comparatively rare that we are actually interested in determining whether our moment conditions are correct. More typically, the goal is to estimate $\theta$.

\todo[inline]{Add asymptotics for Andrews criteria under my asymptotics.}


\end{document}