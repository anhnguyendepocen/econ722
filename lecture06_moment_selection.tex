
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 6: Moment Selection for GMM}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Review of GMM}
The best reference for GMM is Hall (2005). This review follows Chapter 3.
\subsection{The Basic Idea}
	$$\widehat{\theta} = \underset{\theta \in \Theta}{\arg \min} \left[\frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\right]' W_T \left[ \frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\right]$$

\subsection{Basic Assumptions for GMM}
Let $f$ be a $q$-vector of functions of an observable random $r$-vector $v_t$ and a $p$-vector of parameters $\theta \in \Theta \subseteq \mathbb{R}^p$. The basic assumptions for GMM estimation are as follows.

\paragraph{Strict Stationarity} The sequence $\{v_t\colon -\infty <t <\infty\}$ of random $r$-vectors is a strictly stationary process with sample space $\mathcal{V}\subseteq \mathbb{R}^r$. Importantly, this implies that the expectations of \emph{any} functions of $v_t$ are constant over time.

\paragraph{Regularity Conditions for Moment Functions}
The $q$ moment functions $f\colon \mathcal{V}\times \Theta \rightarrow \mathbb{R}^q$ satisfy the following conditions:
	\begin{enumerate}[(i)]
		\item $f$ is $v_t$-almost surely continuous on $\Theta$
		\item $E[f(v_t, \theta)]<\infty$ exists $\forall \theta \in \Theta$
		\item $E[f(v_t,\theta)]$ is continuous on $\Theta$
	\end{enumerate}

\paragraph{Population Moment Condition} $E[f(v_t, \theta_0)]=0$

\paragraph{Regularity Conditions for Derivative Matrix}
	\begin{enumerate}[(i)]
		\item The $q\times p$ derivative matrix $\nabla_\theta f(v_t, \theta)'$ exists and is $v_t$-almost continuous on $\Theta$
		\item $\theta_0 \in \mbox{interior}(\Theta)$ [Needed for Mean-value Expansion]
		\item $E[\nabla_{\theta}f(v_t, \theta_0)]<\infty$ exists
	\end{enumerate}

\paragraph{Global Identification} For any $\widetilde{\theta}\in \Theta$ such that $\widetilde{\theta}\neq \theta_0$, $E[f(v_t,\widetilde{\theta})]\neq 0 $.

\paragraph{Local Identification}

\paragraph{Weighting Matrix} The weighting matrix $W_T$ is positive semi-definite and converges in probability to a postitive definite constant matrix $W$.


\subsection{Identifying and Overidentifying Restrictions}
The ``Generalized'' in Generalized Method of Moments, comes from the fact that we may use more moment conditions in estimation than we have parameters to estimate. In this case we say that our estimator is ``overidentified.'' In the overidentified case, the GMM estimator does \emph{not} in fact use all of the information contained in the moment conditions for estimation. This turns out to be \emph{crucial} because it is what allows us to carry out specification tests and moment selection. The key point is that GMM estimation can be viewed as a \emph{plain-vanilla method of moments} problem for a \emph{transformed} set of moment conditions.

The population moment condition for GMM estimation is $E[f(v_t, \theta_0)]=0$ where $f$ is $q\times 1$. If $f$ is differentiable and we can interchange expectation and derivative, then


\section{Andrews (1999)}

\subsection{Overview}
The consistency and asymptotic normality results for GMM estimation reviewed above rely on the assumption that the moment conditions used in estimation are correct. That is, they assume that $E[f(v_t,\theta_0)]=0$. But what if we are unsure of this assumption? In many real-world applications we have a fairly large collection of moment functions, the $q$ elements of $f$, some of which may have been derived under different economic or statistical assumptions that others. It could easily be the case that only \emph{some} of the moment functions in $f$ satisfy the moment conditions, while others do not. To take a simple example, we may have a collection of instrumental variables that arise from different sources or different assumptions on the DGP. Perhaps only some of these instruments are truly exogenous but we are unsure which. Andrews (1999) proposes a family of \emph{moment selection criteria} (MSC) for this situation, in which the aim is to consistently select \emph{any and all} elements of $f$ that satisfy the moment condition, and eliminate those that do not. 

Roughly speaking, the intuition is as follows. When we studied AIC, BIC and friends, we discussed how the maximized log-likelihood measures model fit but unfairly advantages models with more parameters. The various model selection criteria we examined amounted to adding some kind of ``penalty'' term to correct for this by \emph{penalizing} more complicated models. In a similar vein, so long as we have more moment conditions than parameters, the $J$-test statistic provides a measure of how well the data ``fit'' the moment conditions: the bigger the statistic, the greater the evidence that the moment conditions are violated. The problem is that $J$-test statistic tends to increase as we add additional moment conditions \emph{even if they are correct}. Thus, if we simply compared $J$-statistics, we would be led to select \emph{too few} moment conditions. To correct for this, Andrews (1999) considers a variety of ``bonus terms'' that \emph{reward} estimators based on a lager number of moment conditions. Using this idea, he derives GMM analogues of AIC, BIC and the Hannan-Quinn information criterion, and studies the conditions under which a bonus term will yield consistent moment selection.

\subsection{Notation}
Talk about why identification is tricky in this setting.

\subsection{Consistent Selection}

\paragraph{Regularity Conditions for the $J$-test Statistic}
	\begin{enumerate}[(i)]
		\item If $E[f(v_t, \theta;c)] =0$ for a unique $\theta \in \Theta$, then $J_T(c) \overset{d}{\rightarrow} \chi^2_{|c| -p}$
		\item If $E[f(v_t, \theta;c)] \neq 0$ for a \emph{all} $\theta \in \Theta$ then $T^{-1} J_T(c) \overset{p}{\rightarrow} a(c)$, a finite, positive constant that may depend on $c$.
	\end{enumerate}

\paragraph{Regularity Conditions for Bonus Term}
	\begin{enumerate}[(i)]
		\item $h(\cdot)$ is strictly increasing
		\item $\kappa_T \rightarrow \infty$ as $T\rightarrow \infty$ and $\kappa_T =o(T)$
	\end{enumerate}

\paragraph{Identification Conditions}
	\begin{enumerate}[(i)]
		\item $\mathcal{MZ}^0 = \{c_0\}$
		\item $E[f(v_t, \theta_0; c_0)] = 0$ and $E[f(v_t, \theta; c_0)]\neq 0$ for any $\theta \neq \theta_0$
	\end{enumerate}


\begin{thm}
	Under the preceding assumptions, $\widehat{c}_T \overset{p}{\rightarrow} c_0$.
\end{thm}
\begin{proof}
We're trying to show that the moment conditions $\widehat{c}_T$ selected by our criterion are consistent for the maximal set $c_0$ of correct moment conditions. By definition $\widehat{c}_T = \underset{c\in \mathcal{C}}{\arg \min} \; MSC_T(c)$, so what we need to establish is
	$$\lim_{T\rightarrow \infty} P\left[\left\{MSC_T(c_0) < MSC_T(c), \; \forall c \neq c_0\right\} \right] = 1$$
To simplify the notation, define
	\begin{eqnarray*}
		\Delta_T(c, c_0) &=& MSC_T(c) - MSC_T(c_0)\\
						&=& \left[J_T(c) -h(|c|)\kappa_T \right] - \left[J_T(c_0) -h(|c_0|)\kappa_T \right]\\
						&=& \left[J_T(c) - J_T(c_0)\right] + \kappa_T \left[h(|c_0|) - h(|c|) \right]
	\end{eqnarray*}
Now, we are interested in $\Delta_T(c, c_0)$ \emph{only} for situations in which $c\neq c_0$. Subject to this restriction, there are two cases, which we consider in turn. 
\paragraph{Case 1} Consider $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In this case the first Regularity Condition for the $J$-test Statistic applies to \emph{both} $c_1$ \emph{and} $c_0$ and we have 
	$$J_T(c_1) - J_T(c_0) \overset{d}{\rightarrow} \chi^2_{|c_1| - p} - \chi^2_{|c_0|-p} = O_p(1)$$
By the first Identification Condition, $c_0$ is the \emph{unique} maximal set of correct moment conditions. Hence $|c_0| > |c_1|$. Now, by the first Regularity Condition for the Bonus Term, $h$ is strictly increasing. It follows that $h(|c_0|) - h(|c_1|)>0$. By the second Regularity Condition for the Bonus Term, $\kappa_T \rightarrow \infty$. Thus, 
	$$\kappa_T \left[h(|c_0|) - h(|c|) \right] \rightarrow \infty$$ 
and it follows that $\Delta_T(c_1, c_0) \rightarrow \infty$.


\paragraph{Case 2} Consider $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In this case, the first Regularity Condition for the $J$-test Statistic applies to $c_0$, while the second applies to $c_2$.
\end{proof}





\section{DiTraglia (2014)}

\section{Other Approaches}

\end{document}