\documentclass[handout]{beamer}  

%Smaller gap at between top and bottom of block when there are displayed equations
\addtobeamertemplate{block begin}{\setlength\abovedisplayskip{0pt}}
{\setlength{\belowdisplayskip}{0pt}}


\usepackage{setspace}
\linespread{1.2}
\usepackage{amssymb, amsmath, amsthm} 
\usepackage{rotating}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{synttree}
\usepackage{verbatim}
\usepackage{fancybox}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds}
\usepackage{hyperref}
\usetikzlibrary{trees}
\newcommand{\p}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}


%\setbeamertemplate{blocks}[rounded][shadow=true] 
%gets rid of bottom navigation bars
\setbeamertemplate{footline}{
   \begin{beamercolorbox}[ht=4ex,leftskip=0.3cm,rightskip=0.3cm]{author in head/foot}
%    \usebeamercolor{UniBlue}
    \vspace{0.1cm}
    %\insertshorttitle \ - \insertdate 
    \hfill \insertframenumber / \inserttotalframenumber
   \end{beamercolorbox}
   \vspace*{0.1cm}
} 

%Put the math in red
\setbeamercolor{math text}{fg=red}

%gets rid of navigation symbols
\setbeamertemplate{navigation symbols}{}


%Include or exclude the notes?
%\setbeameroption{show notes}
\setbeameroption{hide notes}

\title[Econ 722]{High Dimensional Forecasting} 
\author[F. DiTraglia]{Francis J.\ DiTraglia}
\institute{University of Pennsylvania}
\date{Econ 722}


\begin{document} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}[plain]
	\titlepage 
	

\end{frame} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{What Have We Learned So Far?}
    
    \begin{enumerate}
    	\item Classical Model Selection
    	\item Focused Model Selection
    	\item Moment Selection for GMM
    	\item Shrinkage Estimation
    	\item Factor Models
    \end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Today's Lecture -- A Bit of a Grab Bag}
    
``Tie everything together'' by looking at high-dimensional forecasting problems, consider some avenues for future research. Also give a brief overview of some topics I had hoped to cover in more detail but didn't have time for.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Some Special Problems in High-dimensional Problems}
    

\begin{block}
	{Estimation Uncertainty} 
	We've already seen that OLS can perform very badly if the number of regressors is large relative to sample size.
\end{block}

\begin{block}
	{Best Subsets Infeasible}
	With more than 30 or so regressors, we can't check all subsets of predictors making classical model selection problematic.
\end{block}

\begin{block}
	{Noise Accumulation} 
	Large $N$ is supposed to help in factor models: averaging over the cross-section gives a consistent estimator of factor space. This can fail in practice, however, since it relies on the assumption that the factors are \emph{pervasive}. See Boivin \& Ng (2006).
\end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Main References}
	\begin{block}
		{Stock \& Watson (2006) -- ``Forecasting with Many Predictors''} Overview of high-dimesional forecasting with a review of forecast combination, factor models, and Bayesian approaches.
	\end{block}
	\begin{block}
		{Ng (2013) -- ``Variable Selection in Predictive Regressions''}
		Reviews and relates a number of shrinkage \& selection methods.
	\end{block}
	\begin{block}
		{Stock \& Watson (2012)}
		Examines a wide range of shrinkage procedures to see if they can improve on diffusion index forecasts.
	\end{block}
	\begin{block}
		{Kim \& Nelson (2013)}
		``Horse Race'' of various factor and shrinkage methods for forecasting.
	\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Diffusion Index Forecasting -- Stock \& Watson (2002a,b)}
 \framesubtitle{JASA paper has the theory, JBES paper has macro forecasting example.}

\begin{block}
	{Basic Setup}
	Forecast scalar time series $y_{t+1}$ using $N$-dimensional collection of time series $X_t$ where we observe periods $t = 1, \hdots, T$.
\end{block}

\begin{block}
	{Assumption}
	Static representation of Dynamic Factor Model:
	\begin{eqnarray*}
		y_{t}&\alert{=}& \beta' F_t + \gamma(L)y_t + \epsilon_{t+1}\\
		\alert{X_t} &\alert{=}& \alert{\Lambda F_t + e_t}
	\end{eqnarray*}
\end{block}

\begin{block}
	{``Direct'' Multistep Ahead Forecasts}
	``Iterated'' forecast would be linear in $F_t$, $y_t$ and lags:
	$$y_{t+h}^h = \alpha_h + \beta_h(L) + \gamma_h(L)+ \gamma_h(L)y_t + \epsilon^h_{t+h}$$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\begin{center}
	\Huge This is really just PCR
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Diffusion Index Forecasting -- Stock \& Watson (2002a,b)}
    
    \begin{block}
    	{Estimation Procedure}
    	\begin{enumerate}
    		\item Data Pre-processing
    			\begin{enumerate}
    			\item Transform all series to stationarity (logs or first difference)
				\item Center and standardize all series
				\item Remove outliers (ten times IQR from median)
				\item Optionally augment $X_t$ with lags
    			\end{enumerate}
    		\item Estimate the Factors
    			\begin{itemize}
    				\item No missing observations: PCA on $X_t$ to estimate $\widehat{F}_t$ 
		\item Missing observations/Mixed-frequency: EM-algorithm
    			\end{itemize}
    		\item Fit the Forecasting Regression
    		\begin{itemize}
    			\item Regress $y_{t}$ on a constant and lags of $\widehat{F}_t$ and $y_t$ to estimate the parameters of the ``Direct'' multistep forecasting regression.
    		\end{itemize}
    	\end{enumerate}
    \end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Diffusion Index Forecasting -- Stock \& Watson (2002b)}
    
\alert{Recall from last time that, under certain assumptions, PCA consistently estimates the space spanned by the factors. Broadly similar assumptions are at work here.}

\vspace{2em}

\begin{block}
	{Main Theoretical Result}
	Moment restrictions on $(\epsilon, e, F)$ plus a ``rank condition'' on $\Lambda$ imply that the MSE of the procedure on the previous slide converges to that of the infeasible optimal procedure, provided that  $N,T \rightarrow \infty$.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Diffusion Index Forecasting -- Stock \& Watson (2002a)}
    
\begin{block}
	{Forecasting Experiment}
	\begin{itemize}
		\item Simulated real-time forecasting of eight monthly macro variables from 1959:1 to 1998:12
		\item Forecasting Horizons: 6, 12, and 24 months
		\item ``Training Period'' 1959:1 through 1970:1
		\item Predict $h$-steps ahead out-of-sample, roll and re-estimate.
		\item BIC to select lags and \# of Factors in forecasting regression
		\item Compare Diffusion Index Forecasts to Benchmark
			\begin{itemize}
				\item AR only
				\item Factors only
				\item AR + Factors
			\end{itemize}
	\end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Diffusion Index Forecasting -- Stock \& Watson (2002a)}
   \begin{block}
   	{Empirical Results}
   	\begin{itemize}
   		\item Factors provide a substantial improvement over benchmark forecasts in terms of MSPE
   		\item Six factors explain 39\% of the variance in the 215 series; twelve explain 53\%  
   		\item Using all 215 series tends to work better than restricting to balanced panel of 149 (PCA estimation)
   		\item Augmenting $X_t$ with lags isn't helpful
   	\end{itemize}
   \end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
% 	\frametitle{Another Line of Research on Factor Models}

% The assumption of a factor structure sure is convenient, but how does it relate to other kinds of assumptions we're more familiar with? (e.g.\ everything follows a VAR). Does this structure come out of our economic models, or is it just an assumption of convenience? I haven't seen much work on this, but it seems important to me. Two recent papers that consider questions along these lines: Onatski and Ruge (2012), Dufour \& Stevanovic
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{What about Ridge and Lasso?}
    
 \begin{block}
 	{Basic Idea}
 	Diffusion index forecasts are really just PCR. Why not try Ridge or Lasso with all predictors rather than estimating factors?
 \end{block}

 \begin{block}
 	{De Mol, Giannone \& Reichlin (2008)}
 	\begin{itemize}
 		\item Compare PCA-based factor forecasts to Ridge and Lasso
 		\item In a small out-of-sample experiment, Ridge and Lasso with appropriate penalty parameters give results comparable to diffusion index.
 		\item Analyze asymptotics of Ridge under assumptions typically used to justify PCA
 	\end{itemize}
 
 \end{block}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Other Ways of Extracting Factors}
    
    \begin{block}
    	{Sparse PCA}
    	Add a Lasso-type penalty to the ``regression'' formulation of PCA: encourage the factors to load on small number of variables.
    \end{block}

    \begin{block}
    	{Independent Components Analysis (ICA)}
    	Extract factors that maximize non-Gaussianity
    \end{block}

\vspace{1em}

\alert{Both of these are considered in Kim \& Swanson (2014) and seem to work very well when combined with second-stage shrinkage.}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{To Target or Not to Target?}
    
    \begin{block}
    	{Problem with PCA and Friends}
    	Completely ignores $Y$ in constructing the factors! Should we take the forecast target into account when extracting factors?
	\end{block}

\begin{block}
	{Some References}
	\begin{itemize}
		\item Bai \& Ng (2008) -- Forecasting Economic Time Series Using Targeted Predictors
		\item Kelly \& Pruitt (2012) -- The Three-pass Regression Filter 
	\end{itemize}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Partial Least Squares (PLS)}

\begin{block}
	{As an Optimization Problem}
	Construct a sequence of linear combinations of $X$ that solve
	$$\underset{\alpha}{\max} \; Corr^2(\textbf{y}, X\alpha)Var(X\alpha)$$
subject to $||\alpha ||=1$ and the constraint that each PLS ``factor'' is orthogonal to the preceding ones.
\end{block}

\begin{block}
	{As a Probabilistic Model}
	``Shared'' factor $F_t$ and $X$-specific factor $Z_t$
	\begin{eqnarray*}
		Y_t &=& \mu_Y + \Lambda_Y F_t + \epsilon_t\\
		X_t &=& \mu_X + \Lambda_X F_t + \Pi Z_t + u_t
	\end{eqnarray*}
	where $F_t \perp Z_t$
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]\frametitle{Bootstrap Aggregation -- ``Bagging''}
    
\begin{block}
	{Bagging Algorithm}
	\begin{enumerate}
		\item Make a bootstrap draw  
		\item Carry out selection/shrinkage/estimation using boostrap data
		\item Use estimated parameters from to construct a forecast $\widehat{y}_{T+h}^{(b)}$
		\item Repeat for $b = 1, \hdots, B$
		\item Average to get ``Bagged'' Forecast: $\widehat{y}_{T+h}^{(Bag)} = \frac{1}{B}\sum_{b=1}^B \widehat{y}_{T+h}^{(b)}$
	\end{enumerate}
\end{block}

\begin{block}
	{Details}
	\begin{itemize}
		\item If the data are dependent, need block bootstrap.
		\item In step 3, we forecast using the \emph{parameters} estimated from the bootstrap data but the \emph{predictors} from the \emph{real} dataset.
	\end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Bootstrap Aggregation -- ``Bagging''}

\begin{block}
 	{Why Bagging?}
 		\begin{itemize}
 			\item Aims to reduce the forecast error of ``unstable'' procedures such as variable selection of Lasso, by reducing their variance.
 			\item Completely portable: you can bag \emph{anything} provided you have an appropriate way to carry out the bootstrap.
 			\item May provide a way of attacking the problem of inference post-model selection. See Efron (JASA, \emph{Forthcoming}) ``Estimation and Accuracy after Model Selection''
 		\end{itemize}
 \end{block} 


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Bagging in Economics}
	\begin{block}
		{Inoue \& Killian (2008, JASA)}
		Compares performance of bagged ``pre-test'' estimator (variable selection via a t-test) to other methods of forecasting US Inflation. Bagging is carried out via a block bootstrap.
	\end{block}
	\begin{block}
		{Stock \& Watson (2012)}
		Among other shrinkage procedures, they consider a large-sample approximation to bagging pre-test estimators that doesn't require making bootstrap draws.
	\end{block}
	\begin{block}
		{Other Papers That Use Bagging}
		\begin{itemize}
			\item Hillebrand \& Medeiros (2010): Realized Volatility Forecasts
			\item Hillebrand et al (2012): Forecasting the Equity Premium
			\item \alert{Kim and Swanson (2013)}
		\end{itemize}
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Boosting}
	\begin{block}
		{Ensemble Methods}
		Machine learning term for ``non-Bayesian model averaging''
	\end{block}
	\begin{block}
		{What is Boosting?}
		\begin{itemize}
			\item Combine large number of ``weak learners'' (i.e.\ crappy predictive models) so that the \emph{ensemble} predicts well.
			\item Explicitly designed around predictive loss
			\item Arbitrarily improve in-sample fit of arbitrarily the weak learners!
		\end{itemize}
	\end{block}
	\begin{block}
		{Book-Length Treatment}
		Shapire \& Freund (2012) -- \emph{Boosting: Foundations and Algorithms}
	\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Boosting}
    

 \begin{block}
 	{Bai \& Ng (2009) -- Boosting Diffusion Indices}
 	Use boosting to select which lags of factors to include in a forecasting regression estimated following PCA. 
 \end{block}

 \begin{block}
 	{Buchen \& Wohlrabe (2011) -- Is Boosting a Viable Alternative?}
 	Boosting performs well compared to other methods in the example from the 2006 Stock \& Watson Handbook Chapter.
 \end{block}

\vspace{1em}
\alert{We'll learn more about boosting in the presentations!}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Some Avenues to Explore}
   
\begin{itemize}
	\item Kernel Methods (Exterkate et al, 2013) 
		\begin{itemize}
			\item Nonlinear factors
		\end{itemize}
	\item Model selection for factor / shrinkage estimators
		\begin{itemize}
			\item Generated regressor problem
			\item Choice of Lasso and Ridge Penalties in time-series Forecasting 
			\item Efficient selection of number of factors
		\end{itemize}
	\item When and how should we extract targeted factors?
	\item Can we provide an economic or statistical justification for sparse PCA or ICA?
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}