\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 2: Various Model Selection Criteria}

\author{Francis J.\ DiTraglia}

\maketitle 


\section{The Corrected AIC}
To derive the TIC and AIC we used asymptotic theory to construct an analytical bias correction. Such approximations tend to work as long as $T$ is fairly large relative to $p$ but when this is not the case, they can break down. We'll now consider an alternative that makes stronger assumptions and relies on \emph{exact} small-sample theory rather than asymptotics: the ``Corrected'' AIC, or AIC$
_c$, of Hurvich and Tsai (1989). Suppose that the true DGP is a linear regression model:
$$\textbf{y} = X\beta_0 + \boldsymbol{\epsilon}$$
where $\mathbf{\epsilon} \sim N(\mathbf{0}, \sigma_0^2 \mathbf{I}_T)$. Then $\mathbf{y}|X \sim N(X\beta_0, \sigma_0^2 \mathbf{I}_T)$ so the likelihood is
$$g(\textbf{y}|X;\beta_0, \sigma^2_0) = \left(2\pi\sigma_0^2\right)^{-T/2} \exp\left\{ -\frac{1}{2\sigma^2}(y - X\beta_0)'(y - X\beta_0)\right\}$$
and the log-likelihood is
$$\log\left[g(\textbf{y}|X;\beta_0, \sigma_0^2)\right] = -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_0) - \frac{1}{2\sigma_0^2}\left(\textbf{y} - X\beta_0\right)'\left(\textbf{y} -X\beta_0\right)$$
Now suppose we evaluated the log-likelihood at some \emph{other} parameter values $\beta_1$ and $\sigma^2_1$. The vector $\beta_1$ might, for example, correspond to dropping some regressors from the model by setting their coefficients to zero, or perhaps adding in some additional regressors. We have
$$\log[f(\textbf{y}|X;\beta_1, \sigma_1^2)] = -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_1) - \frac{1}{2\sigma_1^2}\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right)$$
Since we've specified the density from which the data were generated as well as the density of the approximating model, we can \emph{directly calculate} the KL divergence rather than trying to find a reasonable large sample approximation. It turns out that for this example 
$$KL(g;f) = \frac{T}{2}\left[\frac{\sigma_0^2}{\sigma_1^2} - \log\left(\frac{\sigma_0^2}{\sigma_1^2}\right) - 1 \right] + \left(\frac{1}{2 \sigma_1^2}\right)\left(\beta_0 - \beta_1\right)'X'X\left(\beta_0 - \beta_1\right)$$ 
\textbf{You will demonstrate this on the problem set}. We need to estimate this quantity for it to be of any use in model selection. If let $\widehat{\beta}$ and $\widehat{\sigma}^2$ be the maximum likelihood estimators of $\beta_1$ and $\sigma_1^2$ and substitute them into the expression for the KL divergence, we have
\begin{eqnarray*}
\widehat{KL}(g;f) &=& \frac{T}{2}\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} - \log\left(\frac{\sigma_0^2}{\widehat{\sigma}^2} \right) - 1\right] + \left(\frac{1}{2\widehat{\sigma}^2} \right)\left(\beta_0 - \widehat{\beta}\right)X'X\left(\beta_0 - \widehat{\beta}\right)
\end{eqnarray*}
We still have two problems. First, we haven't been entirely clear about what $\beta_1$ and $\sigma_1$ are. At the moment, they seem to be something like ``pseudo-true'' values. Second, and more importantly, we don't know $\beta_0$ and $\sigma_0^2$ so we can't use the preceding expression to compare models.

Hurvich and Tsai (1989) address both of these problems with the assumption that all models under consideration are \emph{at least correctly specified}. That is, while they may include a regressor whose coefficient is in fact zero, they do not exclude any regressors with a non-zero coefficient. This is the same assumption that we used above to reduce TIC to AIC. Under this assumption, $\beta_1$ and $\sigma_1^2$ \emph{are precisely the same} as $\beta_0$ and $\sigma_0^2$. More importantly, we can use all of the standard results for the exact finite sample distribution of regression estimators to help us. The idea is to construct an \emph{unbiased} estimator of the KL divergence. Taking expecations and rearranging slightly, we have
\begin{eqnarray*}
E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{ E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]
\end{eqnarray*}
Now, under our assumptions $T\widehat{\sigma}^2/\sigma_0^2 \sim \chi^2_{T-k}$ where $k$ is the number of estimated coefficients in $\widehat{\beta}$. Further, if $Z \sim \chi^2_\nu$ then $E[1/Z] = 1/(\nu-2)$. It follows that
$$E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] = E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
We can rewrite the final term similarly:
$$E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right] = E\left[\left(\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right)\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right]$$
Under our assumptions the two terms in the product are independent, so we can break apart the expectation. First, we have
$$E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
as above. For the second part,
$$\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \sim \chi^2_k$$
and hence
$$E\left[\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right] = k$$
Putting all the pieces together,
\begin{eqnarray*}
E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{ E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] + \log(\sigma_0^2) - E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]\\
&=& \frac{T}{2} \left( \frac{T}{T-k-2} - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right) + \frac{T}{2}\left(\frac{k}{T - k -2}\right)\\
&=& \frac{T}{2} \left(\frac{T + k}{T - k -2} - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right)
\end{eqnarray*}
Since $\log(\widehat{\sigma}^2)$ is an unbiased estimator of $E[\log(\widehat{\sigma}^2)]$, substituting this give us an unbiased estimator of $E\left[\widehat{KL}(g;f) \right]$ as desired.
The only terms that vary across candidate models are the first and the third. Moreover, the multiplicative factor of $T/2$ does not affect model selection. Hence, the criterion is
$$AIC_c = \log(\widehat{\sigma}^2) + \frac{T + k}{T - k -2}$$
In its broad strokes, this makes perfect sense. The residual error variance $\widehat{\sigma}^2$ measures in-sample fit. But since in-sample fit is a mis-leading guide to out-of-sample fit, we add a complexity penalty. Note that the way this expression is written, \emph{smaller} values indicate a better model. 

So how does this compare to the plain-vanilla AIC for normal linear regression? The maximum likelihood estimators for this problem are
\begin{eqnarray*}
\widehat{\beta} &=& (X'X)^{-1}X'\mathbf{y}\\
\widehat{\sigma}^2 &=& \frac{(\mathbf{y} - X\widehat{\beta})'(\mathbf{y} - X\widehat{\beta})}{T}
\end{eqnarray*}
It follows that the maximized log-likehood is
\begin{eqnarray*}
\log\left[f(\mathbf{y}|X;\widehat{\beta})\right] &=& -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{1}{2\widehat{\sigma}^2}(y - X\widehat{\beta})'(y -X\widehat{\beta})\\
&=& -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{T}{2}
\end{eqnarray*}
by substituting $T\widehat{\sigma}^2$ for the numerator of the second term. Hence, the AIC for this problem is
$$AIC = 2\left(\ell(\widehat{\beta}) - k \right) = -T\log(\widehat{\sigma}^2) - T - 2k $$
But this way of writing things uses the \emph{opposite} sign convention from AIC$_c$. It's important to keep track of this, since different authors use different sign conventions for information criteria. To make the AIC comparable with our scaling of the AIC$_c$, we multiply through by $-1/T$ yielding
$$AIC = \log(\widehat{\sigma}^2) + \frac{T + 2k}{T}$$
where \emph{smaller} values now indicate a better model.


\section{Cross-Validation}
In the last lecture, we learned that choosing a model by minimizing the KL divergence is equivalent to choosing a model by maximizing the expected log likelihood. We also learned that the sample analogue
 $$E_{\widehat{G}}\left[\log f(\textbf{y}|\widehat{\theta}) \right]= \frac{\ell(\widehat{\theta})}{T}= \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta})$$
provides a biased estimator of this quantity. Intuitively, the problem is that it uses the data twice: first to estimate $\widehat{\theta}$ and then to approximate the integral
	$$\int g(y) \log f(y|\widehat{\theta}) \; dy = E_G\left[\log f(Y_{new}|\widehat{\theta}) \right]$$
using the empirical CDF constructed from the sample observations. If the problem is that we've used the data twice, then an obvious idea is to find some way to use \emph{different} data for estimating $\theta$ than we use to approximate the integral. This is the idea behind cross-validation. We split the data into two parts, use one for estimation and the \emph{other} for model evaluation. To avoid ``wasting data'' we repeat this process sucessively for \emph{different} splits, so each observation has a chance to be used for for estimation and evaluation but \emph{never for both at the same time}.

To make this more concrete, we'll consider what is perhaps the most common form of cross-validation: ``leave-one-out'' cross-validation. For simplicity, suppose we have iid observations $Y_1, \hdots, Y_T$ and let $\widehat{\theta}_{(t)}$ denote the ML estimator for $\theta$ using all observations \emph{except} $Y_t$. The leave-one-out cross-validation estimator of the expected log likelihood is
	$$CV(1) = \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})$$
The idea is that, since the data are iid, $\widehat{\theta}_{(t)}$ is \emph{independent} of $Y_t$. Accordingly, the cross-validation estimate of the expected log-likelihood should \emph{not} be subject to the over-optimism problem that plagues the maximized log-likelihood. To use cross-validation for model selection, we simply calculate $CV(1)$ for the various models under consideration, and choose the one with the \emph{highest} value.

As it turns out, leave-one-out cross-validation is intimately connected with the TIC. In fact the two are \emph{asymptotically equivalent} as we'll now show. To begin note that, by a first-order Taylor Expansion of the leave-one-out estimator around the full-sample MLE we have
	\begin{eqnarray*}
		CV(1) &=& \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})\\
			&=&\frac{1}{T} \sum_{t=1}^T \left[\log f(y_t|\widehat{\theta}) + \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) \right] + o_p(1)\\
			&=& \frac{\ell(\widehat{\theta})}{T} + \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) + o_p(1)
	\end{eqnarray*}
so we simply need to show that
	$$\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T}\mbox{trace}\left(\widehat{J}^{-1} \widehat{K} \right) + o_p(1)$$
\textbf{\emph{Everything that follows amounts to proving this statement}}. 

To understand the preceding assertion, we'll need to take a slight detour and talk about \emph{influence functions}, an idea from the robust estimation literature. Let $\mathbb{T}=\mathbb{T}(G)$ be a functional and $G$ be some probability distribution. Then the influence function of $\mathbb{T}$ at a point $y$ is defined as 
	$$\mbox{infl}(G,y) = \lim_{\epsilon \rightarrow 0} \frac{\mathbb{T}\left(\left(1-\epsilon\right)G + \epsilon \delta_y\right) - \mathbb{T}(G)}{\epsilon}$$
where $\delta_y$ is a \emph{point mass} at $y$, that is
		$$\delta_y(a)\left\{\begin{array}{c} 0, \; a<y \\ 1, \; a\geq y\end{array} \right.$$
 All kinds of quantities that we know and love can be viewed as functionals of a distribution, for example the mean and variance.\footnote{If you're interested in learning more about this, the book ``Information Criteria and Statistical Modeling'' by Konishi and Kitagawa (2008) provides a good overview.} Here we're going to be concerned with a particular functional that should look familiar from our last lecture:
	$$\theta_0 = \mathbb{T}(G) = \underset{\theta \in \Theta}{\arg \min} \;E_G\left[\log\left\{\frac{g(Y)}{f(Y|\theta)} \right\} \right]$$
What this says is that we can view $\theta_0$ as the result of applying an \emph{operator} $\mathbb{T}$ to the distribution $G$. In this case $\theta_0$ is simply the pseudo-true value: the probability limit of the maximum likelihood estimator of $\theta$ based on $f(y|\theta)$. Clearly the pseudo-true value depends on the DGP, namely $G$. Different distributions $G$ would yield different pseudo-true values for the \emph{same} likelihood $f$. If we evaulate $\mathbb{T}$ at the \emph{empirical} distribution $\widehat{G}$ we get the maximum likelihood estimator $\widehat{\theta}$ rather than the pseudo-true value $\theta_0$.


The influence function is in fact a \emph{functional derivative}. It allows us to evaulate, for example, how the pseudo-true value $\theta_0$  would change if we \emph{slightly} changed the distribution $G$ that generated the data by ``polluting'' it with a tiny mass point located at $y$. We could also consider how the maximum likelihood estimator, $\widehat{\theta}$, would change if we slightly changed the dataset, represented by empirical distribution function. 
% Under appropriate regularity conditions, it can be shown that
% 	$$T(\widehat{G}_T) = T(G) + \frac{1}{T}\sum_{t=1}^T \mbox{infl}(G,Y_t) + o_p(T^{-1/2})$$
% so that
% 	$$\sqrt{T}\left(\mathbb{T}(\widehat{G}_T) - \mathbb{T}(G) \right) \overset{d}{\rightarrow}N(0,\Omega)$$
Now, since the empirical distribution is given by
$$\widehat{G}(a) = \frac{1}{T}\sum_{t=1}^T \textbf{1}\left\{y_t \leq a\right\} = \frac{1}{T}\sum_{t=1}^T \delta_{y_t}(a)$$
we can re-write it as 
$$\widehat{G} = (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T$$
where $\widehat{G}_{(t)}$ is the empirical distribution with $y_t$ excluded from the dataset. Applying $\mathbb{T}$ to both sides, subtracting $\mathbb{T}(\widehat{G}_{(t)})$ and multiplying the right-hand-side by $T/T$, we have
	$$\mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) = \frac{1}{T}\left[\frac{\mathbb{T}\left((1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T \right) - \mathbb{T}(\widehat{G}_{(t)})}{1/T} \right]$$
If we take $\epsilon = 1/T$, the term in square brackets is \emph{exactly} the expression whose limit is defined as the influence function. Hence, for $T$ large we have the approximation
$$\mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) = \frac{1}{T} \mbox{infl}\left(\widehat{G}_{(t)}, y_t \right) + o_p(1)$$
Now, since $\mathbb{T}(\widehat{G}) = \widehat{\theta}$ and $\mathbb{T}(\widehat{G}_{(t)}) =\widehat{\theta}_{(t)}$, we have the following expression for the leave-one-out estimator
\begin{eqnarray*}
	\widehat{\theta}_{(t)} &=& \widehat{\theta} - \frac{1}{T} \mbox{infl}\left(\widehat{G}_{(t)}, y_t\right) + o_p(1)\\
	&=& \widehat{\theta} - \frac{1}{T} \mbox{infl}\left(\widehat{G}, y_t\right) + o_p(1)
\end{eqnarray*}
The second expression indicates that dropping one observation is asymptotically negligible in its effect, through the empirical CDF, on the influence function. Now, it can be shown that the influence function for maximum likelihood estimation is
	$$\mbox{infl}(G,y) = J^{-1} \left(\frac{\partial \log f(y|\theta_0)}{\partial \theta}\right)$$
where $\theta_0 = \mathbb{T}(G)$ is the pseudo-true value.\footnote{See, for example, Konishi and Kitagawa (2008).} Hence, evaluating this expression at $\widehat{G}$ and $y_t$ and substituting into our expression for $\widehat{\theta}_{(t)}$
	$$\widehat{\theta}_{(t)} = \widehat{\theta} - \frac{1}{T}\widehat{J}^{-1} \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)$$
since $\mathbb{T}(\widehat{G}) = \widehat{\theta}$. This gives us an asymptotic expansion for $\left(\widehat{\theta}_{(t)} - \widehat{\theta}\right)$, namely 
$$\left(\widehat{\theta}_{(t)} - \widehat{\theta}\right) =  - \frac{1}{T}\widehat{J}^{-1} \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)$$
which is exactly what we need. Finally, substituting this back into the expression we initially set out to prove,
\begin{eqnarray*}
	\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) &=& -\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right)' \frac{\widehat{J}^{-1}}{T}\left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)\\
		&=& -\frac{1}{T}\mbox{trace}\left\{\widehat{J}^{-1}\left[\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right]\right\}\\
		&&\quad \quad + \; o_p(1)\\
		&=&-\frac{1}{T}\mbox{trace}\left\{\widehat{J}^{-1} \widehat{K} \right\}
\end{eqnarray*}

Although we've used cross-validation to evaulate the KL divergence here, it's actually a very general idea and can be used with \emph{any} loss function. For example, we could use a zero-one loss function for binary predictions, or a quadratic loss function for continuous predictions. The idea is the same in any case: we creat a ``pseudo-out-of-sample observations'' by witholding one observation at a time, and use these to evaluate the loss. While cross-validation can be computationally intensive, it turns out that there's a shortcut for models that can be expressed as linear smoothers. You'll examine a simple case of this on the problem set. It's also worth pointing out that there are varieties of cross-validation other than leave-one-out, but these have different properties. 



\section{Mallow's $C_p$}
Suppose that we want to predict $y$ from $\mathbf{x}$ using a linear regression model:
	$$\underset{(T\times1)}{\textbf{y}} = \underset{(T\times K)}{X} \underset{(K\times 1)}{\beta} + \boldsymbol{\epsilon}$$
Where $E[\boldsymbol{\epsilon}|X] = 0$ and $Var(\boldsymbol{\epsilon}|X) = \sigma^2 \mathbf{I}$. We know that the conditional mean is the minimum mean-squared error predictor. This means that if $\beta$ were \emph{known}, we could never improve upon simply using all the regressors for prediction. But since $\beta$ must be \emph{estimated} from the data, a bias-variance tradeoff arises. In particular, we might be better off \emph{excluding} a regressor with small coefficient, since it adds very little predictive power but introduces additional estimation uncertainty. Mallow's $C_p$ is a model selection criterion that trys to capture this idea by approximating the \emph{predictive mean squared error} of each model, relative to the infeasible optimum where $\beta$ is known.

We'll now consider using \emph{subsets} of $X$ rather than the full data matrix. Let $X_M$ denote a design matrix that possibly excludes some columns of $X$. The index $M$ refers to a particular \emph{model}. Accordingly, let $\widehat{\beta}_M$ be the least-squares estimator based on the design matrix $X_M$. We'll adopt the convention that $\widehat{\beta}_M$ is padded out with zeros for the elements of $\beta$ that are \emph{not estimated} under model $M$. This way we can write
	$$X\widehat{\beta}_M = X_{(-M)}\textbf{0} + X_M (X_M'X_M)^{-1} X_M'\textbf{y} = P_M \textbf{y}$$
Now, suppose we want to compare the predictive power of the competing estimators $\widehat{\beta}_M$ using mean-squared error. A na\"{i}ve idea would be to use in-sample prediction error to compare models:
	$$RSS(M) = (\textbf{y} - X\widehat{\beta}_M)'(\textbf{y} - X\widehat{\beta}_M)$$
As is well-known, however, the residual sum of squares can \emph{never} decrease even as we add irrelevant predictors to our model. In constrast, out-of-sample predictive ability can easily decrease when we add more predictors: there's a bias-variance tradeoff that arises from estimation uncertainty. Somehow or other we need to develop a criterion to take this into account.

We'll start off by calculating the predictive mean-squared error of $X\widehat{\beta}_M$ relative to the infeasible optimum, namely $X\beta$. Let $P_M = X_M(X_M'X_M)^{-1}X_M$. Then we have
\begin{eqnarray*}
	\left|\left|X\widehat{\beta}_M - X\beta\right|\right|^2 &=& (P_M \mathbf{y} - X\beta)'(P_M \mathbf{y} - X\beta)\\
		&=&\left\{P_M(Y-X\beta) - (\mathbf{I} - P_M)X\beta \right\}' \left\{P_M(Y-X\beta) - (\mathbf{I} - P_M)X\beta  \right\}\\
		&=&\left\{ P_M \boldsymbol{\epsilon} - (\mathbf{I}- P_M)X\beta\right\}'\left\{ P_M \boldsymbol{\epsilon} + (\mathbf{I}- P_M)X\beta \right\}\\
		&=&\boldsymbol{\epsilon}'P_M'P_M \boldsymbol{\epsilon} - \beta'X'(\mathbf{I}-P_M)'P_M\boldsymbol{\epsilon} \\
			&&\quad \quad - \;\boldsymbol{\epsilon}'P_M'(\mathbf{I} - P_M)X\beta + \beta'X' (\mathbf{I} - P_M)(\mathbf{I} - P_M)X\beta\\
		&=& \boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon} + \beta'X'(\mathbf{I} - P_M)X\beta
\end{eqnarray*}
since $P_M$ and $(\mathbf{I} - P_M)$ are both symmetric and idempotent and their product in any order is zero. Thus, evaluating the predictive mean-squared error conditional on $X$, we have
	\begin{eqnarray*}
		\mbox{MSE}(M|X) &=& E\left[(X\widehat{\beta}_M - X\beta)'(X\widehat{\beta}_M - X\beta)|X \right]\\
		 &=& E\left[\boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon}|X\right] + E\left[\beta'X'(\mathbf{I} - P_M)X\beta |X\right]\\
			&=&E\left[\mbox{trace}\left\{\boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon}\right\}|X\right] + \beta'X'(\mathbf{I} - P_M)X\beta \\
		&=&\mbox{trace}\left\{E[\boldsymbol{\epsilon} \boldsymbol{\epsilon}'|X]P_M\right\} + \beta'X'(\mathbf{I} - P_M)X\beta \\
	&=&\mbox{trace}\left\{\sigma^2 P_M\right\} + \beta'X'(\mathbf{I} - P_M)X\beta \\
	&=& \sigma^2 k_M + \beta'X'(\mathbf{I} - P_M)X\beta
	\end{eqnarray*}
where $k_M$ denotes the number of regressors included in $X_M$ and we have used the fact that the trace of a projection matrix equals its dimension. 

So far, so good: we have derived an expression of the predictive mean-squared error of each model $M$. The problem is that it's infeasible: it depends on the unknown quantities $\sigma^2$ and $\beta$. To get around this, Mallow's $C_p$ constructs an \emph{unbiased} estimator of MSE. We proceed as follows. First, let $\widehat{\beta}$ be the estimator based on the full set of regressors, i.e.\ $\widehat{\beta} = (X'X)^{-1}X'\mathbf{y}$ and let $P_{X}$ be the corresponding projection matrix so that we have 
		$$X \widehat{\theta} = X(X'X)^{-1}X'\mathbf{y} = P_{X}\mathbf{y}$$
Using the fact that $P_MP_X = P_XP_M = P_M$, 
	\begin{eqnarray*}
		E\left[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} |X\right] &=& E\left[\mathbf{y}'P_X'(\mathbf{I} - P_M)P_{X}\mathbf{y} |X\right]\\
			&=& E\left[\mathbf{y}'(P_X'P_X - P_X'P_MP_X)\mathbf{y} |X\right]\\
			&=& E\left[\mathbf{y}'(P_X - P_M)\mathbf{y} |X\right]\\
			&\vdots& \\
			&&\boxed{\mbox{\textbf{You'll fill in the details on the problem set}}}\\  
			&\vdots& \\
			&=& \beta'X'(\mathbf{I}- P_M)X\beta +  \sigma^2 (K - k_M) 
	\end{eqnarray*}
In other words, substituting the estimator $\widehat{\beta}$ from the full model in order to estaimate $\beta'X(\mathbf{I}- P_M)X\beta$ \emph{doesn't work}. The estimator $\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta}$ is \emph{biased upwards}. However, we have an explicit expression for the bias, namely $\sigma^2 (K - k_M)$. This means that if we can find an unbiased estimator of $\sigma^2$, we can \emph{correct} the bias in our estimator of $\beta'X(\mathbf{I}- P_M)X\beta$. Fortunately there is an obvious unbiased estimator of $\sigma^2$: we simply use the residuals from the full model:
	$$\widehat{\sigma}^2 = \frac{\mathbf{y}'(\mathbf{I} - P_X)\mathbf{y}}{(T-K)}$$
Thus, 
	$$E[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - \widehat{\sigma}^2(K- k_M) |X ] = \beta'X(\mathbf{I}- P_M)X\beta$$ 
Now we've managed to construct an unbiased estimator of the \emph{second} term of the MSE. What about the first? This one is easy. We already have an unbiased estimator of $\widehat{\sigma}^2$ and $k_M$ is a known constant: it's simply the number of regressors in model $M$. Therefore, collecting terms
	\begin{eqnarray*}
		\mbox{MC}_M &=& \widehat{\sigma}^2 k_M + \left[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - \widehat{\sigma}^2(K- k_M) \right]\\
			&=& \widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} + \widehat{\sigma}^2 (k_M - K)
	\end{eqnarray*}
is an unbiased estimator of $\mbox{MSE}(M|X)$. This criterion contains exactly the same information as Mallow's $C_p$ but expressed in a slightly different way. To get the formula from the textbooks, we need to do some more algebra. First, 
\begin{eqnarray*}
	MC_M - 2\widehat{\sigma}^2k_M &=& \widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - K\widehat{\sigma}^2\\
	&=& \mathbf{y}'(P_X - P_M)\mathbf{y} -  K\widehat{\sigma}^2\\
			&\vdots& \\
			&&\boxed{\mbox{\textbf{You'll fill in the details on the problem set}}}\\  
			&\vdots& \\
			&=& \mathbf{y}'(\mathbf{I} - P_M)\mathbf{y} - T\widehat{\sigma}^2\\
			&=& RSS(M) - T\widehat{\sigma^2}
\end{eqnarray*}
Substituting this into the expression for $\mbox{MC}_M$ we see that 
	$$\mbox{MC}_M = RSS(M) + \widehat{\sigma}^2(2 k_M - T)$$
which is much easier to interpret than the formula we have before. Finally, dividing through by $\widehat{\sigma}^2$ gives Mallow's $C_p$
	$$C_p(M) = \frac{RSS(M)}{\widehat{\sigma}^2} + 2k_M - T$$
This expression tells us how we need to \emph{adjust} the residual sum of squares to account for the fact that in-sample fit is a misleading guide to out-of-sample predictive performance.

% \section{Time Series Examples}
% We won't go through all of the specifics here since they're almost identical to the material from above. Some more details can be found in
% McQuarrie and Tsai (1998). The AR and VAR models are straightforward since, in the conditional formulation, they're just univariate and multivariate regression, respectively.

% \subsection{Autoregressive Models}

% \paragraph{Cross-Validation for AR}
% The way we described it above, CV depended in independence. How can we adapt it for AR models? Roughly speaking, the idea is to use the fact that dependence dies out over time and treat observations that are ``far enough apart'' as \emph{approximately} independent. Specifically, we choose an integer value $h$ and assume that $y_t$ and $y_s$ can be treated as independent as long as $|s - t|>h$. This idea is called ``$h$-block cross-validation'' and was introduced by Burman, Chow \& Nolan (1994). As in the iid version of leave-one-out cross-validation, we still evaluate a loss function by predicting \emph{one} witheld observation at a time using a model estimated without it. The difference is that we also omit the $h$ neighboring observations \emph{on each side} when fitting the model. For example, if we choose to evaluate squared-error loss, the criterion is
% 	$$CV_h(1) = \frac{1}{T-p}\sum_{t = p+1}^T \left(y_t - \hat{y}_{(t)}^h\right)^2$$
% where 
% $$\hat{y}^h_{(t)} = \hat{\phi}^h_{1(t)} y_{t-1} + \hdots + \hat{\phi}^h_{1(t)}y_{t-p}$$
% and $\hat{\phi}^h_{j(t)}$ denotes the $j$th parameter estimate from the conditional least-squares estimator with observations $y_{t-h}, \hdots,  y_{t+h}$ removed. We still have the question of what $h$ to choose. Here there is a trade-off between making the assumption of independence more plausible and leaving enough observations to get precise model estimates. Intriguingly, the simulation evidence presented in McQuarrie and Tsai (1998) suggests that setting $h=0$, which yields plain-vanilla leave-one-out CV, works well even in settings with dependence.

% The idea of $h$-block cross-validation can also be adapted to versions of cross-validation other than leave-one-out. For details, see Racine (1997, 2000).



% \subsection{Vector Autoregression Models}
% Write without an intercept for simplicity (just demean everything)
% 	\begin{eqnarray*}
% 		\underset{(q\times 1)}{\textbf{y}_t} &=& \underset{(q\times q)}{\Phi_1} \textbf{y}_{t-1} + \hdots + \Phi_{p}\textbf{y}_{t-p} + \epsilon_t\\
% 		\boldsymbol{\epsilon}_t &\overset{iid}{\sim}& N_q(\mathbf{0}, \Sigma)
% 	\end{eqnarray*}
% Conditional least squares estimation, sample size, etc.	
% \begin{eqnarray*}
% 	FPE &=& \left| \widehat{\Sigma}_p \right| \left( \frac{T + qp}{T - qp}\right)^q\\ \\
% 	AIC &=& \log \left| \widehat{\Sigma}_p\right| + \frac{2pq^2 + q(q+1)}{T}\\ \\ 
% 	AIC_c &=& \log \left| \widehat{\Sigma}_p\right|  + \frac{(T + qp)q}{T - qp - q -1}\\ \\
% 	BIC &=& \log \left| \widehat{\Sigma}_p\right| +  \frac{\log(T)pq^2}{T}\\ \\ 
% 	HQ &=& \log \left| \widehat{\Sigma}_p\right| +  \frac{2 \log\log(T)pq^2}{T}
% \end{eqnarray*}

% \paragraph{Problems with VAR model selection}
% 	\begin{enumerate}
% 		\item If we fit $p$ lags, we lose $p$ observations under the conditional least squares estimation procedure.
% 		\item Adding a lag introduces $q^2$ additional parameters. 
% 	\end{enumerate}

% \paragraph{Cross-Validation for VARs} In principle we could use the same $h$-block idea here as we did for for the AR example above. However, given the large number of parameters we need to estimate, the sample sizes witholding $2h+1$ observations at a time may be too small for this to work well. 




% \subsection{Corrected AIC for State Space Models}
% Problem with VARs and state space more generally is that we can easily have sample size small relative to number of parameters. In this case AIC-type criteria don't work well. Suggestions for simulation-based selection.

% \paragraph{Cavanaugh \& Shumway (1997)} 

% \paragraph{}
 


\section{Bayesian Information Criterion}
Since Frank2 talked about this in his part of the course, I won't discuss this derivation in class but I wanted to provide the details for completeness. As in our derivation of TIC and AIC, we'll consider a setting with an iid sample of scalar random variables $Y_1, \hdots, Y_T$. The results still hold in the more general case, but this simplifies the notation. 

\subsection{Overview of the BIC}
Despite its name, the BIC is \emph{not} a Bayesian procedure. It is a large-sample Frequentist \emph{approximation} to Bayesian model selection:
	\begin{enumerate}
		\item Begin with a uniform prior on the set of candidate models so that it suffices to maximize the Marginal Likelihood.
		\item The BIC is a large sample approximation to the Marginal Likelihood:
		$$\int \pi(\beta_i)f_i(\mathbf{y}|\beta_i)d\beta_i$$
		where $i$ indexes models $M_i$ in a set $\mathcal{M}$.
		\item As usual when Bayesian procedures are subjected to Frequentist asymptotics, the priors on parameters vanish in the limit.
		\item We proceed by a \emph{Laplace Approximation} to the Marginal Likelihood
	\end{enumerate}

\subsection{Laplace Approximation}
For the moment simplify the notation by suppressing dependence on $M_i$. We want to approximate: 
	$$\int \pi(\beta)f(\mathbf{y}|\beta)d\beta$$
This is actually a common problem in applications of Bayesian inference:
	\begin{itemize} 
		\item Notice that $\pi(\beta)f(\mathbf{y}|\beta)$ is the \emph{kernel} of some probability density, i.e.\ the density without its normalizing constant. 
		\item \emph{How do we know this?} By Bayes' Rule 
	$$\pi(\beta|\mathbf{y}) = \frac{\pi(\beta)f(\mathbf{y}|\beta)}{\int \pi(\beta)f(\mathbf{y}|\beta) d\beta }$$
is a proper probability density and the denominator is \emph{constant} with respect to $\beta$. (The parameter has been ``integrated out.'')
	\item In Bayesian inference, we specify $\pi(\beta)$ and $f(\mathbf{y}|\beta)$, so $\pi(\beta)f(\mathbf{y}|\beta)$ is known. But to calculate the posterior we need to \emph{integrate} to find the normalizing constant.
	\item Only in special cases (e.g.\ conjugate families) can we find the exact normalizing constant. Typically some kind of approximation is needed:  
		\begin{itemize}
			\item Importance Sampling
			\item Markov-Chain Monte Carlo (MCMC)
			\item \emph{Laplace Approximation}
		\end{itemize}
	\end{itemize}
The Laplace Approximation is an \emph{analytical approximation} based on Taylor Expansion arguments. In Bayesian applications, the expansion is carried out around the posterior mode, i.e.\ the mode of $\pi(\beta)f(\mathbf{y}|\beta)$, but we will expand around the Maximum likelihood estimator. 

\begin{pro}[Laplace Approximation]
	\label{pro:laplace}
$$\int \pi(\beta)f(\mathbf{y}|\beta)d\beta \approx \frac{\exp\left\{ \ell(\hat{\beta}) \right\} \pi(\hat{\beta})(2\pi)^{p/2}}{n^{p/2}\left| J(\hat{\beta}) \right|^{1/2}}$$
	Where $\hat{\beta}$ is the \emph{maximum likelihood estimator}, $p$ the dimension of $\beta$ and
		$$J(\hat{\beta}) = -\frac{1}{n} \frac{\partial^2 \log f(\mathbf{y}|\hat{\beta})}{\partial \beta \partial \beta'}$$
\end{pro}

\begin{proof}
A rigorous proof of this result is complicated. The following is a sketch. First write $\ell(\beta)$ for $\log{f(\mathbf{y}|\beta)}$ so that 
$$\pi(\beta)f(\mathbf{y}|\beta) = \pi(\beta) \exp{\left\{ \log{f(\mathbf{y}| \beta)} \right\}}=\pi(\beta) \exp{\left\{ \log{\ell(\beta)} \right\}}$$
By a second-order Taylor Expansion around the MLE $\hat{\beta}$
	\begin{equation}
	\label{taylorell}
		\ell(\beta) = \ell(\hat{\beta}) +\frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell
	\end{equation}
since the derivative of $\ell(\beta)$ is zero at $\hat{\beta}$ by the definition of MLE. A first-order expansion is sufficient for $\pi(\beta)$ because the derivative does not vanish at $\hat{\beta}$
	\begin{equation}
		\label{taylorpi}
		\pi(\beta) = \pi(\hat{\beta}) +  \frac{\partial \pi(\hat{\beta})}{\partial \beta'} \left(\beta - \hat{\beta}  \right)+ R_\pi
	\end{equation}
Substituting Equations \ref{taylorell} and \ref{taylorpi},
	\begin{eqnarray*}
		\int \pi(\beta)f(\mathbf{y}|\beta)d\beta &=& \int \exp\left\{ \ell(\hat{\beta}) +\frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell \right\}\\
		&&\;\;\;\;\;\;\times \left[ \pi(\hat{\beta}) + \left(\beta - \hat{\beta}  \right)' \frac{\partial \pi(\hat{\beta})}{\partial \beta} + R_\pi \right]  d\beta\\\\
		&=& \exp\left\{ \ell(\hat{\beta}) \right\} (I_1 + I_2 + I_3)
	\end{eqnarray*}
where
	\begin{eqnarray*}
		I_1 &=& \pi(\hat{\beta}) \int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta\\
		I_2 &=& \frac{\partial \pi(\hat{\beta})}{\partial \beta'}   \int  \left(\beta - \hat{\beta}  \right) \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta\\
\\
		I_3 &=& \int R_\pi \; \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta
	\end{eqnarray*}
Under certain regularity conditions (not the standard ones!) we can treat $R_\ell$ and $R_\pi$ as approximately equal to zero for large $n$ uniformly in $\beta$, so that
	\begin{eqnarray*}
		I_1 &\approx& \pi(\hat{\beta}) \int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta\\
		I_2 &\approx& \frac{\partial \pi(\hat{\beta})}{\partial \beta'}   \int  \left(\beta - \hat{\beta}  \right) \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta\\
		I_3 &\approx& 0
	\end{eqnarray*}	
Because $\hat{\beta}$ is the MLE, 
	$$\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}$$ 
must be negative definite, so 
	$$-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}$$
is positive definite. It follows that 
	$$ \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} =  \exp{\left\{ -\frac{1}{2} \left( \beta - \hat{\beta}  \right)'\left[ \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right]^{-1}\left( \beta - \hat{\beta}  \right) \right\}}$$
can be viewed as the kernel of a Normal distribution with mean $\hat{\beta}$ and variance matrix 
	$$\left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1}$$
Thus,
	$$\int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta = \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right|^{1/2}$$
and
	$$\int \left(\beta - \hat{\beta}  \right)  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta = 0$$
Therefore,
	\begin{eqnarray*}
		\int \pi(\beta)f(\mathbf{y}|\beta)d\beta &\approx& \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right|^{1/2}\\
		&=&  \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}\left|n \left(-\frac{1}{n}\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right) \right|^{-1/2}\\\\
		&=&\frac{ \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}}{n^{p/2}\left| J(\hat{\beta}) \right|^{1/2}}
	\end{eqnarray*}
\end{proof}


\subsection{Finally the BIC}
Now we re-introduce the dependence on the model $M_i$. Taking logs of the Laplace Approximation and multiplying by two (again, this is traditional but has no effect on model comparisons)
	\begin{eqnarray*}
		2 \log f(y|M_i) &=& 2 \log \left\{ \int f_i(y|\beta_i)\pi(\beta_i)d\beta_i \right\}\\
		&\approx& 2\ell(\hat{\beta}_i) -p\log(n) + p \log(2\pi)- \pi(\hat{\beta}_i)-\log \left| J(\hat{\beta_i}) \right|
	\end{eqnarray*}
The first two terms are $O_p(n)$ and $O_p(\log{n})$, while the last three are $O_p(1)$, hence negligible as $n\rightarrow \infty$. This gives us Schwarz's BIC
	$$BIC(M_i) = 2\log{f_i(\mathbf{y}|\hat{\beta}_i)} - p\log{n}$$
We choose the model $M_i$ for which $BIC(M_i)$ is largest. Notice that the prior on the parameter, $\pi(\beta)$, drops out in the limit, and recall that we began by putting a uniform prior on the \emph{models} under consideration. 


\end{document}