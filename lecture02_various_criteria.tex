\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\begin{document}

\title{Various Model Selection Criteria}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Bayesian Information Criterion (BIC)}
As in the derivation of AIC, simplify by looking only at a scalar random variable and ignoring dependence etc. Results are still generally true but this simplifies the notation.

\subsection{Overview of the BIC}
Despite its name, the BIC is \emph{not} a Bayesian procedure. It is a large-sample Frequentist \emph{approximation} to Bayesian model selection:
	\begin{enumerate}
		\item Begin with a uniform prior on the set of candidate models so that it suffices to maximize the Marginal Likelihood.
		\item The BIC is a large sample approximation to the Marginal Likelihood:
		$$\int \pi(\theta_i)f_i(\mathbf{y}|\theta_i)d\theta_i$$
		\item As usual when Bayesian procedures are subjected to Frequentist asymptotics, the priors on parameters vanish in the limit.
		\item We proceed by a \emph{Laplace Approximation} to the Marginal Likelihood
	\end{enumerate}

\subsection{Laplace Approximation}
For the moment simplify the notation by suppressing dependence on $M_i$. We want to approximate: 
	$$\int \pi(\theta)f(\mathbf{y}|\theta)d\theta$$
This is actually a common problem in applications of Bayesian inference:
	\begin{itemize} 
		\item Notice that $\pi(\theta)f(\mathbf{y}|\theta)$ is the \emph{kernel} of some probability density, i.e.\ the density without its normalizing constant. 
		\item \emph{How do we know this?} By Bayes' Rule 
	$$\pi(\theta|\mathbf{y}) = \frac{\pi(\theta)f(\mathbf{y}|\theta)}{\int \pi(\theta)f(\mathbf{y}|\theta) d\theta }$$
is a proper probability density and the denominator is \emph{constant} with respect to $\theta$. (The parameter has been ``integrated out.'')
	\item In Bayesian inference, we specify $\pi(\theta)$ and $f(\mathbf{y}|\theta)$, so $\pi(\theta)f(\mathbf{y}|\theta)$ is known. But to calculate the posterior we need to \emph{integrate} to find the normalizing constant.
	\item Only in special cases (e.g.\ conjugate families) can we find the exact normalizing constant. Typically some kind of approximation is needed:  
		\begin{itemize}
			\item Importance Sampling
			\item Markov-Chain Monte Carlo (MCMC)
			\item \emph{Laplace Approximation}
		\end{itemize}
	\end{itemize}
The Laplace Approximation is an \emph{analytical approximation} based on Taylor Expansion arguments. In Bayesian applications, the expansion is carried out around the posterior mode, i.e.\ the mode of $\pi(\theta)f(\mathbf{y}|\theta)$, but we will expand around the Maximum likelihood estimator. 

\begin{pro}[Laplace Approximation]
	\label{pro:laplace}
$$\int \pi(\theta)f(\mathbf{y}|\theta)d\theta \approx \frac{\exp\left\{ \ell(\hat{\theta}) \right\} \pi(\hat{\theta})(2\pi)^{p/2}}{n^{p/2}\left| J(\hat{\theta}) \right|^{1/2}}$$
	Where $\hat{\theta}$ is the \emph{maximum likelihood estimator}, $p$ the dimension of $\theta$ and
		$$J(\hat{\theta}) = -\frac{1}{n} \frac{\partial^2 \log f(\mathbf{y}|\hat{\theta})}{\partial \theta \partial \theta'}$$
\end{pro}

\begin{proof}
A rigorous proof of this result is complicated. The following is a sketch. First write $\ell(\theta)$ for $\log{f(\mathbf{y}|\theta)}$ so that 
$$\pi(\theta)f(\mathbf{y}|\theta) = \pi(\theta) \exp{\left\{ \log{f(\mathbf{y}| \theta)} \right\}}=\pi(\theta) \exp{\left\{ \log{\ell(\theta)} \right\}}$$
By a second-order Taylor Expansion around the MLE $\hat{\theta}$
	\begin{equation}
	\label{taylorell}
		\ell(\theta) = \ell(\hat{\theta}) +\frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell
	\end{equation}
since the derivative of $\ell(\theta)$ is zero at $\hat{\theta}$ by the definition of MLE. A first-order expansion is sufficient for $\pi(\theta)$ because the derivative does not vanish at $\hat{\theta}$
	\begin{equation}
		\label{taylorpi}
		\pi(\theta) = \pi(\hat{\theta}) +  \frac{\partial \pi(\hat{\theta})}{\partial \theta'} \left(\theta - \hat{\theta}  \right)+ R_\pi
	\end{equation}
Substituting Equations \ref{taylorell} and \ref{taylorpi},
	\begin{eqnarray*}
		\int \pi(\theta)f(\mathbf{y}|\theta)d\theta &=& \int \exp\left\{ \ell(\hat{\theta}) +\frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell \right\}\\
		&&\;\;\;\;\;\;\times \left[ \pi(\hat{\theta}) + \left(\theta - \hat{\theta}  \right)' \frac{\partial \pi(\hat{\theta})}{\partial \theta} + R_\pi \right]  d\theta\\\\
		&=& \exp\left\{ \ell(\hat{\theta}) \right\} (I_1 + I_2 + I_3)
	\end{eqnarray*}
where
	\begin{eqnarray*}
		I_1 &=& \pi(\hat{\theta}) \int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta\\
		I_2 &=& \frac{\partial \pi(\hat{\theta})}{\partial \theta'}   \int  \left(\theta - \hat{\theta}  \right) \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta\\
\\
		I_3 &=& \int R_\pi \; \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta
	\end{eqnarray*}
Under certain regularity conditions (not the standard ones!) we can treat $R_\ell$ and $R_\pi$ as approximately equal to zero for large $n$ uniformly in $\theta$, so that
	\begin{eqnarray*}
		I_1 &\approx& \pi(\hat{\theta}) \int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta\\
		I_2 &\approx& \frac{\partial \pi(\hat{\theta})}{\partial \theta'}   \int  \left(\theta - \hat{\theta}  \right) \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta\\
		I_3 &\approx& 0
	\end{eqnarray*}	
Because $\hat{\theta}$ is the MLE, 
	$$\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}$$ 
must be negative definite, so 
	$$-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}$$
is positive definite. It follows that 
	$$ \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} =  \exp{\left\{ -\frac{1}{2} \left( \theta - \hat{\theta}  \right)'\left[ \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right]^{-1}\left( \theta - \hat{\theta}  \right) \right\}}$$
can be viewed as the kernel of a Normal distribution with mean $\hat{\theta}$ and variance matrix 
	$$\left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1}$$
Thus,
	$$\int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta = \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right|^{1/2}$$
and
	$$\int \left(\theta - \hat{\theta}  \right)  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta = 0$$
Therefore,
	\begin{eqnarray*}
		\int \pi(\theta)f(\mathbf{y}|\theta)d\theta &\approx& \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right|^{1/2}\\
		&=&  \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}\left|n \left(-\frac{1}{n}\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right) \right|^{-1/2}\\\\
		&=&\frac{ \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}}{n^{p/2}\left| J(\hat{\theta}) \right|^{1/2}}
	\end{eqnarray*}
\end{proof}


\subsection{Finally the BIC}
Now we re-introduce the dependence on the model $M_i$. Taking logs of the Laplace Approximation and multiplying by two (again, this is traditional but has no effect on model comparisons)
	\begin{eqnarray*}
		2 \log f(y|M_i) &=& 2 \log \left\{ \int f_i(y|\theta_i)\pi(\theta_i)d\theta_i \right\}\\
		&\approx& 2\ell(\hat{\theta}_i) -p\log(n) + p \log(2\pi)- \pi(\hat{\theta}_i)-\log \left| J(\hat{\theta_i}) \right|
	\end{eqnarray*}
The first two terms are $O_p(n)$ and $O_p(\log{n})$, while the last three are $O_p(1)$, hence negligible as $n\rightarrow \infty$. This gives us Schwarz's BIC
	$$BIC(M_i) = 2\log{f_i(\mathbf{y}|\hat{\theta}_i)} - p\log{n}$$
We choose the model $M_i$ for which $BIC(M_i)$ is largest. Notice that the prior on the parameter, $\pi(\theta)$, drops out in the limit, and recall that we began by putting a uniform prior on the \emph{models} under consideration. 

\section{Hannan-Quinn}

\section{Final Prediction Error}

\section{Mallow's $C_p$}

\section{Cross-Validation}
Talk about time series version and Racine paper.

\section{Bootstrap Model Selection}
There's a state-space paper here as well. Need to talk about block bootstrap. Mention that we're going to learn more about this when we look at Bagging later in the semester.


\section{Some Time Series Examples}
Reference McQuarrie and Tsai among others. Also the paper by Ng and Renault. 

\end{document}