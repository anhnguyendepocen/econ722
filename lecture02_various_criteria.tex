\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 2: Various Model Selection Criteria}

\author{Francis J.\ DiTraglia}

\maketitle 




\section{The Corrected AIC}
To derive the TIC and AIC we used asymptotic theory to derive an analytical bias correction. These approximations tend to work well as long $n$ is fairly large relative to $p$ but when this is not the case, they can break down. We'll now consider an alternative that makes stronger assumptions and relies on \emph{exact} small-sample theory rather than asymptotics: the ``Corrected'' AIC, or AIC$
_c$, of Hurvich and Tsai (1989). Suppose that the true DGP is a linear regression model:
$$\textbf{y} = X\beta_0 + \boldsymbol{\epsilon}$$
where $\mathbf{\epsilon} \sim N(\mathbf{0}, \sigma_0^2 \mathbf{I}_T)$. Then $\mathbf{y}|X \sim N(X\beta_0, \sigma_0^2 \mathbf{I}_T)$ so the likelihood is
$$g(\textbf{y}|X;\beta_0, \sigma^2_0) = \left(2\pi\sigma_0^2\right)^{-T/2} \exp\left\{ -\frac{1}{2\sigma^2}(y - X\beta_0)'(y - X\beta_0)\right\}$$
and the log-likelihood is
$$\log\left[g(\textbf{y}|X;\beta_0, \sigma_0^2)\right] = -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_0) - \frac{1}{2\sigma_0^2}\left(\textbf{y} - X\beta_0\right)'\left(\textbf{y} -X\beta_0\right)$$
To keep the notation from getting out of control, we'll use the shorthand $\log[g(\mathbf{y})]$ for this quantity. Now suppose we evaluated the log-likelihood at some \emph{other} parameter values $\beta_1$ and $\sigma^2_1$. The vector $\beta_1$ might, for example, correspond to dropping some regressors from the model by setting their coefficients to zero, or perhaps adding in some additional regressors. We have
$$\log[f(\textbf{y}|X;\beta_1, \sigma_1^2)] = -\frac{T}{2}\log(2\pi) -\frac{T}{2} \log(\sigma^2_1) - \frac{1}{2\sigma_1^2}\left(\textbf{y} - X\beta_1\right)'\left(\textbf{y} -X\beta_1\right)$$
Since we've specified the density from which the data were generated as well as the density of the approximating model, we can directly compute the KL divergence. It turns out that for this example:
$$KL(g;f) = \frac{T}{2}\left[\frac{\sigma_0^2}{\sigma_1^2} - \log\left(\frac{\sigma_0^2}{\sigma_1^2}\right) - 1 \right] + \left(\frac{1}{2 \sigma_1^2}\right)\left(\beta_0 - \beta_1\right)'X'X\left(\beta_0 - \beta_1\right)$$
\todo[inline]{You will show this on the problem set!}

We need to estimate this quantity for it to be of any use in model selection. If let $\widehat{\beta}$ and $\widehat{\sigma}^2$ be the maximum likelihood estimators of $\beta_1$ and $\sigma_1^2$ and substitute them into the expression for the KL divergence, we have
\begin{eqnarray*}
\widehat{KL}(g;f) &=& \frac{T}{2}\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} - \log\left(\frac{\sigma_0^2}{\widehat{\sigma}^2} \right) - 1\right] + \left(\frac{1}{2\widehat{\sigma}^2} \right)\left(\beta_0 - \widehat{\beta}\right)X'X\left(\beta_0 - \widehat{\beta}\right)
\end{eqnarray*}
We still have two problems. First, we haven't been entirely clear about what $\beta_1$ and $\sigma_1$ are. At the moment, they seem to be something like ``pseudo-true'' values. Second, and more importantly, we don't know $\beta_0$ and $\sigma_0^2$ so we can't use the preceding expression to compare models.

Hurvich and Tsai (1989) address both of these problems with the assumption that all models under consideration are \emph{at least correctly specified}. That is, while they may include a regressor whose coefficient is in fact zero, they do not exclude any regressors with a non-zero coefficient. This is the same assumption that we used above to reduce TIC to AIC. Under this assumption, $\beta_1$ and $\sigma_1^2$ \emph{are precisely the same} as $\beta_0$ and $\sigma_0^2$. More importantly, we can use all of the standard results for the exact finite sample distribution of regression estimators to help us. The idea is to construct an \emph{unbiased} estimator of the KL divergence. Taking expecations and rearranging slightly, we have
\begin{eqnarray*}
E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{ E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]
\end{eqnarray*}
Now, under our assumptions $T\widehat{\sigma}^2/\sigma_0^2 \sim \chi^2_{T-k}$ where $k$ is the number of estimated coefficients in $\widehat{\beta}$. Further, if $Z \sim \chi^2_\nu$ then $E[1/Z] = 1/(\nu-2)$. It follows that
$$E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] = E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
We can rewrite the final term similarly:
$$E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right] = E\left[\left(\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right)\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right]$$
Under our assumptions the two terms in the product are independent, so we can break apart the expectation. First, we have
$$E\left[\frac{T}{T\widehat{\sigma}^2/\sigma_0^2} \right] = \frac{T}{T - k - 2}$$
as above. For the second part,
$$\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \sim \chi^2_k$$
and hence
$$E\left[\frac{\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right)}{\sigma_0^2} \right] = k$$
Putting all the pieces together,
\begin{eqnarray*}
E\left[\widehat{KL}(g;f) \right] &=&\frac{T}{2}\left\{ E\left[\frac{\sigma_0^2}{\widehat{\sigma}^2} \right] + \log(\sigma_0^2) - E\left[\log(\widehat{\sigma}^2)\right] -1 \right\}\\
&& \quad + \; \frac{1}{2}E\left[\left(\frac{1}{\widehat{\sigma}^2} \right)\left(\widehat{\beta}-\beta_0 \right)X'X\left(\widehat{\beta}-\beta_0\right) \right]\\
&=& \frac{T}{2} \left( \frac{T}{T-k-2} - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right) + \frac{T}{2}\left(\frac{k}{T - k -2}\right)\\
&=& \frac{T}{2} \left(\frac{T + k}{T - k -2} - \log(\sigma_0^2) + E\left[\log(\widehat{\sigma}^2)\right] -1\right)
\end{eqnarray*}
Since $\log(\widehat{\sigma}^2)$ is an unbiased estimator of $E[\log(\widehat{\sigma}^2)]$, substituting this give us an unbiased estimator of $E\left[\widehat{KL}(g;f) \right]$ as desired.
The only terms that vary across candidate models are the first and the third. Moreover, the multiplicative factor of $T/2$ does not affect model selection. Hence, the criterion is
$$AIC_c = \log(\widehat{\sigma}^2) + \frac{T + k}{T - k -2}$$
Note that the way this expression is written, \emph{smaller} values indicate a better model. So how does this compare to the plain-vanilla AIC for normal linear regression? The maximum likelihood estimators for this problem are
\begin{eqnarray*}
\widehat{\beta} &=& (X'X)^{-1}X'\mathbf{y}\\
\widehat{\sigma}^2 &=& \frac{(\mathbf{y} - X\widehat{\beta})'(\mathbf{y} - X\widehat{\beta})}{T}
\end{eqnarray*}
It follows that the maximized log-likehood is
\begin{eqnarray*}
\log\left[f(\mathbf{y}|X;\widehat{\theta})\right] &=& -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{1}{2\widehat{\sigma}^2}(y - X\widehat{\beta})'(y -X\widehat{\beta})\\
&=& -\frac{T}{2} \log(\widehat{\sigma}^2) - \frac{T}{2}
\end{eqnarray*}
by substituting $T\widehat{\sigma}^2$ for the numerator of the second term. Hence, the AIC for this problem is
$$AIC = 2\left(\ell(\widehat{\theta}) - k \right) = -T\log(\widehat{\sigma}^2) - T - 2k $$
But this way of writing things uses the \emph{opposite} sign convention from AIC$_c$. It's important to keep track of this, since different authors use different sign conventions for information criteria. To make the AIC comparable with our scaling of the AIC$_c$, we multiply through by $-1/T$ yielding
$$AIC^* = \log(\widehat{\sigma}^2) + \frac{T + 2k}{T}$$
where \emph{smaller} values now indicate a better model.

\todo[inline]{Add some discussion of the practical differences.}

\section{Simulation-Based Model selection}

\subsection{Bootstrap Model Selection}

\paragraph{State Space Version}
There's a state-space paper here as well. Need to talk about block bootstrap. Mention that we're going to learn more about this when we look at Bagging later in the semester.

\subsection{Cross-Validation}
Talk about time series version and Racine paper.

\section{Some other Model Selection Criteria}
\subsection{Hannan-Quinn}
\subsection{Final Prediction Error}
\subsection{Mallow's $C_p$}




\section{Time Series Examples}
Reference McQuarrie and Tsai among others. Also the paper by Ng and Renault. 

\newpage

\section*{Addendum: Bayesian Information Criterion}
Since Frank2 talked about this in his part of the course, I won't discuss this derivation in class but I wanted to provide the details for completeness. As in our derivation of TIC and AIC, we'll consider a setting with an iid sample of scalar random variables $Y_1, \hdots, Y_T$. The results still hold in the more general case, but this simplifies the notation. 

\subsection{Overview of the BIC}
Despite its name, the BIC is \emph{not} a Bayesian procedure. It is a large-sample Frequentist \emph{approximation} to Bayesian model selection:
	\begin{enumerate}
		\item Begin with a uniform prior on the set of candidate models so that it suffices to maximize the Marginal Likelihood.
		\item The BIC is a large sample approximation to the Marginal Likelihood:
		$$\int \pi(\theta_i)f_i(\mathbf{y}|\theta_i)d\theta_i$$
		where $i$ indexes models $M_i$ in a set $\mathcal{M}$.
		\item As usual when Bayesian procedures are subjected to Frequentist asymptotics, the priors on parameters vanish in the limit.
		\item We proceed by a \emph{Laplace Approximation} to the Marginal Likelihood
	\end{enumerate}

\subsection{Laplace Approximation}
For the moment simplify the notation by suppressing dependence on $M_i$. We want to approximate: 
	$$\int \pi(\theta)f(\mathbf{y}|\theta)d\theta$$
This is actually a common problem in applications of Bayesian inference:
	\begin{itemize} 
		\item Notice that $\pi(\theta)f(\mathbf{y}|\theta)$ is the \emph{kernel} of some probability density, i.e.\ the density without its normalizing constant. 
		\item \emph{How do we know this?} By Bayes' Rule 
	$$\pi(\theta|\mathbf{y}) = \frac{\pi(\theta)f(\mathbf{y}|\theta)}{\int \pi(\theta)f(\mathbf{y}|\theta) d\theta }$$
is a proper probability density and the denominator is \emph{constant} with respect to $\theta$. (The parameter has been ``integrated out.'')
	\item In Bayesian inference, we specify $\pi(\theta)$ and $f(\mathbf{y}|\theta)$, so $\pi(\theta)f(\mathbf{y}|\theta)$ is known. But to calculate the posterior we need to \emph{integrate} to find the normalizing constant.
	\item Only in special cases (e.g.\ conjugate families) can we find the exact normalizing constant. Typically some kind of approximation is needed:  
		\begin{itemize}
			\item Importance Sampling
			\item Markov-Chain Monte Carlo (MCMC)
			\item \emph{Laplace Approximation}
		\end{itemize}
	\end{itemize}
The Laplace Approximation is an \emph{analytical approximation} based on Taylor Expansion arguments. In Bayesian applications, the expansion is carried out around the posterior mode, i.e.\ the mode of $\pi(\theta)f(\mathbf{y}|\theta)$, but we will expand around the Maximum likelihood estimator. 

\begin{pro}[Laplace Approximation]
	\label{pro:laplace}
$$\int \pi(\theta)f(\mathbf{y}|\theta)d\theta \approx \frac{\exp\left\{ \ell(\hat{\theta}) \right\} \pi(\hat{\theta})(2\pi)^{p/2}}{n^{p/2}\left| J(\hat{\theta}) \right|^{1/2}}$$
	Where $\hat{\theta}$ is the \emph{maximum likelihood estimator}, $p$ the dimension of $\theta$ and
		$$J(\hat{\theta}) = -\frac{1}{n} \frac{\partial^2 \log f(\mathbf{y}|\hat{\theta})}{\partial \theta \partial \theta'}$$
\end{pro}

\begin{proof}
A rigorous proof of this result is complicated. The following is a sketch. First write $\ell(\theta)$ for $\log{f(\mathbf{y}|\theta)}$ so that 
$$\pi(\theta)f(\mathbf{y}|\theta) = \pi(\theta) \exp{\left\{ \log{f(\mathbf{y}| \theta)} \right\}}=\pi(\theta) \exp{\left\{ \log{\ell(\theta)} \right\}}$$
By a second-order Taylor Expansion around the MLE $\hat{\theta}$
	\begin{equation}
	\label{taylorell}
		\ell(\theta) = \ell(\hat{\theta}) +\frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell
	\end{equation}
since the derivative of $\ell(\theta)$ is zero at $\hat{\theta}$ by the definition of MLE. A first-order expansion is sufficient for $\pi(\theta)$ because the derivative does not vanish at $\hat{\theta}$
	\begin{equation}
		\label{taylorpi}
		\pi(\theta) = \pi(\hat{\theta}) +  \frac{\partial \pi(\hat{\theta})}{\partial \theta'} \left(\theta - \hat{\theta}  \right)+ R_\pi
	\end{equation}
Substituting Equations \ref{taylorell} and \ref{taylorpi},
	\begin{eqnarray*}
		\int \pi(\theta)f(\mathbf{y}|\theta)d\theta &=& \int \exp\left\{ \ell(\hat{\theta}) +\frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell \right\}\\
		&&\;\;\;\;\;\;\times \left[ \pi(\hat{\theta}) + \left(\theta - \hat{\theta}  \right)' \frac{\partial \pi(\hat{\theta})}{\partial \theta} + R_\pi \right]  d\theta\\\\
		&=& \exp\left\{ \ell(\hat{\theta}) \right\} (I_1 + I_2 + I_3)
	\end{eqnarray*}
where
	\begin{eqnarray*}
		I_1 &=& \pi(\hat{\theta}) \int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta\\
		I_2 &=& \frac{\partial \pi(\hat{\theta})}{\partial \theta'}   \int  \left(\theta - \hat{\theta}  \right) \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta\\
\\
		I_3 &=& \int R_\pi \; \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) + R_\ell\right\}} d\theta
	\end{eqnarray*}
Under certain regularity conditions (not the standard ones!) we can treat $R_\ell$ and $R_\pi$ as approximately equal to zero for large $n$ uniformly in $\theta$, so that
	\begin{eqnarray*}
		I_1 &\approx& \pi(\hat{\theta}) \int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta\\
		I_2 &\approx& \frac{\partial \pi(\hat{\theta})}{\partial \theta'}   \int  \left(\theta - \hat{\theta}  \right) \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta\\
		I_3 &\approx& 0
	\end{eqnarray*}	
Because $\hat{\theta}$ is the MLE, 
	$$\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}$$ 
must be negative definite, so 
	$$-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}$$
is positive definite. It follows that 
	$$ \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} =  \exp{\left\{ -\frac{1}{2} \left( \theta - \hat{\theta}  \right)'\left[ \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right]^{-1}\left( \theta - \hat{\theta}  \right) \right\}}$$
can be viewed as the kernel of a Normal distribution with mean $\hat{\theta}$ and variance matrix 
	$$\left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1}$$
Thus,
	$$\int  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta = \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right|^{1/2}$$
and
	$$\int \left(\theta - \hat{\theta}  \right)  \exp{\left\{ \frac{1}{2} \left( \theta - \hat{\theta}  \right)' \frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'} \left( \theta - \hat{\theta}  \right) \right\}} d\theta = 0$$
Therefore,
	\begin{eqnarray*}
		\int \pi(\theta)f(\mathbf{y}|\theta)d\theta &\approx& \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right)^{-1} \right|^{1/2}\\
		&=&  \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}\left|n \left(-\frac{1}{n}\frac{\partial^2 \ell(\hat{\theta})}{\partial \theta \partial \theta'}\right) \right|^{-1/2}\\\\
		&=&\frac{ \exp\left\{ \ell(\hat{\theta}) \right\}\pi(\hat{\theta}) \left(2\pi\right)^{p/2}}{n^{p/2}\left| J(\hat{\theta}) \right|^{1/2}}
	\end{eqnarray*}
\end{proof}


\subsection{Finally the BIC}
Now we re-introduce the dependence on the model $M_i$. Taking logs of the Laplace Approximation and multiplying by two (again, this is traditional but has no effect on model comparisons)
	\begin{eqnarray*}
		2 \log f(y|M_i) &=& 2 \log \left\{ \int f_i(y|\theta_i)\pi(\theta_i)d\theta_i \right\}\\
		&\approx& 2\ell(\hat{\theta}_i) -p\log(n) + p \log(2\pi)- \pi(\hat{\theta}_i)-\log \left| J(\hat{\theta_i}) \right|
	\end{eqnarray*}
The first two terms are $O_p(n)$ and $O_p(\log{n})$, while the last three are $O_p(1)$, hence negligible as $n\rightarrow \infty$. This gives us Schwarz's BIC
	$$BIC(M_i) = 2\log{f_i(\mathbf{y}|\hat{\theta}_i)} - p\log{n}$$
We choose the model $M_i$ for which $BIC(M_i)$ is largest. Notice that the prior on the parameter, $\pi(\theta)$, drops out in the limit, and recall that we began by putting a uniform prior on the \emph{models} under consideration. 


\end{document}