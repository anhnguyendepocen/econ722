%!TEX root = ../main.tex
\chapter{Frequentist Model Averaging}
\section{Hjort \& Claeskens (2003)}


\paragraph{Compromise Estimators:} From Lemma 3.2, we know that 
	$$
	\hat{\delta}_S \equiv \sqrt{n} \left(\hat{\gamma}_S - \gamma_{0,S}\right)\overset{d}{\rightarrow} D_S = K_S\pi_S K^{-1}(\delta + W)
$$
In the case of the full model, that is $S = \left\{1,2, \overset{d}{\rightarrow}ots, q \right\}$ and $\pi_S = I_q$ so that $K_S = K$, this gives
	$$
	D_n \equiv \hat{\delta}_{full} \equiv \sqrt{n} \left(\hat{\gamma}_{full} - \gamma_0\right)\overset{d}{\rightarrow} D = (\delta + W)
$$
Thus, \emph{any} submodel estimator $\hat{\delta}_S$ of $\delta$ converges in distribution to a linear combination of $D$, while the full model estimator of $D_n$ of $\delta$ simply converges in distribution to $D$. In other words, the behavior of $\hat{\delta}_S$ is ``essentially determined'' by that of $D_n$. 
More precisely, the difference between $\hat{\delta}_S$ and $K_S \pi_S K^{-1}D_n$ is at most $o_p(1)$. Now consider a \textbf{Compromise Estimator} of the form:
	$$
	\hat{\mu} = \sum_S c\left(S|D_n\right)\hat{\mu}_S
$$
that is, we weight and sum submodel estimators where the weights are a function of $D_n = \hat{\delta}_{full} =\sqrt{n}(\hat{\gamma}_{full}-\gamma_0)$. \emph{To ensure consistency, the weights must sum to one}.

\paragraph{Notation:} Define $G$, a $q\times q$ matrix of functions, by
	$$
	G(\overset{d}{\rightarrow}ot) = K^{-1/2} \left\{\sum_S c(S|\overset{d}{\rightarrow}ot) H_S  \right\} K^{1/2}
$$
and $\hat{\delta}(D)$, an estimator of $\delta$ based on $D$, by
	$$
	\hat{\delta}(D) = G(D)'D 
$$
Since $H_S$ is symmetric and the weights $c(\overset{d}{\rightarrow}ot|\overset{d}{\rightarrow}ot)$ are scalars,
	$$
	\hat{\delta}(D) = \left[K^{-1/2} \left\{\sum_S c(S|D) H_S  \right\} K^{1/2}\right]'D = K^{1/2} \left\{\sum_S c(S|D) H_S  \right\} K^{-1/2} D
$$

\paragraph{Theorem 4.1} As long as the weight functions $c(\overset{d}{\rightarrow}ot|\overset{d}{\rightarrow}ot)$ sum to one and have at most a countable number of discontinuities, then
	$$
	\sqrt{n} \left(\hat{\mu} - \mu_{true}  \right) \overset{d}{\rightarrow} \sum_S c(S|D) \Lambda_S \equiv \Lambda
$$
and
	$$
\Lambda = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(D) \right] 
$$
This is, in general, a \textbf{non-normal distribution} with
	\begin{eqnarray*}
		\hbox{mean} &=& \omega' \left\{ \delta - E\left[  \hat{\delta}(D) \right] \right\}\\
		\hbox{variance} &=& \tau_0^2 + \omega'Var\left[\hat{\delta}(D)\right]\omega
	\end{eqnarray*}
where
	\begin{eqnarray*}
	\tau_0^2 &\equiv& \nabla+\theta \mu(\theta_0, \gamma_0)' J_{00}^{-1}\\
	\omega &\equiv& J_{10}J_{00}^{-1}\nabla_\theta \mu(\theta_0, \gamma_0) - \nabla_\gamma \mu(\theta_0, \gamma_0)
\end{eqnarray*}
and the MSE of $\Lambda$ is
	$$
	E[\Lambda^2] = \tau_0^2 + R(\delta)
$$
where
	$$R(\delta) = \omega' \left[ \left\{ \hat{\delta}(D) - \delta \right\}  \left\{ \hat{\delta}(D) - \delta \right\}  ' \right]\omega$$

\begin{proof}
First, using the fact that $\sum_S c(S|D_n) = 1$, we have
	\begin{eqnarray*}
		\sqrt{n}\left( \hat{\mu} -\mu_{true} \right) &=& \sqrt{n}\left[\sum_{S} c(S|D_n)\hat{\mu}_S - \mu_{true} \right]\\
			&=& \sqrt{n}\left[\sum_{S} c(S|D_n)\hat{\mu}_S - \left\{\sum_{S} c(S|D_n)\right\}\mu_{true} \right]\\
			&=&\sum_S \left[ c(S|D_n) \sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)  \right]
\end{eqnarray*}
So we see that $\sqrt{n}\left( \hat{\mu} -\mu_{true} \right)$ is an almost-surely continuous function of the submodel estimators $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ and $D_n = \sqrt{n}\left( \hat{\gamma}_{full} - \gamma_0 \right)$. Thus, to find the limiting distribution of the compromise estimator, we can apply the continuous mapping theorem, provided we have \emph{joint} convergence in distribution of the submodel estimators and $D_n$.

Fortunately, we have already established precisely this joint convergence! In Lemma 3.3, we showed that the limit distribution of each submodel estimator $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ is a linear combination of
	$$
	\left(\begin{array}{c}M \\ N \end{array}\right) \sim \mathcal{N}_{p+q}(0, J_{full})
$$
Further the limit distribution of $D_n = \sqrt{n}\left( \hat{\gamma}_{full} - \gamma_0 \right)$ is another linear combination of $M$ and $N$, namely 
	$$D = (\delta + W) = \delta + K\left(N - J_{10}J_{00}^{-1}M\right)$$
Therefore, the limiting distribution of all the submodel estimators \emph{jointly} with $D_n$ can be written as the appropriate linear combination of $(M',N')'$, so the joint distribution is multivariate normal. Now we can apply the continuous mapping theorem as desired, to find:
	$$
\sqrt{n}\left( \hat{\mu} -\mu_{true} \right) = \sum_S \left[ c(S|D_n) \sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)  \right] \overset{d}{\rightarrow} \sum_S c(S|D_n) \Lambda_S
$$
where $\Lambda_S$ is the limit distribution of $\sqrt{n}\left( \hat{\mu}_S -\mu_{true} \right)$ defined above. Let
	$$\Lambda \equiv \sum_S c(S|D_n) \Lambda_S$$
We want to express $\Lambda$ in a more convenient form using the fact that
	$$\Lambda_S = \nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)$$
as shown in Lemma 3.3. Substituting,
	\begin{eqnarray*}
	\Lambda &=& \sum_S c(S|D) \left[\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M + \omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right)  \right] \\
			&=&\left[ \sum_S c(S|D)\right]\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \sum_S \left[c(S|D)\omega'\left(\delta -K^{1/2}H_S K^{-1/2}D\right) \right] \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \left[\sum_S c(S|D)\right]\omega'\delta - \sum_S c(S|D)\omega 'K^{1/2}H_S K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \sum_S c(S|D)\omega 'K^{1/2}H_S K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'K^{1/2}\left[ \sum_S c(S|D)H_S\right] K^{-1/2}D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega '\left(K^{-1/2}\left[ \sum_S c(S|D)H_S\right] K^{1/2}\right)'D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'G(D)'D \\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega' \hat{\delta}(D)\\
			&=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\left\{\delta - \hat{\delta}(D)\right\}
\end{eqnarray*}
where we have used the following facts:
	\begin{enumerate}
		\item Only $H_S$ depends on $S$.
		\item The weights sum to one.
		\item As scalars, the weights commute and are (trivially) symmetric.
		\item $H_S$, $K^{1/2}$, and $K^{-1/2}$ are symmetric.
	\end{enumerate}
along with the definitions of $G(\overset{d}{\rightarrow}ot)$ and $\hat{\delta}(D)$. Now we have shown that
	$$
\Lambda = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(D) \right] 
$$
Notice that, since $\hat{\delta}(D)$ depends only on $D = \delta + W$, and $M$ is independent of $W$, it follows that the two terms in this expression are likewise independent. The first follows a normal distribution but the second is, in general, non-normal.

Now, since $M$ and $D = \delta + W$ are independent, it follows that the distribution of $M|D$ is the same as that of $M$. Thus, 
	$$
	\Lambda|(D=d) \sim  \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left[\delta - \hat{\delta}(d) \right] 
$$
which is a normal distribution, since $\hat{\delta}(d)$ is a constant taking into account the conditioning. The mean and variance are as follows:
	\begin{eqnarray*}
	E\left[\Lambda  | D= d\right] &=& E\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + \omega' \left[\delta - \hat{\delta}(d) \right] \\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}E\left[M\right] + \omega' \left[\delta - \hat{\delta}(d) \right] \\
			&=& \omega' \left[\delta - \hat{\delta}(d) \right] 	
\end{eqnarray*}
	\begin{eqnarray*}
	Var\left[\Lambda  | D= d\right] &=& Var\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right]\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}Var[M]J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}J_{00}J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
				&=& \nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}\nabla_\theta\mu(\theta_0, \gamma_0)\\
	&\equiv& \tau_0^2
\end{eqnarray*}
since $\omega'(\delta - \hat{\delta}(d) )$ is a constant. Note that $\tau^2_0$ is the \emph{minimal variance} of the estimators under consideration. Although the \emph{unconditional distribution} of $\Lambda$ is non-normal, we can still calculate its mean and variance using our decomposition into two independent terms and the linearity of expectation:
	\begin{eqnarray*}
	E\left[\Lambda\right] &=& E\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + E\left[\omega' \left\{\delta - \hat{\delta}(d) \right\}\right] \\
			&=& \omega' \delta  -\omega'  E\left[\hat{\delta}(d)\right] \\	
			&=& \omega' \left\{\delta  - E\left[\hat{\delta}(d)\right]\right\}
\end{eqnarray*}
	\begin{eqnarray*}
	Var\left[\Lambda\right] &=& Var\left[\nabla_\theta\mu(\theta_0, \gamma_0)' J_{00}^{-1}M\right] + Var\left[\omega' \left\{\delta - \hat{\delta}(d) \right\}\right]\\
			&=& \tau^2_0 + \omega'Var\left[\hat{\delta}(D)  \right]\omega
\end{eqnarray*}
Now, $\Lambda$ is the limit distribution of $\sqrt{n}(\hat{\mu}-\mu_{true})$ where $\hat{\mu}$ is the compromise estimator, thus if asymptotically unbiased, it should be centered around zero. Accordingly we find the MSE of $\Lambda$ as follows:
	\begin{eqnarray*}
		MSE(\Lambda) &=& E\left[(\Lambda - 0)^2  \right] = E\left[\Lambda^2  \right]\\
					&=& E\left[\left( \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M + \omega' \left\{\delta - \hat{\delta}(D)\right\}\right)^2 \right]\\
					&=& E\left[\left\{\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \right\}^2\right] + E\left[\left(\omega' \left\{\delta - \hat{\delta}(D)\right\}\right)^2 \right]\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2E \left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=& E\left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M M' J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\right]\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; + E\left[\omega' \{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \omega\right]\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2E \left[\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=&\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} E\left[M M' \right]J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; +\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
		&& \;\;\;\;\;\;\;\;\;\;\;\;+ 2\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1}E \left[ M\right]E\left[ \omega' \left\{\delta - \hat{\delta}(D)\right\} \right]\\
		&=&\nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} J_{00}J_{00}^{-1}\nabla_\theta \mu(\theta_0,\gamma_0)\\
			&& \;\;\;\;\;\;\;\;\;\;\;\; +\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
		&=&\tau_0^2+\omega'  E\left[\{\delta - \hat{\delta}(D)\}\{\delta - \hat{\delta}(D)\}' \right]\omega\\
	&=& \tau^2_0 + R(\delta)
\end{eqnarray*}
Where we have used the fact that $E[M]=0$ and hence $Var[M] = E[MM']$ and the independence of $M$ and $D$ and hence of any measurable functions thereof.
\end{proof}

\paragraph{IMPORTANT:} For convenience, define $\Lambda_0 = \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M$. The \emph{key point} here is that the distribution of
	$$\Lambda = \sum_{S}c(S|D)\Lambda_S = \Lambda_0 + \omega'\{\delta - \hat{\delta}(D) \}$$
is often \textbf{dramatically non-normal}. To find the density of $\Lambda$, first condition on $D$ using the result from above:
	\begin{eqnarray*}
		\Lambda|(D=x) &=& \Lambda_0 + \omega'\{\delta - \hat{\delta}(x) \}\\
		 &\sim& \mathcal{N}(0,\tau_0^2) +  \omega'\{\delta - \hat{\delta}(x) \}
\end{eqnarray*}
Now, let $h(z)$ denote the density of $\Lambda$. We can calculate $h$ by integrating $D$ out of the joint density of $(\Lambda,D)$. Let $f(x)$ denote the density of $D$. We have
	$$
	\left\{\begin{array}{l}
		h(z|D=x) \sim \mathcal{N}\left( \omega'\{\delta - \hat{\delta}(x) \} , \tau_0^2\right)\\
		D = \delta + W \sim \mathcal{N}_q(\delta,K)
\end{array}\right.
$$
Now factor the joint density according to $h(z|D=x)f(x)$ and integrate out $D$ as follows:
	$$
	h(z) = \int h(z|D=x)f(x)\; dx
$$
We can then substitute the two normal distributions and then either numerically integrate or simulate. \emph{Notice}, however, that \textbf{the result depends on the unknown constant} $\delta$. 



\paragraph{Using the Full Model Variance} One approach to constructing a confidence interval that takes account of model selection uncertainty is to essentially use the variance of the full model. Define
	\begin{eqnarray*}
		\tau_{full}^2 &=& \mbox{AVAR}(\hat{\mu}_{full})= Var[\Lambda_{full}]\\
			&=&  \nabla_{\theta}\mu(\theta_0,\gamma_0)'J_{00}^{-1}\nabla_{\theta}\mu(\theta_0,\gamma_0) + \omega'K^{1/2}H_{full} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2}H_{full} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2}\left\{ K^{-1/2}(\pi_{full}' K_{full} \pi_{full})  K^{-1/2}  \right\} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega'K^{1/2} K^{-1/2} K K^{-1/2} K^{1/2}\omega\\
			&=& \tau_0^2 + \omega' K \omega
\end{eqnarray*}
And accordingly $\tau_{full} = (\tau_0^2 + \omega' K \omega)^{1/2}$. Now let $\hat{\omega}$ be a consistent estimator of $\omega$ and $\widehat{\kappa}$ be a consistent estimator of $\tau_{full}$. Define
	$$T_n =\frac{\left[\sqrt{n}(\hat{\mu} - \mu_{true})  - \widehat{\omega}'\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \right]}{\widehat{\kappa}}$$
 From above, we know that the following converges jointly in distribution:
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		D_n
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}\\
		D
\end{array}\right] 	
$$
Thus
	\begin{eqnarray*}	
T_n &\overset{d}{\rightarrow}& \frac{\left[\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}  - \omega'\left\{D -  \sum_{S\in \mathcal{A}} c(S|D)G_S D \right\} \right]}{\tau_{full}}\\ \\
		&=& (\tau_0^2 + \omega' K \omega)^{-1/2}\left[\Lambda_0 + \omega'\left( \delta - D\right)\right] 
\end{eqnarray*}
We know from above that
	$$
	\left[\begin{array}{c}
		M\\
		W
	\end{array}\right] \sim \mathcal{N}_{p+q}\left(
	\left[\begin{array}{c}
		0\\
		0
	\end{array}\right],
	\left[\begin{array}{cc}
		J_{00}&0\\
		0&K
	\end{array}\right]\right)
$$
where $K = J^{11}$, so
	\begin{eqnarray*}
		\Lambda_0 &\equiv& \nabla_\theta \mu(\theta_0,\gamma_0)' J_{00}^{-1} M\\
			&\sim&  \mathcal{N}\left(0, \tau_0^2\right)
\end{eqnarray*}
independently of
	\begin{eqnarray*}
		\omega'\left( \delta - D\right)&\equiv&- \omega'W\\
			&\sim& \mathcal{N}\left(0, \omega'K\omega \right)
\end{eqnarray*}
Therefore
	\begin{eqnarray*}
		T_n &\overset{d}{\rightarrow}& (\tau_0^2 + \omega' K \omega)^{-1/2}\left[\Lambda_0 + \omega'\left( \delta - D\right)\right] \\
		&=& (\tau_0^2 + \omega' K \omega)^{-1/2} \times \mathcal{N}\left(0, \tau_0^2 + \omega'K\omega\right)\\
		&\sim& \mathcal{N}\left(0,1 \right)
\end{eqnarray*}
which is a \textbf{standard normal}! We can use this result to create a approximate confidence interval for $\hat{\mu}$ as follows. For large $n$, 
	$$T_n =\widehat{\kappa}^{-1}\left[\sqrt{n}(\hat{\mu} - \mu_{true})  - \omega'\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \right] \approx \mathcal{N}(0,1)$$
To create a $(1-\alpha)\times 100\%$ interval, define $z_{\alpha/2}$ as the appropriate quantile of a standard normal random variable so that
	$$P \left[-z_{\alpha/2} \leq T_n \leq z_{\alpha/2} \right] \approx 1-\alpha $$
Then,
	\begin{eqnarray*}
		1-\alpha&\approx&P \left[-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq (\hat{\mu} - \mu_{true})  - \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \geq (\mu_{true} - \hat{\mu})  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \geq -\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\ \\
	&=&P \left[-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq (\mu_{true} - \hat{\mu})  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq \mu_{true}  + \frac{\widehat{\omega}'}{\sqrt{n}}\left\{D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right\} \leq \hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \leq \mu_{true}  + \Delta_n\leq \hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}} \right]\\\\
	&=&P \left[\left(\hat{\mu}-\frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}}\right) - \Delta_n \leq \mu_{true}  \leq \left(\hat{\mu} + \frac{z_{\alpha/2}\widehat{\kappa}}{\sqrt{n}}\right) - \Delta_n \right]
\end{eqnarray*}
where
	$$\Delta_n \equiv \frac{\widehat{\omega}'}{\sqrt{n}}\left[D_n -  \sum_{S\in \mathcal{A}} c(S|D_n)G_S D_n \right]$$
According to Claeskens and Hjort: ``this method is first-order equivalent to using the full model for confidence interval construction, with a modification for location.''




\paragraph{Correcting Confidence Intervals: Simulation} Another possibility is to simulate from the limiting distribution of $\Lambda$ \emph{for a range fixed value of} $\delta$, using consistent estimates of all other unknown quantities. This procedure can then be repeated for a variety of choices of $\delta$. To make this clearer, first rewrite $\Lambda$ using an expression from the proof Theorem 4.1
	\begin{eqnarray*}
		\Lambda &=&\nabla_{\theta}\mu(\theta_0, \gamma_0)'J_{00}^{-1}M+  \omega'\delta - \omega 'K^{1/2}\left[ \sum_S c(S|D)H_S\right] K^{-1/2}D \\
			&=& \Lambda_0 + \omega' \left[ \delta - K^{1/2} \sum_S c(S|D)H_S K^{-1/2}D \right]\\
			&=& \Lambda_0 + \omega' \left[ \delta - \sum_S c(S|D)K^{1/2} H_S K^{-1/2}D \right]\\
			&=& \Lambda_0 + \omega' \left[ \delta - \sum_S c(S|D)G_S D \right]
	\end{eqnarray*}
where we have defined $G_S = K^{1/2} H_S K^{-1/2}$. We know from above  that $\Lambda_0 \sim \mathcal{N}(0,\tau_0^2)$ independent of $D\sim \mathcal{N}_q(\delta,K)$. The simulation procedure is as follows:
		\begin{enumerate}
			\item Calculate consistent estimates of $G_S$, $\tau_0^2$, $\omega$ and $K$ using the estimation results and fix a value of $\delta$.
			\item Using the result of step one, generate:
				\begin{enumerate}
					\item $\Lambda_{0,j}\sim\mathcal{N}(0, \hat{\tau}^{2}_0)$ independently of
					\item  $D_j \sim \mathcal{N}_q(\delta, \hat{K})$
				\end{enumerate}
			\item Calculate the weights $c$ using $D_j$ and set 
				$$\Lambda_j = \Lambda_{0,j} + \hat{\omega}' \left[ \delta - \sum_S c(S|D_j)\hat{G}_S D_j \right]$$
		\item Repeat steps 1 and 2 for $j= 1, 2, \hdots, B$
		\item Using the samples $\{\Lambda_1, \Lambda_2, \hdots, \Lambda_B\}$ generated in steps 3 and 4, calculate quantiles $a(\delta)$ and $b(\delta)$ that satisfy:
			$$P\left[ a(\delta) \leq \Lambda(\delta) \leq b(\delta) \right]= 0.95$$
		\item Repeat steps 2--5 for varying choices of $\delta$.
		\end{enumerate}
Now, suppose we know the values $[a(\delta), b(\delta)]$. Since $\Lambda$ is the limit distribution of $\sqrt{n}(\hat{\mu} - \mu_{true})$, it follows that
	$$P\left[a(\delta)\leq \sqrt{n}(\hat{\mu} - \mu_{true}) \leq b(\delta)  \right]\rightarrow P\left[a(\delta)\leq \Lambda(\delta) \leq b(\delta)  \right] = 0.95$$
Thus, $\left[\hat{\mu} - b(\delta)/\sqrt{n}, \hat{\mu} - a(\delta)/\sqrt{n}  \right]$ covers $\mu_{true}$ with probability $0.95$ asymptotically.

\paragraph{But We Don't Know $\delta$!} A naive approach would be to substitute our estimate $D_n = \hat{\delta}_{full} = \sqrt{n}(\hat{\gamma}_{full} - \gamma_0)$, carrying about the above simulations at this value and creating an interval based on $\hat{a} = a(D_n)$ and $\hat{b}=b(D_n)$. This is simple, but may not always work well. Let $p_n(\delta)$ be the coverage probability for this procedure. Its limit is
	$$p(\delta) = P\left[a(D)\leq \Lambda(\delta) \leq b(D)  \right]$$
which can be simulated by the method described above. It turns out that this method sometimes gives coverage that is \emph{far too low}.

\paragraph{A Better Procedure:} Instead of simply substituting $D_n$ for $\delta$ in the simulation described above, we could first construct a confidence region for $\delta$ and use this region to create an interval for $\hat{\mu}$. Since 
	$$D_n \overset{d}{\rightarrow} D = \delta + W \sim \mathcal{N}_q\left(\delta,  K\right)$$
where $K = J^{11}$, we have
	$$\left(D_n - \delta\right)' \widehat{K}^{-1}\left(D_n - \delta\right) \overset{d}{\rightarrow} \chi^2_q$$
Now, define 
	$$\rho_n(D_n,\delta) =\left[\left(D_n - \delta\right)' \widehat{K}^{-1}\left(D_n - \delta\right)\right]^{1/2}$$
and the event
	$$A_n(c) = \left\{\rho_n(D_n,\delta) \leq c  \right\}$$
Now, since $\rho_n(D_n, \delta)^2 \approx \chi^2_q$ we have
		$$P\left\{A_n(c)\right\}=P\left\{\rho_n(D_n,\delta) \leq c  \right\} = P\left\{\rho_n(D_n,\delta)^2 \leq c^2  \right\} \approx P\left\{\chi^2_q \leq c^2  \right\} 
$$
where we have used the fact that $x^2$ is strictly increasing for $x\geq0$ and that $\rho_n \geq 0$. Now define $z = (\chi^2_{q,0.95})^{1/2}$ and $A_n = A_n(z)$, so that $P\left\{A_n  \right\}\approx 0.95$. In the simulations described above in which we assumed that $\delta$ was known, we defined $a(\delta)$ and $b(\delta)$ so that
	$$P\left[ a(\delta) \leq \Lambda(\delta) \leq b(\delta) \right]= 0.95$$
Now, let
	\begin{eqnarray*}
		\widehat{a}_0(D_n)&=& \min \left\{a(\delta)\colon \rho_n(D_n, \delta) \leq z\right\}\\
		\widehat{b}_0(D_n)&=& \max \left\{b(\delta)\colon \rho_n(D_n, \delta) \leq z\right\}
\end{eqnarray*}
The claim is that the limit coverage level of
	$$\mbox{CI}_n^* = \left[\widehat{\mu} - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} , \;\;\; \widehat{\mu} - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]$$
is always \emph{above} 0.90, resulting in a conservative procedure. To see why this is the case, we return to the limit experiment, in which we have joint convergence of all the necessary random variables, as described above. This implies that the coverage probability $r_n(\delta)$ to which $\{\mu_{true}\in \mbox{CI}_n^*\}$ converges is given by
	$$r(\delta) = P\left\{ a_0(D) \leq \Lambda(\delta) \leq b_0(D)\right\}$$
where $\rho(D,\delta)^2 = \left(D - \delta\right)' K^{-1}\left(D- \delta \right)$ and
		\begin{eqnarray*}
		a_0(D)&=&\min \left\{a(\delta)\colon \rho(D, \delta) \leq z\right\}\\
		b_0(D)&=&\min \left\{b(\delta)\colon \rho(D, \delta) \leq z\right\}
\end{eqnarray*}
How does this work? The interval $\mbox{CI}_n^*$ is based on 
		\begin{eqnarray*}	
0.9&\leq&P\left[ \widehat{\mu} - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} \leq \mu_{true}\leq \widehat{\mu} - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]\\\\
		&=& P\left[  - \frac{\widehat{b}_0(D_n)}{\sqrt{n}} \leq \mu_{true} - \widehat{\mu}\leq  - \frac{\widehat{a}_0(D_n)}{\sqrt{n}}  \right]\\\\
		&=& P\left[  - \widehat{b}_0(D_n) \leq \sqrt{n}\left(\mu_{true} - \widehat{\mu}\right)\leq  - \widehat{a}_0(D_n) \right]\\
		&=& P\left[ \widehat{b}_0(D_n) \geq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\geq  \widehat{a}_0(D_n) \right]\\
		&=& P\left[ \widehat{a}_0(D_n) \leq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\leq  \widehat{b}_0(D_n) \right]
\end{eqnarray*}
Now, we know that
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		D_n
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda_0 + \omega' \left\{ \delta - \sum_{S \in \mathcal{A}}  c(S|D)G_S D \right\}\\
		D
\end{array}\right] 	
$$
so, by the Continuous Mapping Theorem,
	$$
	\left[\begin{array}{c}
		\sqrt{n}(\hat{\mu} - \mu_{true})\\
		\rho_n(D_n,\delta)\\
		\widehat{a}_0(D_n)\\
		\widehat{b}_0(D_n)
\end{array}\right] \overset{d}{\rightarrow}
		\left[\begin{array}{c}
		\Lambda(\delta)\\
		\rho(D,\delta)\\
		a_0(D)\\
		b_0(D)
\end{array}\right] 	
$$
and thus, 
	$$P\left[ \widehat{a}_0(D_n) \leq \sqrt{n}\left( \widehat{\mu}-\mu_{true}\right)\leq  \widehat{b}_0(D_n) \right] \rightarrow P\left\{ a_0(D) \leq \Lambda(\delta) \leq b_0(D)\right\} = r(\delta)$$
Now, let $A = \{  \rho(D,\delta)\leq z\}$ where, as before, $z = (\chi^2_{q,0.95})^{1/2}$ implying that $P\left\{A  \right\}=0.95$. Then,
	\begin{eqnarray*}
		0.95 &=& P\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\\
			&=& P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right] + P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right] 
\end{eqnarray*}
Now, since
		\begin{eqnarray*}
		a_0(D)&=&\min \left\{a(\delta)\colon \rho(D, \delta) \leq z\right\}\\
		b_0(D)&=&\min \left\{b(\delta)\colon \rho(D, \delta) \leq z\right\}
\end{eqnarray*}
we have
$$\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \Rightarrow \left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\}$$
and hence
	$$P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right]  \leq P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\}$$
Further, since
	$$\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \Rightarrow A^c$$
we have
	$$P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right]  \leq P(A^c)$$
Combining:
		\begin{eqnarray*}
		0.95 &=& P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A \right] + P\left[\left\{a(\delta) \leq \Lambda(\delta) \leq b(\delta)  \right\}\cap A^c \right] \\
			&\leq&  P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} + P(A^c)\\
			&=& P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} + 0.05
\end{eqnarray*}
since $A$ is defined with reference to a $95\%$ confidence interval. Subtracting, 
	$$P\left\{a_0(D)  \leq \Lambda(\delta) \leq b_0(D)\right\} \geq 0.90$$
as claimed.

Here's the intuition for what just happened. $\Lambda$ is a random variable whose distribution depends on the unknown constant $\delta$. The constants $a(\delta)$ and $b(\delta)$ are quantiles of the distribution of $\Lambda$ such that 
	$$P\left[a(\delta) \leq \Lambda(\delta) \leq b(\delta)\right] = 0.95$$
Since $\Lambda$ depends on $\delta$, so do its quantiles: different values of $\delta$ would result in different intervals. 

This is the procedure. First we use $\rho(D,\delta)\leq z$ to get a confidence interval for $\delta$. Then we plug each point in this interval for $\delta$ into $\Lambda(\delta)$ and calculate the corresponding bounds $a(\delta)$ and $b(\delta)$. For each value of $\delta$ such that $\rho(D,\delta)\leq z$ we get a \emph{different} confidence interval for $\Lambda$. The lower bound of all these intervals is $a_0(D)$ while the upper bound is $b_0(D)$. The point here is to assess the coverage of the resulting interval.

The confusion here comes from bad notation: sometimes $\delta$ is being treated as fixed, other times as variable. Need to come up with some clearer notation...








