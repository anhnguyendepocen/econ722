%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Kullback-Leibler (KL) Divergence}

  \begin{block}{Motivation}
    How well does a given density $f(y)$ approximate an unknown true density $g(y)$? Use this to select between parametric models.
  \end{block}

  \pause

  \begin{alertblock}{Definition}
    \vspace{-3em}
  \[
    \text{KL}(g;f) = \underbrace{\mathbb{E}_G \left[ \log\left\{ \frac{g(Y)}{f(Y)} \right\}\right]}_{\text{True density on top}} 
    = \underbrace{\mathbb{E}_G\left[ \log g(Y) \right]}_{\substack{\text{Depends only on truth} \\ \text{Fixed across models}}} - \underbrace{\mathbb{E}_G \left[ \log f(Y) \right]}_{\substack{\text{Expected} \\ \text{log-likelihood}}}
  \]
\end{alertblock}

\vspace{-2.5em}

\pause

\begin{block}{Properties}
  \begin{itemize}
    \item \emph{Not} symmetric: $\text{KL}(g;f) \neq \text{KL}(f;g)$
    \item By Jensen's Inequality: $\text{KL}(g;f) \geq 0$ (strict iff $g=f$ a.e.)
    \item Minimize KL $\iff$ Maximize Expected log-likelihood
  \end{itemize}

\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{KL Divergence and Mis-specified MLE}

  \begin{block}{Pseudo-true Parameter Value $\theta_0$}
    \vspace{-1em}
  \[
    \widehat{\theta}_{MLE} \overset{p}{\rightarrow} \theta_0 \equiv \underset{\theta \in \Theta}{\arg \min} \; \text{KL}(g;f_\theta) = \underset{\theta \in \Theta}{\arg \max} \; \mathbb{E}_G[\log f(Y|\theta)] 
  \]
\end{block}


\pause

\begin{block}{What if $f_\theta$ is correctly specified?}
  If $g = f_\theta$ for some $\theta$ then $\text{KL}(g;f_\theta)$ is minimized at zero.
\end{block}

\pause

\begin{alertblock}{Goal: Compare Mis-specified Models}
  \vspace{-2.5em}
  \begin{align*}
    \mathbb{E}_G \left[ \log f(Y|\theta_0) \right] && \mbox{versus} && 
    \mathbb{E}_G \left[ \log h(Y|\gamma_0) \right]
  \end{align*}
  \vspace{-2em}

 where $\theta_0$ is the pseudo-true parameter value for $f_\theta$ and $\gamma_0$ is the pseudo-true parameter value for $h_\gamma$.
  
\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{How to Estimate Expected Log Likelihood?}
  \framesubtitle{For simplicity: $Y_1, \dots, Y_n \sim \mbox{ iid } g(y)$}


  \begin{block}{Unbiased but Infeasible}
    \[
      \mathbb{E}_G \left[\frac{1}{T}\ell(\theta_0)\right] = \mathbb{E}_G\left[ \frac{1}{T} \sum_{t=1}^T f(Y_t|\theta_0)\right] = \mathbb{E}_G\left[ f(Y|\theta_0) \right]
    \]
    
  \end{block}
   
  \pause

  \vspace{-1em}

  \begin{block}{Biased but Feasible}
    $T^{-1}\ell(\widehat{\theta}_{MLE})$ is a \alert{biased} estimator of $\mathbb{E}_G[\log f(Y|\theta_0)]$. 
  \end{block}

  \pause

  \begin{block}{Intuition for the Bias}
    $T^{-1}\ell(\widehat{\theta}_{MLE}) > T^{-1}\ell(\theta_0)$ unless $\widehat{\theta}_{MLE} = \theta_0$. 
    Maximized sample log-like.\ is an \alert{overly optimistic} estimator of expected log-like.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What to do about this bias?}

  \begin{enumerate}
    \item General-purpose asymptotic approximation of ``degree of over-optimism'' of maximized sample log-likelihood.
      \begin{itemize}
        \item Takeuchi's Information Criterion (TIC)
        \item Akaike's Information Criterion (AIC)
      \end{itemize}
      \pause
    \item Problem-specific finite sample approach, assuming $g \in f_\theta$.
      \begin{itemize}
        \item Corrected AIC (AIC$_c$) of Hurvich and Tsai (1989)
      \end{itemize}
  \end{enumerate}

  \pause
  \begin{alertblock}{Tradeoffs}
   TIC is most general and makes weakest assumptions, but requires very large $T$ to work well. 
   AIC is a good approximation to TIC that requires less data.
   Both AIC and TIC perform poorly when $T$ is small relative to the number of parameters, hence AIC$_c$.
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Recall: Asymptotics for Mis-specified ML Estimation}
  \framesubtitle{Model $f(y|\theta)$, pseudo-true parameter $\theta_0$. For simplicity $Y_1, \dots, Y_T \sim \mbox{ iid } g(y)$.}

  \pause

  \begin{block}{Fundamental Expansion}
    \vspace{-1.5em}
  \[
    \alert{\sqrt{T} ( \widehat{\theta} - \theta_0)   = J^{-1}\left(\sqrt{T} \bar{U}_T\right) + o_p(1)}
  \]
  \footnotesize
\[
      J = -\mathbb{E}_G \left[ \frac{\partial \log f(Y|\theta_0)}{\partial \theta \partial \theta'} \right], \quad
      \bar{U}_T =\frac{1}{T} \sum_{t=1}^T \frac{\partial \log f(Y_t)}{\partial \theta}
\] 
\end{block}

\normalsize

\vspace{-1em}

\pause

\begin{block}{Central Limit Theorem}
  \vspace{-1em}
  \footnotesize
  \[
    \sqrt{T} \bar{U}_T \rightarrow_d U \sim N_p(0, K), \quad
    K = \text{Var}_G\left[\frac{\partial \log f(Y|\theta_0)}{\partial \theta}\right]
  \] 
  \normalsize

  \vspace{-1em}

  \[
    \alert{\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U \sim N_p(0, J^{-1}KJ^{-1})}
  \]
\end{block}

\vspace{-1em}

\pause

\begin{block}{Information Matrix Equality}
  If $g = f_\theta$ for some $\theta \in \Theta$ then $K = J \implies \text{AVAR}(\widehat{\theta}) = J^{-1}$
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Bias Relative to Infeasible Plug-in Estimator}


  \begin{block}{Definition of Bias Term $B$}
  \[
    B = \underbrace{\frac{1}{T} \ell(\widehat{\theta})}_{\substack{ \text{feasible}\\ \text{overly-optimistic}}} - \underbrace{\int g(y) \log f(y|\widehat{\theta})\; dy}_{\substack{\text{uses data only once} \\ \text{infeas.\ not overly-optimistic}}}
  \]
\end{block}

\pause

\begin{alertblock}{Question to Answer}
  On average, over the sampling distribution of $\widehat{\theta}$, how large is $B$? AIC and TIC construct an asymptotic approximation of $\mathbb{E}[B]$.
\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Derivation of AIC/TIC}
  \begin{block}{Step 1: Taylor Expansion}
    \vspace{-1em}
    \[B = \bar{Z}_T + (\widehat{\theta} - \theta_0) + (\widehat{\theta} - \theta_0)' J  (\widehat{\theta} - \theta_0) + o_p(T^{-1})\]
    \footnotesize
    \[\bar{Z}_T = \frac{1}{T} \sum_{t=1}^T \left\{ \log f(Y_t|\theta_0) - \mathbb{E}_G[\log f(Y|\theta_0) \right\}\]
  \end{block}

  \vspace{-1em}

  \pause

  \begin{block}{Step 2: $\mathbb{E}[\bar{Z}_T] = 0$}
    \vspace{-1.5em}
    \[\mathbb{E}[B] \approx \mathbb{E}\left[ (\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \right]\]
  \end{block}

  \pause

  \vspace{-1em}

  \begin{block}{Step 3: $\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U$}
    \vspace{-1.5em}
    \[
      T(\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \rightarrow_d U'J^{-1}U
    \]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Derivation of AIC/TIC Continued\dots}
  \begin{block}{Step 3: $\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U$}
    \vspace{-1.5em}
    \[
      T(\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \rightarrow_d U'J^{-1}U
    \]
  \end{block}

  \pause
  
  \begin{block}{Step 4: $U \sim N_p(0, K)$}
    \vspace{-1.5em}
   \[
     \mathbb{E}[B] \approx \frac{1}{T} \mathbb{E}[U'J^{-1}U] = \text{tr}\left\{ J^{-1}K \right\}
   \]
  \end{block}

  \pause

  \vspace{-1em}

  \begin{block}{Final Result:}
    $T^{-1}\text{tr}\left\{ J^{-1}K \right\}$ is an asymp.\ unbiased estimator of the over-optimism of $T^{-1}\ell(\widehat{\theta})$ relative to $\int g(y)\log f(y|\widehat{\theta})\; dy$.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{TIC and AIC}
  \begin{block}{Takeuchi's Information Criterion}
    Multiply by $2T$, estimate $J,K \Rightarrow \alert{\text{TIC} = 2\left[ \ell(\widehat{\theta}) - \text{tr}\left\{ \widehat{J}^{-1}\widehat{K} \right\} \right]}$
  \end{block}

  \pause

  \begin{block}{Akaike's Information Criterion}
    If $g = f_\theta$ then $J^{-1} = K \Rightarrow \text{tr}\left\{ J^{-1  }K \right\} = p \Rightarrow \alert{\text{AIC} = 2\left[ \ell(\widehat{\theta}) - p \right]}$ 
  \end{block}

  \pause

  \begin{block}{Contrasting AIC and TIC} 
    Technically, AIC requires that all models under consideration are at least correctly specified while TIC doesn't. 
    But $J^{-1}K$ is hard to estimate, and if a model is badly mis-specified, $\ell(\widehat{\theta})$ dominates.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
