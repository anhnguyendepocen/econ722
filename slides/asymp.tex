%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AIC versus BIC in a Simple Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Consistency versus Efficiency in a Simple Example}

  \begin{block}{Information Criteria}
    Consider criteria of the form $\text{IC}_m = 2\ell(\theta) - d_T \times \text{length}(\theta)$.
  \end{block}
  
  
  \begin{block}{True DGP}
  $Y_{1}, \dots, Y_T \sim \mbox{iid N}(\mu, 1)$
  \end{block}

  \pause

  \begin{block}{Candidate Models}
    $\text{M}_0$ assumes $\mu = 0$, $\text{M}_1$ does not restrict $\mu$. Only one parameter:

    \vspace{-1em}
  \begin{align*}
    \text{IC}_0 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_0 \right\} \\ 
    \text{IC}_1 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_1 \right\} - d_T
  \end{align*}
  \end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Log-Likelihood Function}

  Since $\; \alert{\sum_{t=1}^T (Y_t -\mu)^2 = T(\bar{Y} - \mu)^2 + T \widehat{\sigma}^2}$,

\begin{eqnarray*}
	\ell_T(\mu)&=& \sum_{t=1}^T \log \left( \frac{1}{2\pi} \exp \left\{-\frac{1}{2}(Y_t - \mu)^2 \right\}\right)\\
  &=& -\frac{T}{2} \log\left( 2\pi \right) - \frac{1}{2} \sum_{t=1}^{T} (Y_t - \mu)^2\\
  &=& -\frac{T}{2} \log\left( 2\pi \right) - \frac{T}{2} \widehat{\sigma}^2 - \frac{T}{2}(\bar{Y} - \mu)^2\\
  &=& \text{Constant} - \frac{T}{2}(\bar{Y} - \mu)^2
\end{eqnarray*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Side Calculation: \normalsize $\sum_{t=1}^T (Y_t -\mu)^2 = T(\bar{Y} - \mu)^2 + T \widehat{\sigma}^2$}

\scriptsize
\begin{eqnarray*}
  T\widehat{\sigma}^2 &=& \sum_{t=1}^T \left(Y_t - \bar{Y}\right)^2 = \sum_{t=1}^T \left(Y_t - \mu + \mu - \bar{Y}\right)^2 = \sum_{t=1}^T \left[(Y_t -\mu) - (\bar{Y} - \mu)\right]^2\\
		&=&\sum_{t=1}^T (Y_t -\mu)^2 - \sum_{t=1}^T 2(Y_t -\mu)(\bar{Y} - \mu) + \sum_{t=1}^T (\bar{Y} - \mu)^2\\
				&=& \left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2(\bar{Y} - \mu) \left( \sum_{t=1}^T Y_t- \sum_{t=1}^T \mu \right)+T(\bar{Y} - \mu)^2\\
				&=& \left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2(\bar{Y} - \mu)(T\bar{Y}-T\mu)+T(\bar{Y} - \mu)^2\\
				&=&\left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2T(\bar{Y} - \mu)^2+T(\bar{Y} - \mu)^2\\
				&=&\left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - T(\bar{Y} - \mu)^2
\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Selected Model $\widehat{\text{M}}$}

  \begin{block}{Information Criteria}
  M$_0$ sets $\mu=0$ while M$_1$ uses the MLE $\bar{Y}$, so we have
  \vspace{1em}
  \begin{align*}
    \text{IC}_0 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_0 \right\} = 2 \times \text{Constant} - T\bar{Y}^2\\ 
    \text{IC}_1 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_1 \right\} = 2 \times \text{Constant} - d_T\\ 
  \end{align*}
\end{block}

\vspace{-2em}

  \begin{block}{Difference of Criteria}
    \vspace{-1em}
  \[
    \text{IC}_1 - \text{IC}_0 = T \bar{Y}^2 - d_T
  \]
\end{block}

\vspace{-1em}

  \begin{block}{Selected Model}
    \vspace{-1em}
   \[
     \widehat{\text{M}} = \left\{
       \begin{array}{cc}
         \text{M}_1, & |\sqrt{T} \bar{Y}| \geq \sqrt{d_T}\\
         \text{M}_0, & |\sqrt{T} \bar{Y}| \leq \sqrt{d_T}
       \end{array}
       \right.
   \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Case I: $\mu \neq 0$}
  \framesubtitle{Apply theory from earlier in lecture\dots}

  \begin{block}{KL-Divergence of $\text{M}_1$}
    $\text{M}_1$ is the true DGP with minimized KL-divergence equal to zero.
  \end{block}

  \begin{block}{KL-Divergence of $\text{M}_0$}
    \begin{itemize}
      \item Truth: $g(y) = (2\pi)^{-1/2}\exp\left\{ -(y-\mu)^2/2 \right\}$ 
      \item $\text{M}_0$: $f(y) = (2\pi)^{-1/2}\exp\left\{ -y^2/2\right\}$ 
      \item Hence: $\log g(y) - \log f(y) = -\frac{1}{2}(y-\mu)^2 + \frac{1}{2}y^2
          = \mu \left(y - \frac{\mu}{2}\right)$
    \end{itemize}

    \vspace{-1em}
          \begin{align*}
          \text{KL}(g;\text{M}_0) &= \int_{\mathbb{R}}\mu(y - \mu/2) (2\pi)^{-1/2}\exp\left\{ (y-\mu)^2/2 \right\}\; \text{d}y \\
          &= \mu(\mu - \mu/2) = \mu^2 /2
        \end{align*}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Verifying Weak Consistency: $\mu \neq 0$}

  \begin{block}{Condition on KL-Divergence}
  \small
  \vspace{-2em}
  \[
    \underset{T\rightarrow \infty}{\lim\inf} \frac{1}{T}\sum_{t = 1}^T \left\{ KL(g; \text{M}_0) - KL(g;\text{M}_1) \right\} = \underset{n\rightarrow \infty}{\lim\inf}\ \frac{1}{T}\sum_{t = 1}^T  \left(\frac{\mu^2}{2} - 0\right) > 0
  \]
\end{block}
\begin{block}{Condition on Penalty}
  \begin{itemize}
    \item Need $c_{T,k} = o_p(T)$, i.e.\ $c_{T,k}/T \overset{p}{\rightarrow} 0$.  
    \item Both AIC and BIC satisfy this
    \item If $\mu \neq 0$, both AIC and BIC select $\text{M}_1$ wpa 1 as $T\rightarrow \infty$.
  \end{itemize}
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Case II: $\mu = 0$}

  \begin{block}{What's different?}
  \begin{itemize}
    \item Both $M_1$ and $M_0$ are true and minimize KL divergence at zero. 
    \item \alert{Consistency} says choose most parsimonious true model: $\text{M}_0$
  \end{itemize}
  \end{block}

  \begin{block}{Verifying Conditions for Consistency}
    Use the second set of sufficient conditions:
    \begin{itemize}
      \item $N(0,1)$ model nested inside $N(\mu,1)$ model
      \item Truth is $N(0,1)$ so LR-stat is asymptotically $\chi^2(1) = O_p(1)$.
      \item For penalty term, need $\mathbb{P}(c_{T,k} - c_{T,0})\rightarrow \infty$
      \item BIC satisfies this but AIC doesn't.
    \end{itemize}
    
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Finite-Sample Selection Probabilities: AIC}
  \begin{block}{AIC Sets $d_T = 2$}


	$$\widehat{M}_{AIC} = \left\{\begin{array}
		{cc} M_1, &|\sqrt{T}\bar{Y}| \geq \sqrt{2} \\
		M_0, & |\sqrt{T} \bar{Y}| < \sqrt{2}
	\end{array} \right.$$
  \end{block}

  \vspace{-2em}

    \footnotesize
	\begin{eqnarray*}
		P\left(\widehat{M}_{AIC} = M_1\right) &=& P\left(\left|\sqrt{T}\bar{Y} \right| \geq \sqrt{2}  \right)\\
		&=& P\left(\left|\sqrt{T}\mu + Z\right| \geq \sqrt{2}  \right)\\
		&=& P\left(\sqrt{T}\mu + Z \leq -\sqrt{2}\right) + \left[1 - P\left(\sqrt{T} \mu +Z \leq \sqrt{2}\right) \right]\\
			&=& \Phi\left(-\sqrt{2} - \sqrt{T}\mu\right) + \left[1 -  \Phi\left(\sqrt{2} - \sqrt{T} \mu \right)\right]
	\end{eqnarray*}

  \normalsize
where $Z \sim N(0,1)$ since $\bar{Y} \sim N(\mu, 1/T)$ because $Var(Y_t)=1$.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Finite-Sample Selection Probabilities: BIC}

  \begin{block}{BIC sets $d_T = \log(T)$}
	$$\widehat{M}_{BIC} = \left\{\begin{array}
		{cc} M_1, & |\sqrt{T}\bar{Y} | \geq \sqrt{\log(T)} \\
		M_0, & |\sqrt{T} \bar{Y}| < \sqrt{\log(T)}
	\end{array} \right.$$
  \end{block}
Same steps as for the AIC except with $\sqrt{\log(T)}$ in the place of $\sqrt{2}$:
\footnotesize
	\begin{eqnarray*}
		P\left(\widehat{M}_{BIC} = M_1\right) &=& P\left(\left|\sqrt{T}\bar{Y} \right| \geq \sqrt{\log(T)}  \right)\\
			&=& \Phi\left(-\sqrt{\log(T)} - \sqrt{T}\mu\right) + \left[1 -  \Phi\left(\sqrt{\log(T)} - \sqrt{T} \mu \right)\right]
	\end{eqnarray*}

  \begin{block}{Interactive Demo: AIC vs BIC}

\url{https://fditraglia.shinyapps.io/CH\_Figure\_4\_1/}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Probability of Over-fitting}

  \small

  \begin{itemize}
    \item If $\mu = 0$ both models are true but $M_0$ is more parsimonious. 
    \item Probability of over-fitting ($Z$ denotes standard normal): 
\begin{eqnarray*}
	P\left(\widehat{M} = M_1\right) &=& P\left(|\sqrt{T}\bar{Y}|\geq \sqrt{d_T}\right) = P(|Z|\geq \sqrt{d_T})\\
	 &=& P(Z^2 \geq d_T) = P(\chi^2_1 \geq d_T)
\end{eqnarray*}
\item AIC: $d_T = 2$ and $P(\chi^2_1 \geq 2)\approx 0.157$.  
\item BIC: $d_T = \log(T)$ and $P(\chi^2_1 \geq \log T) \rightarrow 0$ as $T\rightarrow 0$.
  \end{itemize}

  \alert{AIC has $\approx$ 16\% prob.\ of over-fitting; BIC does not over-fit in the limit.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Risk of the Post-Selection Estimator}
  \begin{block}{The Post-Selection Estimator}
	$$\widehat{\mu}=\left\{\begin{array}
		{cc} \bar{Y}, & |\sqrt{T}\bar{Y} | \geq \sqrt{d_T} \\
		0, & |\sqrt{T}\bar{Y} | < \sqrt{d_T}
		\end{array}\right.$$
  \end{block}

  \vspace{-2em}

  \begin{block}{Recall from above}
Recall from above that $\sqrt{T} \bar{Y} = \sqrt{T}\mu +Z$ where $Z\sim N(0,1)$
  \end{block}
  \begin{block}{Risk Function}
MSE risk times $T$ since Var.\ of well-behaved estimator $ = O(1/T)$

\[
  R_T(\mu) = T \cdot \mathbb{E}\left[\left( \widehat{\mu} - \mu\right)^2\right] = \mathbb{E}\left[\left(\sqrt{T} \widehat{\mu} - \sqrt{T} \mu\right)^2\right] 
\]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simplifying the MSE Risk Function}
  \framesubtitle{$\sqrt{T} \bar{Y} = \sqrt{T}\mu +Z$ where $Z\sim N(0,1)$}

  \footnotesize

  Let $X = \mathbf{1}\left\{ A \right\}$ where $A = \left\{|\sqrt{T}\mu + Z|\geq \sqrt{d_T}  \right\}$
  \begin{eqnarray*}
  R_T(\mu) &=& \mathbb{E}\left[\left(\sqrt{T} \widehat{\mu} - \sqrt{T} \mu\right)^2\right] \\
  &=& \mathbb{E}\left\{ \left[ \left( \sqrt{T}\mu + Z \right)X - \sqrt{T}\mu \right]^2 \right\}\\
  &=& \mathbb{P}(A)\; \mathbb{E}\left\{ \left.\left[ \left(\sqrt{T}\mu + Z\right) - \sqrt{T}\mu\right]^2\right| X = 1 \right\} + \left[ 1 - \mathbb{P}(A) \right]\left( \sqrt{T}\mu \right)^2\\
  &=& \mathbb{P}(A)\; \mathbb{E}
  \left[ Z^2 |X = 1 \right] + \left[ 1 - \mathbb{P}(A) \right]T\mu^2
\end{eqnarray*}

\alert{So we need to calculate $\mathbb{E}[Z^2|X=1]$ and $\mathbb{P}(A)$.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
